{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch/XLA ResNet18/CIFAR10 Training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e9bb5ba3b01343be9f4e05dbb2a20843": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_11a22407527c4c2281c4e805a15eb177",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2ae8e563a7494dbabd93d4efc2e9e22c",
              "IPY_MODEL_8640cde766f34cd981a80293b56f4edb"
            ]
          }
        },
        "11a22407527c4c2281c4e805a15eb177": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2ae8e563a7494dbabd93d4efc2e9e22c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6cfd4503ec5846518bc0252231bea8f2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e8fdc0bdbb4b481e87f17be5272a87d1"
          }
        },
        "8640cde766f34cd981a80293b56f4edb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_23b76fa410384813b3611dcd38a6ab7d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [00:20&lt;00:00, 37957678.62it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bc959d99f6a34205963ec7e49194c112"
          }
        },
        "6cfd4503ec5846518bc0252231bea8f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e8fdc0bdbb4b481e87f17be5272a87d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "23b76fa410384813b3611dcd38a6ab7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bc959d99f6a34205963ec7e49194c112": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iypc-team/CoLab/blob/master/PyTorch_XLA_ResNet18_CIFAR10_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "id": "Z1iRTK9cAJ2a",
        "outputId": "403d5b27-6930-438e-ae38-1c49b499e336"
      },
      "source": [
        "from __future__ import absolute_import, division,print_function\n",
        "from IPython.display\n",
        "import os, pathlib, shutil, sys\n",
        "from os.path import *\n",
        "from pathlib import Path\n",
        "from google.colab import drive, files\n",
        "drive.flush_and_unmount()\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "print(f'cwd: {Path.cwd()}')\n",
        "\n",
        "contentPth = '/content'\n",
        "drivePth = '/content/drive'\n",
        "myDrivePth = '/content/drive/My Drive/'\n",
        "pythonFilesPth = '/content/drive/My Drive/PythonFiles'\n",
        "deleteDataPth = join(drivePth, 'sample_data')\n",
        "\n",
        "os.chdir(pythonFilesPth)\n",
        "\n",
        "def deleteBullshit(deletePth):\n",
        "    try:\n",
        "        os.chdir(deletePth)\n",
        "        # os.chdir(pythonFilesPth)\n",
        "        shutil.rmtree(deletePth)\n",
        "    except Exception as err: print(err)\n",
        "bs='''\n",
        "os.chdir(pythonFilesPth)\n",
        "'''\n",
        "from BashColors import C\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-20603c73ca91>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    from IPython.display\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YX1hxqUQn47M"
      },
      "source": [
        "## PyTorch/XLA ResNet18/CIFAR10 (GPU or TPU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLQPoJ6Fn8wF"
      },
      "source": [
        "### [RUNME] Install Colab compatible PyTorch/XLA wheels and dependencies\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O53lrJMDn9Rd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0e226ff-0eb0-40ba-f033-92c2751bcaa5"
      },
      "source": [
        "# PyTorch/XLA\n",
        "!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.7-cp36-cp36m-linux_x86_64.whl"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: cloud-tpu-client==0.10 in /usr/local/lib/python3.6/dist-packages (0.10)\n",
            "Requirement already satisfied: torch-xla==1.7 from https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.7-cp36-cp36m-linux_x86_64.whl in /usr/local/lib/python3.6/dist-packages (1.7)\n",
            "Requirement already satisfied: google-api-python-client==1.8.0 in /usr/local/lib/python3.6/dist-packages (from cloud-tpu-client==0.10) (1.8.0)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.6/dist-packages (from cloud-tpu-client==0.10) (4.1.3)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.17.4)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.0.4)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.1)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.15.0)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.16.0)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.17.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client==0.10) (0.2.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client==0.10) (4.6)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client==0.10) (0.4.8)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.52.0)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.12.4)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=34.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (50.3.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2018.9)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (4.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2020.11.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IednejwkIW-K"
      },
      "source": [
        "Only run the below commented cell if you would like a nightly release"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-bBdzgeISaP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7a33b45-37a4-47a9-b017-9739e25d2c93"
      },
      "source": [
        "import datetime\n",
        "datetime.time.microsecond\n",
        "\n",
        "# VERSION = \"nightly\"  #@param [\"nightly\", \"20200516\"]  # or YYYYMMDD format\n",
        "# !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "# !python pytorch-xla-env-setup.py --version $VERSION"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<attribute 'microsecond' of 'datetime.time' objects>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiFzLg5gy7l6"
      },
      "source": [
        "# PyTorch/XLA GPU Setup (only if GPU runtime)\n",
        "import os\n",
        "if os.environ.get('COLAB_GPU', '0') == '1':\n",
        "  os.environ['GPU_NUM_DEVICES'] = '1'\n",
        "  os.environ['XLA_FLAGS'] = '--xla_gpu_cuda_data_dir=/usr/local/cuda/'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMojPWZUqr2s"
      },
      "source": [
        "# Result Visualization Helper\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "M, N = 4, 6\n",
        "RESULT_IMG_PATH = '/tmp/test_result.jpg'\n",
        "CIFAR10_LABELS = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "                 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "def plot_results(images, labels, preds):\n",
        "  images, labels, preds = images[:M*N], labels[:M*N], preds[:M*N]\n",
        "  inv_norm = transforms.Normalize(\n",
        "      mean=(-0.4914/0.2023, -0.4822/0.1994, -0.4465/0.2010),\n",
        "      std=(1/0.2023, 1/0.1994, 1/0.2010))\n",
        "\n",
        "  num_images = images.shape[0]\n",
        "  fig, axes = plt.subplots(M, N, figsize=(16, 9))\n",
        "  fig.suptitle('Correct / Predicted Labels (Red text for incorrect ones)')\n",
        "\n",
        "  for i, ax in enumerate(fig.axes):\n",
        "    ax.axis('off')\n",
        "    if i >= num_images:\n",
        "      continue\n",
        "    img, label, prediction = images[i], labels[i], preds[i]\n",
        "    img = inv_norm(img)\n",
        "    img = img.permute(1, 2, 0) # (C, M, N) -> (M, N, C)\n",
        "    label, prediction = label.item(), prediction.item()\n",
        "    if label == prediction:\n",
        "      ax.set_title(u'\\u2713', color='blue', fontsize=22)\n",
        "    else:\n",
        "      ax.set_title(\n",
        "          'X {}/{}'.format(CIFAR10_LABELS[label],\n",
        "                          CIFAR10_LABELS[prediction]), color='red')\n",
        "    ax.imshow(img)\n",
        "  plt.savefig(RESULT_IMG_PATH, transparent=True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMdPRFXIn_jH"
      },
      "source": [
        "# Define Parameters\n",
        "FLAGS = {}\n",
        "FLAGS['data_dir'] = \"/tmp/cifar\"\n",
        "FLAGS['batch_size'] = 128\n",
        "FLAGS['num_workers'] = 4\n",
        "FLAGS['learning_rate'] = 0.02\n",
        "FLAGS['momentum'] = 0.9\n",
        "FLAGS['num_epochs'] = 20\n",
        "FLAGS['num_cores'] = 8 if os.environ.get('TPU_NAME', None) else 1\n",
        "FLAGS['log_steps'] = 20\n",
        "FLAGS['metrics_debug'] = False"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Micd3xZvoA-c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "796f9d28-199b-4df0-aff0-84726812186d"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.debug.metrics as met\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.utils.utils as xu\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, \n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        \n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion * planes:\n",
        "            self.shortcut = nn.Sequential(nn.Conv2d(in_planes,\n",
        "                                                    self.expansion * planes,\n",
        "                                                    kernel_size=1,stride=stride,\n",
        "                                                    bias=False),nn.BatchNorm2d(\n",
        "                                                        self.expansion * planes))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = torch.flatten(out, 1)\n",
        "        out = self.linear(out)\n",
        "        return F.log_softmax(out, dim=1)\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:TPU has started up successfully with version pytorch-1.7\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vMl96KLoCq8"
      },
      "source": [
        "SERIAL_EXEC = xmp.MpSerialExecutor()\n",
        "# Only instantiate model weights once in memory.\n",
        "WRAPPED_MODEL = xmp.MpModelWrapper(ResNet18())\n",
        "\n",
        "def train_resnet18():\n",
        "  torch.manual_seed(1)\n",
        "\n",
        "  def get_dataset():\n",
        "    norm = transforms.Normalize(\n",
        "        mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010))\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        norm,\n",
        "    ])\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        norm,\n",
        "    ])\n",
        "    train_dataset = datasets.CIFAR10(\n",
        "        root=FLAGS['data_dir'],\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=transform_train)\n",
        "    test_dataset = datasets.CIFAR10(\n",
        "        root=FLAGS['data_dir'],\n",
        "        train=False,\n",
        "        download=True,\n",
        "        transform=transform_test)\n",
        "    \n",
        "    return train_dataset, test_dataset\n",
        "  \n",
        "  # Using the serial executor avoids multiple processes\n",
        "  # to download the same data.\n",
        "  train_dataset, test_dataset = SERIAL_EXEC.run(get_dataset)\n",
        "\n",
        "  train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "      train_dataset,\n",
        "      num_replicas=xm.xrt_world_size(),\n",
        "      rank=xm.get_ordinal(),\n",
        "      shuffle=True)\n",
        "  train_loader = torch.utils.data.DataLoader(\n",
        "      train_dataset,\n",
        "      batch_size=FLAGS['batch_size'],\n",
        "      sampler=train_sampler,\n",
        "      num_workers=FLAGS['num_workers'],\n",
        "      drop_last=True)\n",
        "  test_loader = torch.utils.data.DataLoader(\n",
        "      test_dataset,\n",
        "      batch_size=FLAGS['batch_size'],\n",
        "      shuffle=False,\n",
        "      num_workers=FLAGS['num_workers'],\n",
        "      drop_last=True)\n",
        "\n",
        "  # Scale learning rate to num cores\n",
        "  learning_rate = FLAGS['learning_rate'] * xm.xrt_world_size()\n",
        "\n",
        "  # Get loss function, optimizer, and model\n",
        "  device = xm.xla_device()\n",
        "  model = WRAPPED_MODEL.to(device)\n",
        "  optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
        "                        momentum=FLAGS['momentum'], weight_decay=5e-4)\n",
        "  loss_fn = nn.NLLLoss()\n",
        "\n",
        "  def train_loop_fn(loader):\n",
        "    tracker = xm.RateTracker()\n",
        "    model.train()\n",
        "    for x, (data, target) in enumerate(loader):\n",
        "      optimizer.zero_grad()\n",
        "      output = model(data)\n",
        "      loss = loss_fn(output, target)\n",
        "      loss.backward()\n",
        "      xm.optimizer_step(optimizer)\n",
        "      tracker.add(FLAGS['batch_size'])\n",
        "      if x % FLAGS['log_steps'] == 0:\n",
        "        print('[xla:{}]({}) Loss={:.5f} Rate={:.2f} GlobalRate={:.2f} Time={}'.format(\n",
        "            xm.get_ordinal(), x, loss.item(), tracker.rate(),\n",
        "            tracker.global_rate(), time.asctime()), flush=True)\n",
        "\n",
        "  def test_loop_fn(loader):\n",
        "    total_samples = 0\n",
        "    correct = 0\n",
        "    model.eval()\n",
        "    data, pred, target = None, None, None\n",
        "    for data, target in loader:\n",
        "      output = model(data)\n",
        "      pred = output.max(1, keepdim=True)[1]\n",
        "      correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "      total_samples += data.size()[0]\n",
        "\n",
        "    accuracy = 100.0 * correct / total_samples\n",
        "    print('[xla:{}] Accuracy={:.2f}%'.format(\n",
        "        xm.get_ordinal(), accuracy), flush=True)\n",
        "    return accuracy, data, pred, target\n",
        "\n",
        "  # Train and eval loops\n",
        "  accuracy = 0.0\n",
        "  data, pred, target = None, None, None\n",
        "  for epoch in range(1, FLAGS['num_epochs'] + 1):\n",
        "    para_loader = pl.ParallelLoader(train_loader, [device])\n",
        "    train_loop_fn(para_loader.per_device_loader(device))\n",
        "    xm.master_print(\"Finished training epoch {}\".format(epoch))\n",
        "\n",
        "    para_loader = pl.ParallelLoader(test_loader, [device])\n",
        "    accuracy, data, pred, target  = test_loop_fn(para_loader.per_device_loader(device))\n",
        "    if FLAGS['metrics_debug']:\n",
        "      xm.master_print(met.metrics_report(), flush=True)\n",
        "\n",
        "  return accuracy, data, pred, target\n",
        "  "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2nL4HmloEyl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e9bb5ba3b01343be9f4e05dbb2a20843",
            "11a22407527c4c2281c4e805a15eb177",
            "2ae8e563a7494dbabd93d4efc2e9e22c",
            "8640cde766f34cd981a80293b56f4edb",
            "6cfd4503ec5846518bc0252231bea8f2",
            "e8fdc0bdbb4b481e87f17be5272a87d1",
            "23b76fa410384813b3611dcd38a6ab7d",
            "bc959d99f6a34205963ec7e49194c112"
          ]
        },
        "outputId": "3cb9fa60-d5e5-4fdb-8e2e-077bb953c8af"
      },
      "source": [
        "# Start training processes\n",
        "def _mp_fn(rank, flags):\n",
        "    global FLAGS\n",
        "    FLAGS = flags\n",
        "    torch.set_default_tensor_type('torch.FloatTensor')\n",
        "    accuracy, data, pred, target = train_resnet18()\n",
        "    if rank == 0:\n",
        "        # Retrieve tensors that are on TPU core 0 and plot.\n",
        "        plot_results(data.cpu(), pred.cpu(), target.cpu())\n",
        "\n",
        "xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=FLAGS['num_cores'],\n",
        "          start_method='fork')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /tmp/cifar/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e9bb5ba3b01343be9f4e05dbb2a20843",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /tmp/cifar/cifar-10-python.tar.gz to /tmp/cifar\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[xla:0](0) Loss=2.43216 Rate=51.77 GlobalRate=51.77 Time=Tue Nov 24 16:37:34 2020\n",
            "[xla:4](0) Loss=2.46078 Rate=292.38 GlobalRate=292.35 Time=Tue Nov 24 16:37:35 2020\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[xla:1](0) Loss=2.41751 Rate=286.31 GlobalRate=286.29 Time=Tue Nov 24 16:37:37 2020\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[xla:7](0) Loss=2.43003 Rate=301.39 GlobalRate=301.36 Time=Tue Nov 24 16:37:40 2020\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[xla:6](0) Loss=2.41625 Rate=258.41 GlobalRate=258.38 Time=Tue Nov 24 16:37:42 2020\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[xla:3](0) Loss=2.48649 Rate=297.02 GlobalRate=297.00 Time=Tue Nov 24 16:37:45 2020\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[xla:2](0) Loss=2.47883 Rate=302.84 GlobalRate=302.81 Time=Tue Nov 24 16:37:47 2020\n",
            "Files already downloaded and verified\n",
            "[xla:5](0) Loss=2.54401 Rate=296.43 GlobalRate=296.41 Time=Tue Nov 24 16:37:50 2020\n",
            "[xla:4](20) Loss=2.48030 Rate=174.35 GlobalRate=98.83 Time=Tue Nov 24 16:38:02 2020\n",
            "[xla:7](20) Loss=2.62733 Rate=190.87 GlobalRate=120.70 Time=Tue Nov 24 16:38:02 2020\n",
            "[xla:1](20) Loss=2.38850 Rate=177.83 GlobalRate=108.78 Time=Tue Nov 24 16:38:02 2020\n",
            "[xla:6](20) Loss=2.26636 Rate=182.79 GlobalRate=135.52 Time=Tue Nov 24 16:38:02 2020\n",
            "[xla:0](20) Loss=2.56322 Rate=77.15 GlobalRate=90.54 Time=Tue Nov 24 16:38:02 2020\n",
            "[xla:5](20) Loss=2.48770 Rate=246.21 GlobalRate=215.63 Time=Tue Nov 24 16:38:02 2020\n",
            "[xla:2](20) Loss=2.70874 Rate=226.92 GlobalRate=179.89 Time=Tue Nov 24 16:38:02 2020\n",
            "[xla:3](20) Loss=2.58539 Rate=210.77 GlobalRate=156.89 Time=Tue Nov 24 16:38:02 2020\n",
            "[xla:4](40) Loss=2.26481 Rate=745.07 GlobalRate=178.06 Time=Tue Nov 24 16:38:04 2020\n",
            "[xla:7](40) Loss=2.32198 Rate=750.46 GlobalRate=213.78 Time=Tue Nov 24 16:38:04 2020\n",
            "[xla:6](40) Loss=2.27544 Rate=747.27 GlobalRate=237.33 Time=Tue Nov 24 16:38:04 2020\n",
            "[xla:3](40) Loss=2.32415 Rate=759.53 GlobalRate=270.41 Time=Tue Nov 24 16:38:04 2020\n",
            "[xla:0](40) Loss=2.26122 Rate=704.99 GlobalRate=164.17 Time=Tue Nov 24 16:38:04 2020\n",
            "[xla:2](40) Loss=2.26071 Rate=764.94 GlobalRate=304.75 Time=Tue Nov 24 16:38:04 2020\n",
            "[xla:1](40) Loss=2.28462 Rate=744.94 GlobalRate=194.44 Time=Tue Nov 24 16:38:04 2020\n",
            "[xla:5](40) Loss=2.32314 Rate=771.62 GlobalRate=355.85 Time=Tue Nov 24 16:38:04 2020\n",
            "Finished training epoch 1\n",
            "[xla:4] Accuracy=12.98%\n",
            "[xla:5] Accuracy=13.31%\n",
            "[xla:0] Accuracy=13.09%\n",
            "[xla:3] Accuracy=13.20%\n",
            "[xla:6] Accuracy=13.19%\n",
            "[xla:1] Accuracy=13.11%\n",
            "[xla:7] Accuracy=12.78%\n",
            "[xla:2] Accuracy=13.19%\n",
            "[xla:3](0) Loss=2.29156 Rate=165.50 GlobalRate=165.50 Time=Tue Nov 24 16:38:17 2020\n",
            "[xla:1](0) Loss=2.26197 Rate=152.60 GlobalRate=152.59 Time=Tue Nov 24 16:38:17 2020\n",
            "[xla:0](0) Loss=2.30813 Rate=132.39 GlobalRate=132.38 Time=Tue Nov 24 16:38:17 2020\n",
            "[xla:5](0) Loss=2.23303 Rate=129.65 GlobalRate=129.65 Time=Tue Nov 24 16:38:18 2020\n",
            "[xla:6](0) Loss=2.24998 Rate=122.16 GlobalRate=122.16 Time=Tue Nov 24 16:38:18 2020\n",
            "[xla:4](0) Loss=2.24646 Rate=121.60 GlobalRate=121.60 Time=Tue Nov 24 16:38:18 2020\n",
            "[xla:7](0) Loss=2.35124 Rate=111.28 GlobalRate=111.27 Time=Tue Nov 24 16:38:18 2020\n",
            "[xla:2](0) Loss=2.25924 Rate=111.93 GlobalRate=111.93 Time=Tue Nov 24 16:38:18 2020\n",
            "[xla:6](20) Loss=2.20413 Rate=630.97 GlobalRate=729.14 Time=Tue Nov 24 16:38:20 2020\n",
            "[xla:5](20) Loss=2.14346 Rate=622.94 GlobalRate=731.05 Time=Tue Nov 24 16:38:20 2020\n",
            "[xla:3](20) Loss=2.20077 Rate=593.46 GlobalRate=729.12 Time=Tue Nov 24 16:38:20 2020\n",
            "[xla:0](20) Loss=2.25026 Rate=617.64 GlobalRate=729.05 Time=Tue Nov 24 16:38:20 2020\n",
            "[xla:4](20) Loss=2.18081 Rate=631.65 GlobalRate=729.01 Time=Tue Nov 24 16:38:20 2020\n",
            "[xla:1](20) Loss=2.18487 Rate=600.50 GlobalRate=729.22 Time=Tue Nov 24 16:38:20 2020\n",
            "[xla:7](20) Loss=2.19303 Rate=652.52 GlobalRate=731.11 Time=Tue Nov 24 16:38:20 2020\n",
            "[xla:2](20) Loss=2.19865 Rate=657.26 GlobalRate=736.16 Time=Tue Nov 24 16:38:20 2020\n",
            "[xla:0](40) Loss=1.97532 Rate=897.16 GlobalRate=867.48 Time=Tue Nov 24 16:38:23 2020\n",
            "[xla:7](40) Loss=2.01575 Rate=911.11 GlobalRate=868.97 Time=Tue Nov 24 16:38:23 2020\n",
            "[xla:3](40) Loss=2.05644 Rate=887.45 GlobalRate=867.52 Time=Tue Nov 24 16:38:23 2020\n",
            "[xla:1](40) Loss=2.01117 Rate=890.31 GlobalRate=867.61 Time=Tue Nov 24 16:38:23 2020\n",
            "[xla:5](40) Loss=2.02320 Rate=899.20 GlobalRate=868.89 Time=Tue Nov 24 16:38:23 2020\n",
            "[xla:6](40) Loss=2.02217 Rate=902.38 GlobalRate=867.49 Time=Tue Nov 24 16:38:23 2020\n",
            "[xla:4](40) Loss=1.97264 Rate=901.78 GlobalRate=866.94 Time=Tue Nov 24 16:38:23 2020\n",
            "[xla:2](40) Loss=2.00211 Rate=911.91 GlobalRate=872.04 Time=Tue Nov 24 16:38:23 2020\n",
            "Finished training epoch 2\n",
            "[xla:3] Accuracy=23.15%\n",
            "[xla:6] Accuracy=23.20%\n",
            "[xla:1] Accuracy=23.33%\n",
            "[xla:0] Accuracy=23.49%\n",
            "[xla:5] Accuracy=23.17%\n",
            "[xla:4] Accuracy=23.30%\n",
            "[xla:7] Accuracy=23.50%\n",
            "[xla:2] Accuracy=23.31%\n",
            "[xla:6](0) Loss=1.97629 Rate=219.13 GlobalRate=219.12 Time=Tue Nov 24 16:38:33 2020\n",
            "[xla:0](0) Loss=2.02302 Rate=160.33 GlobalRate=160.32 Time=Tue Nov 24 16:38:33 2020\n",
            "[xla:3](0) Loss=1.98718 Rate=153.16 GlobalRate=153.15 Time=Tue Nov 24 16:38:33 2020\n",
            "[xla:1](0) Loss=2.01903 Rate=151.12 GlobalRate=151.11 Time=Tue Nov 24 16:38:33 2020\n",
            "[xla:5](0) Loss=1.98002 Rate=142.77 GlobalRate=142.77 Time=Tue Nov 24 16:38:33 2020\n",
            "[xla:4](0) Loss=1.88593 Rate=134.88 GlobalRate=134.87 Time=Tue Nov 24 16:38:33 2020\n",
            "[xla:7](0) Loss=2.18112 Rate=131.84 GlobalRate=131.83 Time=Tue Nov 24 16:38:33 2020\n",
            "[xla:2](0) Loss=1.95938 Rate=133.51 GlobalRate=133.50 Time=Tue Nov 24 16:38:33 2020\n",
            "[xla:7](20) Loss=1.98991 Rate=709.27 GlobalRate=811.97 Time=Tue Nov 24 16:38:36 2020\n",
            "[xla:3](20) Loss=1.86763 Rate=650.16 GlobalRate=780.48 Time=Tue Nov 24 16:38:36 2020\n",
            "[xla:1](20) Loss=2.17335 Rate=658.18 GlobalRate=786.71 Time=Tue Nov 24 16:38:36 2020\n",
            "[xla:0](20) Loss=2.01144 Rate=650.77 GlobalRate=786.72 Time=Tue Nov 24 16:38:36 2020\n",
            "[xla:5](20) Loss=1.97054 Rate=666.62 GlobalRate=786.75 Time=Tue Nov 24 16:38:36 2020\n",
            "[xla:6](20) Loss=1.94801 Rate=624.68 GlobalRate=780.41 Time=Tue Nov 24 16:38:36 2020\n",
            "[xla:2](20) Loss=2.02244 Rate=712.88 GlobalRate=817.55 Time=Tue Nov 24 16:38:36 2020\n",
            "[xla:4](20) Loss=2.00505 Rate=688.92 GlobalRate=798.09 Time=Tue Nov 24 16:38:36 2020\n",
            "[xla:0](40) Loss=1.87902 Rate=922.60 GlobalRate=914.94 Time=Tue Nov 24 16:38:38 2020\n",
            "[xla:1](40) Loss=1.91555 Rate=925.59 GlobalRate=914.94 Time=Tue Nov 24 16:38:38 2020\n",
            "[xla:2](40) Loss=1.89796 Rate=948.42 GlobalRate=936.53 Time=Tue Nov 24 16:38:38 2020\n",
            "[xla:4](40) Loss=1.87617 Rate=938.80 GlobalRate=923.30 Time=Tue Nov 24 16:38:38 2020\n",
            "[xla:3](40) Loss=1.99834 Rate=922.36 GlobalRate=910.60 Time=Tue Nov 24 16:38:38 2020\n",
            "[xla:6](40) Loss=1.95059 Rate=912.28 GlobalRate=910.61 Time=Tue Nov 24 16:38:38 2020\n",
            "[xla:5](40) Loss=1.91541 Rate=929.04 GlobalRate=915.01 Time=Tue Nov 24 16:38:38 2020\n",
            "[xla:7](40) Loss=1.89258 Rate=945.95 GlobalRate=932.17 Time=Tue Nov 24 16:38:38 2020\n",
            "Finished training epoch 3\n",
            "[xla:6] Accuracy=26.74%\n",
            "[xla:0] Accuracy=26.61%\n",
            "[xla:4] Accuracy=26.55%\n",
            "[xla:3] Accuracy=26.69%\n",
            "[xla:5] Accuracy=26.87%\n",
            "[xla:1] Accuracy=26.69%\n",
            "[xla:7] Accuracy=26.36%\n",
            "[xla:2] Accuracy=26.75%\n",
            "[xla:6](0) Loss=1.81030 Rate=238.09 GlobalRate=238.06 Time=Tue Nov 24 16:38:48 2020\n",
            "[xla:5](0) Loss=1.89024 Rate=192.49 GlobalRate=192.49 Time=Tue Nov 24 16:38:48 2020\n",
            "[xla:4](0) Loss=1.86756 Rate=144.61 GlobalRate=144.61 Time=Tue Nov 24 16:38:48 2020\n",
            "[xla:3](0) Loss=1.87221 Rate=144.63 GlobalRate=144.63 Time=Tue Nov 24 16:38:48 2020\n",
            "[xla:1](0) Loss=1.88109 Rate=135.55 GlobalRate=135.54 Time=Tue Nov 24 16:38:48 2020\n",
            "[xla:0](0) Loss=1.90501 Rate=134.15 GlobalRate=134.14 Time=Tue Nov 24 16:38:48 2020\n",
            "[xla:2](0) Loss=1.87428 Rate=122.27 GlobalRate=122.26 Time=Tue Nov 24 16:38:48 2020\n",
            "[xla:7](0) Loss=1.97920 Rate=122.13 GlobalRate=122.12 Time=Tue Nov 24 16:38:48 2020\n",
            "[xla:5](20) Loss=1.90606 Rate=621.44 GlobalRate=771.04 Time=Tue Nov 24 16:38:51 2020\n",
            "[xla:1](20) Loss=2.12206 Rate=658.59 GlobalRate=771.13 Time=Tue Nov 24 16:38:51 2020\n",
            "[xla:3](20) Loss=1.77309 Rate=648.36 GlobalRate=771.05 Time=Tue Nov 24 16:38:51 2020\n",
            "[xla:4](20) Loss=1.93879 Rate=648.37 GlobalRate=771.04 Time=Tue Nov 24 16:38:51 2020\n",
            "[xla:2](20) Loss=1.88525 Rate=686.67 GlobalRate=777.94 Time=Tue Nov 24 16:38:51 2020\n",
            "[xla:7](20) Loss=1.90403 Rate=693.76 GlobalRate=783.71 Time=Tue Nov 24 16:38:51 2020\n",
            "[xla:0](20) Loss=1.88856 Rate=663.35 GlobalRate=773.86 Time=Tue Nov 24 16:38:51 2020\n",
            "[xla:6](20) Loss=1.86201 Rate=610.88 GlobalRate=764.41 Time=Tue Nov 24 16:38:51 2020\n",
            "[xla:0](40) Loss=1.76686 Rate=879.04 GlobalRate=878.13 Time=Tue Nov 24 16:38:53 2020\n",
            "[xla:3](40) Loss=1.94019 Rate=872.37 GlobalRate=875.87 Time=Tue Nov 24 16:38:53 2020\n",
            "[xla:7](40) Loss=1.72727 Rate=890.54 GlobalRate=884.19 Time=Tue Nov 24 16:38:53 2020\n",
            "[xla:5](40) Loss=1.84396 Rate=861.62 GlobalRate=875.87 Time=Tue Nov 24 16:38:53 2020\n",
            "[xla:1](40) Loss=1.83948 Rate=876.49 GlobalRate=875.94 Time=Tue Nov 24 16:38:53 2020\n",
            "[xla:2](40) Loss=1.87182 Rate=887.71 GlobalRate=880.41 Time=Tue Nov 24 16:38:53 2020\n",
            "[xla:6](40) Loss=1.85827 Rate=858.25 GlobalRate=871.98 Time=Tue Nov 24 16:38:53 2020\n",
            "[xla:4](40) Loss=1.83712 Rate=872.31 GlobalRate=875.82 Time=Tue Nov 24 16:38:53 2020\n",
            "Finished training epoch 4\n",
            "[xla:0] Accuracy=27.77%\n",
            "[xla:1] Accuracy=27.51%\n",
            "[xla:6] Accuracy=26.91%\n",
            "[xla:2] Accuracy=27.97%\n",
            "[xla:7] Accuracy=27.95%\n",
            "[xla:5] Accuracy=27.37%\n",
            "[xla:4] Accuracy=27.58%\n",
            "[xla:3] Accuracy=27.18%\n",
            "[xla:0](0) Loss=1.75338 Rate=232.49 GlobalRate=232.48 Time=Tue Nov 24 16:39:04 2020\n",
            "[xla:7](0) Loss=1.87640 Rate=170.81 GlobalRate=170.81 Time=Tue Nov 24 16:39:04 2020\n",
            "[xla:1](0) Loss=1.77980 Rate=164.48 GlobalRate=164.48 Time=Tue Nov 24 16:39:04 2020\n",
            "[xla:6](0) Loss=1.69021 Rate=139.10 GlobalRate=139.09 Time=Tue Nov 24 16:39:04 2020\n",
            "[xla:5](0) Loss=1.74160 Rate=134.74 GlobalRate=134.73 Time=Tue Nov 24 16:39:04 2020\n",
            "[xla:4](0) Loss=1.76582 Rate=128.74 GlobalRate=128.74 Time=Tue Nov 24 16:39:04 2020\n",
            "[xla:3](0) Loss=1.85283 Rate=125.50 GlobalRate=125.50 Time=Tue Nov 24 16:39:04 2020\n",
            "[xla:2](0) Loss=1.73837 Rate=118.15 GlobalRate=118.14 Time=Tue Nov 24 16:39:04 2020\n",
            "[xla:0](20) Loss=1.75020 Rate=611.17 GlobalRate=764.76 Time=Tue Nov 24 16:39:07 2020\n",
            "[xla:1](20) Loss=1.95504 Rate=632.81 GlobalRate=770.84 Time=Tue Nov 24 16:39:07 2020\n",
            "[xla:4](20) Loss=1.86727 Rate=667.72 GlobalRate=770.90 Time=Tue Nov 24 16:39:07 2020\n",
            "[xla:3](20) Loss=1.66766 Rate=680.26 GlobalRate=777.37 Time=Tue Nov 24 16:39:07 2020\n",
            "[xla:6](20) Loss=1.67439 Rate=654.08 GlobalRate=770.88 Time=Tue Nov 24 16:39:07 2020\n",
            "[xla:7](20) Loss=1.86560 Rate=629.40 GlobalRate=770.87 Time=Tue Nov 24 16:39:07 2020\n",
            "[xla:2](20) Loss=1.73380 Rate=686.27 GlobalRate=770.84 Time=Tue Nov 24 16:39:07 2020\n",
            "[xla:5](20) Loss=1.78554 Rate=659.35 GlobalRate=770.88 Time=Tue Nov 24 16:39:07 2020\n",
            "[xla:4](40) Loss=1.66221 Rate=923.14 GlobalRate=900.46 Time=Tue Nov 24 16:39:09 2020\n",
            "[xla:7](40) Loss=1.67374 Rate=907.81 GlobalRate=900.44 Time=Tue Nov 24 16:39:09 2020\n",
            "[xla:2](40) Loss=1.75853 Rate=930.56 GlobalRate=900.42 Time=Tue Nov 24 16:39:09 2020\n",
            "[xla:5](40) Loss=1.67993 Rate=919.83 GlobalRate=900.47 Time=Tue Nov 24 16:39:09 2020\n",
            "[xla:3](40) Loss=1.84805 Rate=928.12 GlobalRate=904.95 Time=Tue Nov 24 16:39:09 2020\n",
            "[xla:1](40) Loss=1.69050 Rate=909.22 GlobalRate=900.44 Time=Tue Nov 24 16:39:09 2020\n",
            "[xla:6](40) Loss=1.74540 Rate=917.71 GlobalRate=900.47 Time=Tue Nov 24 16:39:09 2020\n",
            "[xla:0](40) Loss=1.65148 Rate=900.54 GlobalRate=896.17 Time=Tue Nov 24 16:39:09 2020\n",
            "Finished training epoch 5\n",
            "[xla:5] Accuracy=35.47%\n",
            "[xla:6] Accuracy=35.49%\n",
            "[xla:2] Accuracy=35.43%\n",
            "[xla:4] Accuracy=35.70%\n",
            "[xla:0] Accuracy=35.91%\n",
            "[xla:1] Accuracy=35.59%\n",
            "[xla:3] Accuracy=35.31%\n",
            "[xla:7] Accuracy=35.76%\n",
            "[xla:5](0) Loss=1.55892 Rate=182.82 GlobalRate=182.81 Time=Tue Nov 24 16:39:19 2020\n",
            "[xla:1](0) Loss=1.65227 Rate=156.81 GlobalRate=156.80 Time=Tue Nov 24 16:39:19 2020\n",
            "[xla:6](0) Loss=1.63832 Rate=140.37 GlobalRate=140.36 Time=Tue Nov 24 16:39:19 2020\n",
            "[xla:4](0) Loss=1.69114 Rate=133.56 GlobalRate=133.56 Time=Tue Nov 24 16:39:19 2020\n",
            "[xla:3](0) Loss=1.75438 Rate=130.01 GlobalRate=130.01 Time=Tue Nov 24 16:39:19 2020\n",
            "[xla:2](0) Loss=1.60213 Rate=124.62 GlobalRate=124.62 Time=Tue Nov 24 16:39:19 2020\n",
            "[xla:0](0) Loss=1.66480 Rate=123.53 GlobalRate=123.52 Time=Tue Nov 24 16:39:19 2020\n",
            "[xla:7](0) Loss=1.76746 Rate=117.30 GlobalRate=117.30 Time=Tue Nov 24 16:39:19 2020\n",
            "[xla:5](20) Loss=1.68175 Rate=624.84 GlobalRate=771.48 Time=Tue Nov 24 16:39:21 2020\n",
            "[xla:2](20) Loss=1.66918 Rate=684.28 GlobalRate=779.54 Time=Tue Nov 24 16:39:21 2020\n",
            "[xla:7](20) Loss=1.80523 Rate=720.64 GlobalRate=797.36 Time=Tue Nov 24 16:39:21 2020\n",
            "[xla:6](20) Loss=1.59448 Rate=653.28 GlobalRate=771.48 Time=Tue Nov 24 16:39:21 2020\n",
            "[xla:0](20) Loss=1.60983 Rate=683.91 GlobalRate=777.54 Time=Tue Nov 24 16:39:21 2020\n",
            "[xla:3](20) Loss=1.56329 Rate=673.29 GlobalRate=777.59 Time=Tue Nov 24 16:39:21 2020\n",
            "[xla:4](20) Loss=1.67557 Rate=668.10 GlobalRate=777.49 Time=Tue Nov 24 16:39:21 2020\n",
            "[xla:1](20) Loss=1.85487 Rate=644.42 GlobalRate=777.59 Time=Tue Nov 24 16:39:21 2020\n",
            "[xla:5](40) Loss=1.56322 Rate=941.14 GlobalRate=919.66 Time=Tue Nov 24 16:39:24 2020\n",
            "[xla:7](40) Loss=1.53592 Rate=979.40 GlobalRate=938.23 Time=Tue Nov 24 16:39:24 2020\n",
            "[xla:4](40) Loss=1.50642 Rate=958.41 GlobalRate=924.01 Time=Tue Nov 24 16:39:24 2020\n",
            "[xla:0](40) Loss=1.45669 Rate=964.74 GlobalRate=924.05 Time=Tue Nov 24 16:39:24 2020\n",
            "[xla:2](40) Loss=1.67725 Rate=964.84 GlobalRate=925.47 Time=Tue Nov 24 16:39:24 2020\n",
            "[xla:6](40) Loss=1.67734 Rate=952.43 GlobalRate=919.62 Time=Tue Nov 24 16:39:24 2020\n",
            "[xla:3](40) Loss=1.69200 Rate=960.54 GlobalRate=924.11 Time=Tue Nov 24 16:39:24 2020\n",
            "[xla:1](40) Loss=1.50023 Rate=948.82 GlobalRate=924.02 Time=Tue Nov 24 16:39:24 2020\n",
            "Finished training epoch 6\n",
            "[xla:5] Accuracy=40.09%\n",
            "[xla:3] Accuracy=39.91%\n",
            "[xla:4] Accuracy=40.45%\n",
            "[xla:6] Accuracy=39.97%\n",
            "[xla:2] Accuracy=39.97%\n",
            "[xla:1] Accuracy=39.84%\n",
            "[xla:7] Accuracy=39.50%\n",
            "[xla:0] Accuracy=40.30%\n",
            "[xla:3](0) Loss=1.56564 Rate=214.54 GlobalRate=214.52 Time=Tue Nov 24 16:39:33 2020\n",
            "[xla:1](0) Loss=1.61391 Rate=171.44 GlobalRate=171.44 Time=Tue Nov 24 16:39:34 2020\n",
            "[xla:5](0) Loss=1.44307 Rate=148.06 GlobalRate=148.05 Time=Tue Nov 24 16:39:34 2020\n",
            "[xla:4](0) Loss=1.60269 Rate=148.05 GlobalRate=148.04 Time=Tue Nov 24 16:39:34 2020\n",
            "[xla:6](0) Loss=1.55112 Rate=138.92 GlobalRate=138.92 Time=Tue Nov 24 16:39:34 2020\n",
            "[xla:2](0) Loss=1.59191 Rate=130.47 GlobalRate=130.46 Time=Tue Nov 24 16:39:34 2020\n",
            "[xla:0](0) Loss=1.56134 Rate=132.92 GlobalRate=132.92 Time=Tue Nov 24 16:39:34 2020\n",
            "[xla:7](0) Loss=1.67938 Rate=120.88 GlobalRate=120.87 Time=Tue Nov 24 16:39:34 2020\n",
            "[xla:5](20) Loss=1.58461 Rate=593.26 GlobalRate=718.58 Time=Tue Nov 24 16:39:37 2020\n",
            "[xla:1](20) Loss=1.86214 Rate=586.47 GlobalRate=724.05 Time=Tue Nov 24 16:39:37 2020\n",
            "[xla:6](20) Loss=1.43710 Rate=607.27 GlobalRate=725.41 Time=Tue Nov 24 16:39:37 2020\n",
            "[xla:2](20) Loss=1.54108 Rate=614.41 GlobalRate=723.92 Time=Tue Nov 24 16:39:37 2020\n",
            "[xla:4](20) Loss=1.70518 Rate=593.27 GlobalRate=718.57 Time=Tue Nov 24 16:39:37 2020\n",
            "[xla:0](20) Loss=1.46401 Rate=627.85 GlobalRate=739.31 Time=Tue Nov 24 16:39:37 2020\n",
            "[xla:3](20) Loss=1.52186 Rate=574.37 GlobalRate=718.60 Time=Tue Nov 24 16:39:37 2020\n",
            "[xla:7](20) Loss=1.75937 Rate=644.01 GlobalRate=738.95 Time=Tue Nov 24 16:39:37 2020\n",
            "[xla:0](40) Loss=1.41612 Rate=940.23 GlobalRate=894.83 Time=Tue Nov 24 16:39:39 2020\n",
            "[xla:1](40) Loss=1.50999 Rate=923.65 GlobalRate=883.27 Time=Tue Nov 24 16:39:39 2020\n",
            "[xla:4](40) Loss=1.46771 Rate=926.36 GlobalRate=879.08 Time=Tue Nov 24 16:39:39 2020\n",
            "[xla:7](40) Loss=1.46512 Rate=950.64 GlobalRate=896.49 Time=Tue Nov 24 16:39:39 2020\n",
            "[xla:5](40) Loss=1.55143 Rate=926.33 GlobalRate=879.07 Time=Tue Nov 24 16:39:39 2020\n",
            "[xla:6](40) Loss=1.61815 Rate=931.97 GlobalRate=884.30 Time=Tue Nov 24 16:39:39 2020\n",
            "[xla:2](40) Loss=1.53878 Rate=934.83 GlobalRate=883.17 Time=Tue Nov 24 16:39:39 2020\n",
            "[xla:3](40) Loss=1.63906 Rate=918.81 GlobalRate=879.10 Time=Tue Nov 24 16:39:39 2020\n",
            "Finished training epoch 7\n",
            "[xla:5] Accuracy=42.83%\n",
            "[xla:1] Accuracy=43.06%\n",
            "[xla:3] Accuracy=42.89%\n",
            "[xla:4] Accuracy=43.23%\n",
            "[xla:2] Accuracy=42.85%\n",
            "[xla:6] Accuracy=42.86%\n",
            "[xla:7] Accuracy=43.00%\n",
            "[xla:0] Accuracy=43.36%\n",
            "[xla:3](0) Loss=1.43295 Rate=205.36 GlobalRate=205.35 Time=Tue Nov 24 16:39:49 2020\n",
            "[xla:5](0) Loss=1.42191 Rate=175.16 GlobalRate=175.15 Time=Tue Nov 24 16:39:49 2020\n",
            "[xla:1](0) Loss=1.57657 Rate=158.65 GlobalRate=158.64 Time=Tue Nov 24 16:39:49 2020\n",
            "[xla:2](0) Loss=1.52757 Rate=152.89 GlobalRate=152.88 Time=Tue Nov 24 16:39:49 2020\n",
            "[xla:4](0) Loss=1.53572 Rate=137.05 GlobalRate=137.04 Time=Tue Nov 24 16:39:49 2020\n",
            "[xla:6](0) Loss=1.53321 Rate=131.81 GlobalRate=131.81 Time=Tue Nov 24 16:39:49 2020\n",
            "[xla:0](0) Loss=1.43894 Rate=128.57 GlobalRate=128.57 Time=Tue Nov 24 16:39:49 2020\n",
            "[xla:7](0) Loss=1.56870 Rate=117.11 GlobalRate=117.11 Time=Tue Nov 24 16:39:49 2020\n",
            "[xla:1](20) Loss=1.67986 Rate=672.51 GlobalRate=807.50 Time=Tue Nov 24 16:39:51 2020\n",
            "[xla:3](20) Loss=1.48896 Rate=650.00 GlobalRate=807.64 Time=Tue Nov 24 16:39:51 2020\n",
            "[xla:4](20) Loss=1.54494 Rate=704.00 GlobalRate=814.53 Time=Tue Nov 24 16:39:51 2020\n",
            "[xla:6](20) Loss=1.36510 Rate=711.79 GlobalRate=814.13 Time=Tue Nov 24 16:39:51 2020\n",
            "[xla:2](20) Loss=1.52366 Rate=685.36 GlobalRate=815.05 Time=Tue Nov 24 16:39:51 2020\n",
            "[xla:5](20) Loss=1.48724 Rate=661.29 GlobalRate=807.50 Time=Tue Nov 24 16:39:51 2020\n",
            "[xla:7](20) Loss=1.66082 Rate=742.29 GlobalRate=814.13 Time=Tue Nov 24 16:39:51 2020\n",
            "[xla:0](20) Loss=1.38040 Rate=745.71 GlobalRate=837.93 Time=Tue Nov 24 16:39:51 2020\n",
            "[xla:2](40) Loss=1.50431 Rate=960.55 GlobalRate=948.03 Time=Tue Nov 24 16:39:54 2020\n",
            "[xla:6](40) Loss=1.53616 Rate=971.08 GlobalRate=947.37 Time=Tue Nov 24 16:39:54 2020\n",
            "[xla:0](40) Loss=1.32066 Rate=984.62 GlobalRate=963.67 Time=Tue Nov 24 16:39:54 2020\n",
            "[xla:1](40) Loss=1.37912 Rate=955.30 GlobalRate=942.72 Time=Tue Nov 24 16:39:54 2020\n",
            "[xla:4](40) Loss=1.42880 Rate=967.86 GlobalRate=947.59 Time=Tue Nov 24 16:39:54 2020\n",
            "[xla:7](40) Loss=1.47803 Rate=983.26 GlobalRate=947.36 Time=Tue Nov 24 16:39:54 2020\n",
            "[xla:3](40) Loss=1.60380 Rate=946.27 GlobalRate=942.79 Time=Tue Nov 24 16:39:54 2020\n",
            "[xla:5](40) Loss=1.45055 Rate=950.78 GlobalRate=942.70 Time=Tue Nov 24 16:39:54 2020\n",
            "Finished training epoch 8\n",
            "[xla:3] Accuracy=46.80%\n",
            "[xla:1] Accuracy=47.23%\n",
            "[xla:5] Accuracy=47.03%\n",
            "[xla:4] Accuracy=46.97%\n",
            "[xla:6] Accuracy=46.95%\n",
            "[xla:2] Accuracy=47.21%\n",
            "[xla:7] Accuracy=47.43%\n",
            "[xla:0] Accuracy=47.37%\n",
            "[xla:3](0) Loss=1.32906 Rate=156.93 GlobalRate=156.92 Time=Tue Nov 24 16:40:04 2020\n",
            "[xla:1](0) Loss=1.35507 Rate=156.90 GlobalRate=156.89 Time=Tue Nov 24 16:40:04 2020\n",
            "[xla:4](0) Loss=1.43466 Rate=149.94 GlobalRate=149.93 Time=Tue Nov 24 16:40:04 2020\n",
            "[xla:2](0) Loss=1.36684 Rate=139.69 GlobalRate=139.68 Time=Tue Nov 24 16:40:04 2020\n",
            "[xla:5](0) Loss=1.31338 Rate=135.31 GlobalRate=135.31 Time=Tue Nov 24 16:40:04 2020\n",
            "[xla:6](0) Loss=1.43400 Rate=126.71 GlobalRate=126.71 Time=Tue Nov 24 16:40:04 2020\n",
            "[xla:0](0) Loss=1.37421 Rate=127.06 GlobalRate=127.05 Time=Tue Nov 24 16:40:04 2020\n",
            "[xla:7](0) Loss=1.44624 Rate=121.63 GlobalRate=121.63 Time=Tue Nov 24 16:40:04 2020\n",
            "[xla:0](20) Loss=1.38455 Rate=687.61 GlobalRate=786.07 Time=Tue Nov 24 16:40:06 2020\n",
            "[xla:2](20) Loss=1.40172 Rate=649.23 GlobalRate=766.90 Time=Tue Nov 24 16:40:06 2020\n",
            "[xla:7](20) Loss=1.59851 Rate=698.08 GlobalRate=786.53 Time=Tue Nov 24 16:40:06 2020\n",
            "[xla:5](20) Loss=1.38522 Rate=647.55 GlobalRate=760.53 Time=Tue Nov 24 16:40:06 2020\n",
            "[xla:1](20) Loss=1.66410 Rate=627.67 GlobalRate=760.43 Time=Tue Nov 24 16:40:06 2020\n",
            "[xla:6](20) Loss=1.35319 Rate=665.90 GlobalRate=766.49 Time=Tue Nov 24 16:40:06 2020\n",
            "[xla:4](20) Loss=1.52406 Rate=632.95 GlobalRate=760.52 Time=Tue Nov 24 16:40:06 2020\n",
            "[xla:3](20) Loss=1.43976 Rate=627.22 GlobalRate=759.98 Time=Tue Nov 24 16:40:06 2020\n",
            "[xla:1](40) Loss=1.31804 Rate=870.99 GlobalRate=872.84 Time=Tue Nov 24 16:40:09 2020\n",
            "[xla:2](40) Loss=1.50047 Rate=879.59 GlobalRate=877.17 Time=Tue Nov 24 16:40:09 2020\n",
            "[xla:7](40) Loss=1.39020 Rate=899.09 GlobalRate=890.17 Time=Tue Nov 24 16:40:09 2020\n",
            "[xla:0](40) Loss=1.31447 Rate=894.90 GlobalRate=889.86 Time=Tue Nov 24 16:40:09 2020\n",
            "[xla:6](40) Loss=1.47204 Rate=886.21 GlobalRate=876.88 Time=Tue Nov 24 16:40:09 2020\n",
            "[xla:4](40) Loss=1.33174 Rate=873.05 GlobalRate=872.87 Time=Tue Nov 24 16:40:09 2020\n",
            "[xla:3](40) Loss=1.48902 Rate=871.30 GlobalRate=872.82 Time=Tue Nov 24 16:40:09 2020\n",
            "[xla:5](40) Loss=1.40873 Rate=878.83 GlobalRate=872.85 Time=Tue Nov 24 16:40:09 2020\n",
            "Finished training epoch 9\n",
            "[xla:4] Accuracy=48.70%\n",
            "[xla:5] Accuracy=48.63%\n",
            "[xla:2] Accuracy=48.67%\n",
            "[xla:1] Accuracy=48.59%\n",
            "[xla:6] Accuracy=48.57%\n",
            "[xla:3] Accuracy=48.52%\n",
            "[xla:0] Accuracy=48.82%\n",
            "[xla:7] Accuracy=48.36%\n",
            "[xla:4](0) Loss=1.27946 Rate=265.72 GlobalRate=265.70 Time=Tue Nov 24 16:40:19 2020\n",
            "[xla:3](0) Loss=1.19961 Rate=153.68 GlobalRate=153.67 Time=Tue Nov 24 16:40:19 2020\n",
            "[xla:6](0) Loss=1.37622 Rate=141.28 GlobalRate=141.27 Time=Tue Nov 24 16:40:19 2020\n",
            "[xla:5](0) Loss=1.27657 Rate=135.91 GlobalRate=135.91 Time=Tue Nov 24 16:40:19 2020\n",
            "[xla:1](0) Loss=1.33858 Rate=135.95 GlobalRate=135.95 Time=Tue Nov 24 16:40:19 2020\n",
            "[xla:2](0) Loss=1.27016 Rate=128.80 GlobalRate=128.79 Time=Tue Nov 24 16:40:19 2020\n",
            "[xla:0](0) Loss=1.26044 Rate=126.33 GlobalRate=126.32 Time=Tue Nov 24 16:40:19 2020\n",
            "[xla:7](0) Loss=1.42028 Rate=125.05 GlobalRate=125.04 Time=Tue Nov 24 16:40:20 2020\n",
            "[xla:3](20) Loss=1.30859 Rate=625.52 GlobalRate=755.89 Time=Tue Nov 24 16:40:22 2020\n",
            "[xla:7](20) Loss=1.49014 Rate=686.25 GlobalRate=781.88 Time=Tue Nov 24 16:40:22 2020\n",
            "[xla:4](20) Loss=1.45909 Rate=601.15 GlobalRate=749.67 Time=Tue Nov 24 16:40:22 2020\n",
            "[xla:6](20) Loss=1.24660 Rate=635.89 GlobalRate=755.67 Time=Tue Nov 24 16:40:22 2020\n",
            "[xla:1](20) Loss=1.53843 Rate=641.43 GlobalRate=755.48 Time=Tue Nov 24 16:40:22 2020\n",
            "[xla:0](20) Loss=1.30326 Rate=660.86 GlobalRate=761.49 Time=Tue Nov 24 16:40:22 2020\n",
            "[xla:5](20) Loss=1.27648 Rate=641.03 GlobalRate=755.05 Time=Tue Nov 24 16:40:22 2020\n",
            "[xla:2](20) Loss=1.25522 Rate=652.13 GlobalRate=756.92 Time=Tue Nov 24 16:40:22 2020\n",
            "[xla:5](40) Loss=1.35478 Rate=903.88 GlobalRate=884.64 Time=Tue Nov 24 16:40:24 2020\n",
            "[xla:2](40) Loss=1.35172 Rate=908.36 GlobalRate=885.98 Time=Tue Nov 24 16:40:24 2020\n",
            "[xla:1](40) Loss=1.28119 Rate=903.50 GlobalRate=884.65 Time=Tue Nov 24 16:40:24 2020\n",
            "[xla:6](40) Loss=1.29116 Rate=901.16 GlobalRate=884.71 Time=Tue Nov 24 16:40:24 2020\n",
            "[xla:0](40) Loss=1.16497 Rate=911.48 GlobalRate=888.97 Time=Tue Nov 24 16:40:24 2020\n",
            "[xla:4](40) Loss=1.29384 Rate=887.24 GlobalRate=880.47 Time=Tue Nov 24 16:40:24 2020\n",
            "[xla:7](40) Loss=1.32960 Rate=921.23 GlobalRate=902.82 Time=Tue Nov 24 16:40:24 2020\n",
            "[xla:3](40) Loss=1.37240 Rate=896.57 GlobalRate=884.62 Time=Tue Nov 24 16:40:24 2020\n",
            "Finished training epoch 10\n",
            "[xla:3] Accuracy=48.31%\n",
            "[xla:2] Accuracy=48.84%\n",
            "[xla:4] Accuracy=48.69%\n",
            "[xla:1] Accuracy=48.89%\n",
            "[xla:5] Accuracy=48.36%\n",
            "[xla:0] Accuracy=49.02%\n",
            "[xla:6] Accuracy=48.52%\n",
            "[xla:7] Accuracy=48.81%\n",
            "[xla:3](0) Loss=1.24344 Rate=209.73 GlobalRate=209.72 Time=Tue Nov 24 16:40:34 2020\n",
            "[xla:2](0) Loss=1.15476 Rate=194.65 GlobalRate=194.65 Time=Tue Nov 24 16:40:34 2020\n",
            "[xla:4](0) Loss=1.30364 Rate=171.20 GlobalRate=171.19 Time=Tue Nov 24 16:40:34 2020\n",
            "[xla:5](0) Loss=1.20727 Rate=145.32 GlobalRate=145.32 Time=Tue Nov 24 16:40:34 2020\n",
            "[xla:0](0) Loss=1.18344 Rate=139.90 GlobalRate=139.89 Time=Tue Nov 24 16:40:34 2020\n",
            "[xla:1](0) Loss=1.24553 Rate=138.89 GlobalRate=138.89 Time=Tue Nov 24 16:40:34 2020\n",
            "[xla:6](0) Loss=1.37993 Rate=131.75 GlobalRate=131.75 Time=Tue Nov 24 16:40:34 2020\n",
            "[xla:7](0) Loss=1.31618 Rate=124.57 GlobalRate=124.57 Time=Tue Nov 24 16:40:35 2020\n",
            "[xla:0](20) Loss=1.13374 Rate=677.36 GlobalRate=793.67 Time=Tue Nov 24 16:40:37 2020\n",
            "[xla:1](20) Loss=1.45864 Rate=678.61 GlobalRate=793.65 Time=Tue Nov 24 16:40:37 2020\n",
            "[xla:2](20) Loss=1.20786 Rate=628.13 GlobalRate=779.36 Time=Tue Nov 24 16:40:37 2020\n",
            "[xla:3](20) Loss=1.19195 Rate=625.00 GlobalRate=779.37 Time=Tue Nov 24 16:40:37 2020\n",
            "[xla:5](20) Loss=1.24129 Rate=671.03 GlobalRate=793.64 Time=Tue Nov 24 16:40:37 2020\n",
            "[xla:4](20) Loss=1.39849 Rate=642.90 GlobalRate=785.58 Time=Tue Nov 24 16:40:37 2020\n",
            "[xla:6](20) Loss=1.15144 Rate=695.09 GlobalRate=799.38 Time=Tue Nov 24 16:40:37 2020\n",
            "[xla:7](20) Loss=1.40038 Rate=711.88 GlobalRate=802.97 Time=Tue Nov 24 16:40:37 2020\n",
            "[xla:3](40) Loss=1.31262 Rate=917.48 GlobalRate=912.68 Time=Tue Nov 24 16:40:39 2020\n",
            "[xla:5](40) Loss=1.37144 Rate=935.92 GlobalRate=922.64 Time=Tue Nov 24 16:40:39 2020\n",
            "[xla:1](40) Loss=1.19262 Rate=938.91 GlobalRate=922.63 Time=Tue Nov 24 16:40:39 2020\n",
            "[xla:0](40) Loss=1.14056 Rate=938.48 GlobalRate=922.67 Time=Tue Nov 24 16:40:39 2020\n",
            "[xla:2](40) Loss=1.31533 Rate=918.74 GlobalRate=912.67 Time=Tue Nov 24 16:40:39 2020\n",
            "[xla:4](40) Loss=1.23055 Rate=924.68 GlobalRate=917.04 Time=Tue Nov 24 16:40:39 2020\n",
            "[xla:6](40) Loss=1.29349 Rate=945.50 GlobalRate=926.58 Time=Tue Nov 24 16:40:39 2020\n",
            "[xla:7](40) Loss=1.27033 Rate=951.90 GlobalRate=928.86 Time=Tue Nov 24 16:40:39 2020\n",
            "Finished training epoch 11\n",
            "[xla:1] Accuracy=53.64%\n",
            "[xla:4] Accuracy=54.02%\n",
            "[xla:5] Accuracy=53.58%\n",
            "[xla:3] Accuracy=53.77%\n",
            "[xla:2] Accuracy=53.53%\n",
            "[xla:0] Accuracy=54.10%\n",
            "[xla:6] Accuracy=53.30%\n",
            "[xla:7] Accuracy=53.86%\n",
            "[xla:4](0) Loss=1.18921 Rate=212.48 GlobalRate=212.47 Time=Tue Nov 24 16:40:49 2020\n",
            "[xla:5](0) Loss=1.07199 Rate=178.08 GlobalRate=178.07 Time=Tue Nov 24 16:40:49 2020\n",
            "[xla:1](0) Loss=1.18305 Rate=157.37 GlobalRate=157.37 Time=Tue Nov 24 16:40:49 2020\n",
            "[xla:3](0) Loss=1.14954 Rate=150.14 GlobalRate=150.13 Time=Tue Nov 24 16:40:49 2020\n",
            "[xla:2](0) Loss=1.17298 Rate=151.82 GlobalRate=151.82 Time=Tue Nov 24 16:40:49 2020\n",
            "[xla:0](0) Loss=1.06791 Rate=130.32 GlobalRate=130.32 Time=Tue Nov 24 16:40:49 2020\n",
            "[xla:6](0) Loss=1.33842 Rate=129.61 GlobalRate=129.60 Time=Tue Nov 24 16:40:49 2020\n",
            "[xla:7](0) Loss=1.28627 Rate=129.38 GlobalRate=129.37 Time=Tue Nov 24 16:40:49 2020\n",
            "[xla:7](20) Loss=1.43376 Rate=695.88 GlobalRate=796.68 Time=Tue Nov 24 16:40:52 2020\n",
            "[xla:4](20) Loss=1.36865 Rate=618.26 GlobalRate=771.79 Time=Tue Nov 24 16:40:52 2020\n",
            "[xla:0](20) Loss=1.12467 Rate=691.72 GlobalRate=794.39 Time=Tue Nov 24 16:40:52 2020\n",
            "[xla:3](20) Loss=1.09084 Rate=650.16 GlobalRate=777.89 Time=Tue Nov 24 16:40:52 2020\n",
            "[xla:5](20) Loss=1.30577 Rate=627.04 GlobalRate=771.90 Time=Tue Nov 24 16:40:52 2020\n",
            "[xla:1](20) Loss=1.42978 Rate=638.35 GlobalRate=771.79 Time=Tue Nov 24 16:40:52 2020\n",
            "[xla:6](20) Loss=1.24189 Rate=691.65 GlobalRate=793.31 Time=Tue Nov 24 16:40:52 2020\n",
            "[xla:2](20) Loss=1.12090 Rate=665.65 GlobalRate=794.73 Time=Tue Nov 24 16:40:52 2020\n",
            "[xla:0](40) Loss=1.11403 Rate=908.38 GlobalRate=902.45 Time=Tue Nov 24 16:40:54 2020\n",
            "[xla:7](40) Loss=1.16011 Rate=910.05 GlobalRate=903.96 Time=Tue Nov 24 16:40:54 2020\n",
            "[xla:1](40) Loss=1.13703 Rate=887.03 GlobalRate=887.33 Time=Tue Nov 24 16:40:54 2020\n",
            "[xla:3](40) Loss=1.24243 Rate=849.45 GlobalRate=865.77 Time=Tue Nov 24 16:40:54 2020\n",
            "[xla:4](40) Loss=1.16115 Rate=834.94 GlobalRate=860.80 Time=Tue Nov 24 16:40:54 2020\n",
            "[xla:6](40) Loss=1.19423 Rate=862.57 GlobalRate=873.22 Time=Tue Nov 24 16:40:54 2020\n",
            "[xla:2](40) Loss=1.26315 Rate=850.62 GlobalRate=873.09 Time=Tue Nov 24 16:40:54 2020\n",
            "[xla:5](40) Loss=1.16207 Rate=833.20 GlobalRate=857.55 Time=Tue Nov 24 16:40:54 2020\n",
            "Finished training epoch 12\n",
            "[xla:4] Accuracy=54.88%\n",
            "[xla:2] Accuracy=54.52%\n",
            "[xla:3] Accuracy=54.56%\n",
            "[xla:5] Accuracy=54.34%\n",
            "[xla:0] Accuracy=55.12%\n",
            "[xla:7] Accuracy=54.57%\n",
            "[xla:6] Accuracy=54.18%\n",
            "[xla:1] Accuracy=54.43%\n",
            "[xla:4](0) Loss=1.12458 Rate=228.29 GlobalRate=228.28 Time=Tue Nov 24 16:41:04 2020\n",
            "[xla:3](0) Loss=1.08755 Rate=190.81 GlobalRate=190.81 Time=Tue Nov 24 16:41:05 2020\n",
            "[xla:2](0) Loss=1.07056 Rate=142.85 GlobalRate=142.85 Time=Tue Nov 24 16:41:05 2020\n",
            "[xla:5](0) Loss=1.03077 Rate=139.79 GlobalRate=139.79 Time=Tue Nov 24 16:41:05 2020\n",
            "[xla:0](0) Loss=1.02225 Rate=136.97 GlobalRate=136.96 Time=Tue Nov 24 16:41:05 2020\n",
            "[xla:1](0) Loss=1.06980 Rate=129.94 GlobalRate=129.93 Time=Tue Nov 24 16:41:05 2020\n",
            "[xla:6](0) Loss=1.27702 Rate=128.34 GlobalRate=128.33 Time=Tue Nov 24 16:41:05 2020\n",
            "[xla:7](0) Loss=1.11454 Rate=120.82 GlobalRate=120.81 Time=Tue Nov 24 16:41:05 2020\n",
            "[xla:3](20) Loss=1.12489 Rate=606.59 GlobalRate=753.47 Time=Tue Nov 24 16:41:07 2020\n",
            "[xla:4](20) Loss=1.18488 Rate=597.65 GlobalRate=747.86 Time=Tue Nov 24 16:41:07 2020\n",
            "[xla:6](20) Loss=1.13085 Rate=655.26 GlobalRate=759.16 Time=Tue Nov 24 16:41:07 2020\n",
            "[xla:1](20) Loss=1.29119 Rate=653.01 GlobalRate=759.17 Time=Tue Nov 24 16:41:07 2020\n",
            "[xla:2](20) Loss=1.08511 Rate=598.44 GlobalRate=719.93 Time=Tue Nov 24 16:41:08 2020\n",
            "[xla:7](20) Loss=1.32392 Rate=629.95 GlobalRate=726.41 Time=Tue Nov 24 16:41:08 2020\n",
            "[xla:0](20) Loss=1.07679 Rate=607.47 GlobalRate=723.80 Time=Tue Nov 24 16:41:08 2020\n",
            "[xla:5](20) Loss=1.12560 Rate=596.76 GlobalRate=715.71 Time=Tue Nov 24 16:41:08 2020\n",
            "[xla:1](40) Loss=1.10076 Rate=852.73 GlobalRate=855.09 Time=Tue Nov 24 16:41:10 2020\n",
            "[xla:0](40) Loss=1.12758 Rate=876.66 GlobalRate=855.05 Time=Tue Nov 24 16:41:10 2020\n",
            "[xla:2](40) Loss=1.18531 Rate=871.11 GlobalRate=851.25 Time=Tue Nov 24 16:41:10 2020\n",
            "[xla:5](40) Loss=1.10417 Rate=876.39 GlobalRate=851.34 Time=Tue Nov 24 16:41:10 2020\n",
            "[xla:7](40) Loss=1.21492 Rate=885.63 GlobalRate=856.90 Time=Tue Nov 24 16:41:10 2020\n",
            "[xla:6](40) Loss=1.17924 Rate=853.56 GlobalRate=855.04 Time=Tue Nov 24 16:41:10 2020\n",
            "[xla:3](40) Loss=1.22654 Rate=833.99 GlobalRate=851.27 Time=Tue Nov 24 16:41:10 2020\n",
            "[xla:4](40) Loss=1.08617 Rate=830.36 GlobalRate=847.56 Time=Tue Nov 24 16:41:10 2020\n",
            "Finished training epoch 13\n",
            "[xla:2] Accuracy=59.78%\n",
            "[xla:0] Accuracy=59.88%\n",
            "[xla:3] Accuracy=59.54%\n",
            "[xla:5] Accuracy=59.12%\n",
            "[xla:4] Accuracy=59.24%\n",
            "[xla:1] Accuracy=59.53%\n",
            "[xla:7] Accuracy=59.85%\n",
            "[xla:6] Accuracy=59.44%\n",
            "[xla:2](0) Loss=0.97983 Rate=207.92 GlobalRate=207.90 Time=Tue Nov 24 16:41:20 2020\n",
            "[xla:4](0) Loss=1.03990 Rate=169.04 GlobalRate=169.04 Time=Tue Nov 24 16:41:20 2020\n",
            "[xla:0](0) Loss=0.92842 Rate=152.74 GlobalRate=152.74 Time=Tue Nov 24 16:41:20 2020\n",
            "[xla:5](0) Loss=0.87959 Rate=140.14 GlobalRate=140.13 Time=Tue Nov 24 16:41:20 2020\n",
            "[xla:1](0) Loss=0.96748 Rate=124.01 GlobalRate=124.01 Time=Tue Nov 24 16:41:21 2020\n",
            "[xla:7](0) Loss=1.04492 Rate=123.14 GlobalRate=123.13 Time=Tue Nov 24 16:41:21 2020\n",
            "[xla:3](0) Loss=0.96830 Rate=119.70 GlobalRate=119.69 Time=Tue Nov 24 16:41:21 2020\n",
            "[xla:6](0) Loss=1.14677 Rate=132.85 GlobalRate=132.85 Time=Tue Nov 24 16:41:21 2020\n",
            "[xla:1](20) Loss=1.25065 Rate=681.22 GlobalRate=775.97 Time=Tue Nov 24 16:41:23 2020\n",
            "[xla:5](20) Loss=1.06851 Rate=654.99 GlobalRate=772.87 Time=Tue Nov 24 16:41:23 2020\n",
            "[xla:2](20) Loss=0.99205 Rate=617.20 GlobalRate=769.78 Time=Tue Nov 24 16:41:23 2020\n",
            "[xla:6](20) Loss=1.06329 Rate=706.22 GlobalRate=810.75 Time=Tue Nov 24 16:41:23 2020\n",
            "[xla:7](20) Loss=1.22638 Rate=686.23 GlobalRate=778.93 Time=Tue Nov 24 16:41:23 2020\n",
            "[xla:0](20) Loss=0.98727 Rate=641.71 GlobalRate=771.63 Time=Tue Nov 24 16:41:23 2020\n",
            "[xla:4](20) Loss=1.11719 Rate=632.42 GlobalRate=773.14 Time=Tue Nov 24 16:41:23 2020\n",
            "[xla:3](20) Loss=0.90893 Rate=685.44 GlobalRate=772.74 Time=Tue Nov 24 16:41:23 2020\n",
            "[xla:3](40) Loss=1.10337 Rate=926.88 GlobalRate=899.88 Time=Tue Nov 24 16:41:25 2020\n",
            "[xla:6](40) Loss=1.11066 Rate=934.97 GlobalRate=925.65 Time=Tue Nov 24 16:41:25 2020\n",
            "[xla:2](40) Loss=1.08320 Rate=899.30 GlobalRate=897.68 Time=Tue Nov 24 16:41:25 2020\n",
            "[xla:4](40) Loss=1.01977 Rate=905.35 GlobalRate=899.99 Time=Tue Nov 24 16:41:25 2020\n",
            "[xla:0](40) Loss=0.99368 Rate=908.53 GlobalRate=898.65 Time=Tue Nov 24 16:41:25 2020\n",
            "[xla:5](40) Loss=1.02871 Rate=913.55 GlobalRate=899.34 Time=Tue Nov 24 16:41:25 2020\n",
            "[xla:1](40) Loss=0.97474 Rate=884.46 GlobalRate=878.48 Time=Tue Nov 24 16:41:25 2020\n",
            "[xla:7](40) Loss=1.11643 Rate=886.48 GlobalRate=880.43 Time=Tue Nov 24 16:41:25 2020\n",
            "Finished training epoch 14\n",
            "[xla:5] Accuracy=62.44%\n",
            "[xla:4] Accuracy=62.50%\n",
            "[xla:2] Accuracy=62.72%\n",
            "[xla:0] Accuracy=62.79%\n",
            "[xla:1] Accuracy=62.94%\n",
            "[xla:3] Accuracy=62.44%\n",
            "[xla:7] Accuracy=62.46%\n",
            "[xla:6] Accuracy=62.49%\n",
            "[xla:5](0) Loss=0.81948 Rate=212.27 GlobalRate=212.26 Time=Tue Nov 24 16:41:35 2020\n",
            "[xla:2](0) Loss=0.87651 Rate=190.92 GlobalRate=190.91 Time=Tue Nov 24 16:41:35 2020\n",
            "[xla:0](0) Loss=0.86979 Rate=150.89 GlobalRate=150.88 Time=Tue Nov 24 16:41:36 2020\n",
            "[xla:4](0) Loss=1.01034 Rate=150.78 GlobalRate=150.78 Time=Tue Nov 24 16:41:36 2020\n",
            "[xla:3](0) Loss=1.06225 Rate=132.90 GlobalRate=132.90 Time=Tue Nov 24 16:41:36 2020\n",
            "[xla:1](0) Loss=0.85356 Rate=129.45 GlobalRate=129.45 Time=Tue Nov 24 16:41:36 2020\n",
            "[xla:7](0) Loss=0.96345 Rate=128.15 GlobalRate=128.14 Time=Tue Nov 24 16:41:36 2020\n",
            "[xla:6](0) Loss=1.05601 Rate=131.68 GlobalRate=131.67 Time=Tue Nov 24 16:41:36 2020\n",
            "[xla:4](20) Loss=1.03564 Rate=645.10 GlobalRate=773.41 Time=Tue Nov 24 16:41:38 2020\n",
            "[xla:2](20) Loss=0.88197 Rate=623.96 GlobalRate=773.42 Time=Tue Nov 24 16:41:38 2020\n",
            "[xla:7](20) Loss=1.12047 Rate=678.45 GlobalRate=779.60 Time=Tue Nov 24 16:41:38 2020\n",
            "[xla:6](20) Loss=0.95159 Rate=698.71 GlobalRate=802.47 Time=Tue Nov 24 16:41:38 2020\n",
            "[xla:3](20) Loss=0.94958 Rate=671.08 GlobalRate=779.38 Time=Tue Nov 24 16:41:38 2020\n",
            "[xla:1](20) Loss=1.11026 Rate=675.99 GlobalRate=779.25 Time=Tue Nov 24 16:41:38 2020\n",
            "[xla:0](20) Loss=0.79556 Rate=645.00 GlobalRate=773.40 Time=Tue Nov 24 16:41:38 2020\n",
            "[xla:5](20) Loss=0.93878 Rate=614.33 GlobalRate=767.05 Time=Tue Nov 24 16:41:38 2020\n",
            "[xla:2](40) Loss=1.03837 Rate=897.25 GlobalRate=897.55 Time=Tue Nov 24 16:41:41 2020\n",
            "[xla:1](40) Loss=0.96713 Rate=918.11 GlobalRate=901.58 Time=Tue Nov 24 16:41:41 2020\n",
            "[xla:0](40) Loss=0.96872 Rate=905.71 GlobalRate=897.55 Time=Tue Nov 24 16:41:41 2020\n",
            "[xla:7](40) Loss=0.94402 Rate=918.96 GlobalRate=901.75 Time=Tue Nov 24 16:41:41 2020\n",
            "[xla:4](40) Loss=0.86745 Rate=905.62 GlobalRate=897.49 Time=Tue Nov 24 16:41:41 2020\n",
            "[xla:6](40) Loss=1.00658 Rate=927.16 GlobalRate=917.29 Time=Tue Nov 24 16:41:41 2020\n",
            "[xla:3](40) Loss=1.02486 Rate=915.92 GlobalRate=901.54 Time=Tue Nov 24 16:41:41 2020\n",
            "[xla:5](40) Loss=0.91086 Rate=893.32 GlobalRate=893.10 Time=Tue Nov 24 16:41:41 2020\n",
            "Finished training epoch 15\n",
            "[xla:4] Accuracy=58.50%\n",
            "[xla:2] Accuracy=58.91%\n",
            "[xla:5] Accuracy=58.05%\n",
            "[xla:7] Accuracy=58.98%\n",
            "[xla:0] Accuracy=59.42%\n",
            "[xla:1] Accuracy=58.62%\n",
            "[xla:3] Accuracy=58.47%\n",
            "[xla:6] Accuracy=58.37%\n",
            "[xla:4](0) Loss=0.94958 Rate=271.94 GlobalRate=271.92 Time=Tue Nov 24 16:41:50 2020\n",
            "[xla:2](0) Loss=0.85475 Rate=167.22 GlobalRate=167.22 Time=Tue Nov 24 16:41:51 2020\n",
            "[xla:5](0) Loss=0.78099 Rate=151.16 GlobalRate=151.15 Time=Tue Nov 24 16:41:51 2020\n",
            "[xla:0](0) Loss=0.79968 Rate=151.18 GlobalRate=151.17 Time=Tue Nov 24 16:41:51 2020\n",
            "[xla:3](0) Loss=0.83006 Rate=141.15 GlobalRate=141.14 Time=Tue Nov 24 16:41:51 2020\n",
            "[xla:7](0) Loss=0.95568 Rate=134.30 GlobalRate=134.30 Time=Tue Nov 24 16:41:51 2020\n",
            "[xla:1](0) Loss=0.88928 Rate=127.33 GlobalRate=127.32 Time=Tue Nov 24 16:41:51 2020\n",
            "[xla:6](0) Loss=1.03990 Rate=129.38 GlobalRate=129.37 Time=Tue Nov 24 16:41:51 2020\n",
            "[xla:7](20) Loss=0.97333 Rate=672.13 GlobalRate=782.11 Time=Tue Nov 24 16:41:53 2020\n",
            "[xla:4](20) Loss=0.99448 Rate=621.80 GlobalRate=775.83 Time=Tue Nov 24 16:41:53 2020\n",
            "[xla:0](20) Loss=0.88573 Rate=653.61 GlobalRate=782.23 Time=Tue Nov 24 16:41:53 2020\n",
            "[xla:3](20) Loss=0.80454 Rate=670.94 GlobalRate=789.07 Time=Tue Nov 24 16:41:53 2020\n",
            "[xla:1](20) Loss=1.08319 Rate=690.27 GlobalRate=788.79 Time=Tue Nov 24 16:41:53 2020\n",
            "[xla:6](20) Loss=0.88536 Rate=720.35 GlobalRate=817.83 Time=Tue Nov 24 16:41:53 2020\n",
            "[xla:2](20) Loss=0.84982 Rate=641.53 GlobalRate=781.75 Time=Tue Nov 24 16:41:53 2020\n",
            "[xla:5](20) Loss=0.84641 Rate=653.22 GlobalRate=781.82 Time=Tue Nov 24 16:41:53 2020\n",
            "[xla:3](40) Loss=0.97888 Rate=957.94 GlobalRate=931.48 Time=Tue Nov 24 16:41:56 2020\n",
            "[xla:6](40) Loss=0.91114 Rate=977.76 GlobalRate=951.75 Time=Tue Nov 24 16:41:56 2020\n",
            "[xla:0](40) Loss=0.92049 Rate=950.96 GlobalRate=926.56 Time=Tue Nov 24 16:41:56 2020\n",
            "[xla:7](40) Loss=0.80942 Rate=958.45 GlobalRate=926.51 Time=Tue Nov 24 16:41:56 2020\n",
            "[xla:2](40) Loss=0.90386 Rate=946.57 GlobalRate=926.44 Time=Tue Nov 24 16:41:56 2020\n",
            "[xla:1](40) Loss=0.90948 Rate=965.57 GlobalRate=931.22 Time=Tue Nov 24 16:41:56 2020\n",
            "[xla:5](40) Loss=0.85960 Rate=951.14 GlobalRate=926.44 Time=Tue Nov 24 16:41:56 2020\n",
            "[xla:4](40) Loss=0.89848 Rate=938.15 GlobalRate=921.89 Time=Tue Nov 24 16:41:56 2020\n",
            "Finished training epoch 16\n",
            "[xla:3] Accuracy=65.54%\n",
            "[xla:5] Accuracy=65.16%\n",
            "[xla:0] Accuracy=65.85%\n",
            "[xla:7] Accuracy=65.87%\n",
            "[xla:4] Accuracy=65.82%\n",
            "[xla:2] Accuracy=65.74%\n",
            "[xla:1] Accuracy=65.85%\n",
            "[xla:6] Accuracy=65.49%\n",
            "[xla:3](0) Loss=0.85262 Rate=250.09 GlobalRate=250.07 Time=Tue Nov 24 16:42:05 2020\n",
            "[xla:4](0) Loss=0.88478 Rate=193.83 GlobalRate=193.82 Time=Tue Nov 24 16:42:06 2020\n",
            "[xla:5](0) Loss=0.71433 Rate=145.66 GlobalRate=145.66 Time=Tue Nov 24 16:42:06 2020\n",
            "[xla:7](0) Loss=0.84799 Rate=140.44 GlobalRate=140.43 Time=Tue Nov 24 16:42:06 2020\n",
            "[xla:2](0) Loss=0.80925 Rate=142.86 GlobalRate=142.86 Time=Tue Nov 24 16:42:06 2020\n",
            "[xla:1](0) Loss=0.79618 Rate=136.99 GlobalRate=136.99 Time=Tue Nov 24 16:42:06 2020\n",
            "[xla:0](0) Loss=0.75585 Rate=126.10 GlobalRate=126.10 Time=Tue Nov 24 16:42:06 2020\n",
            "[xla:6](0) Loss=0.90907 Rate=131.67 GlobalRate=131.67 Time=Tue Nov 24 16:42:06 2020\n",
            "[xla:5](20) Loss=0.80704 Rate=668.67 GlobalRate=791.72 Time=Tue Nov 24 16:42:08 2020\n",
            "[xla:0](20) Loss=0.75632 Rate=695.97 GlobalRate=791.87 Time=Tue Nov 24 16:42:08 2020\n",
            "[xla:7](20) Loss=0.92980 Rate=674.73 GlobalRate=791.84 Time=Tue Nov 24 16:42:08 2020\n",
            "[xla:4](20) Loss=0.91761 Rate=639.37 GlobalRate=791.92 Time=Tue Nov 24 16:42:08 2020\n",
            "[xla:3](20) Loss=0.78845 Rate=628.04 GlobalRate=785.76 Time=Tue Nov 24 16:42:08 2020\n",
            "[xla:6](20) Loss=0.82868 Rate=731.29 GlobalRate=830.77 Time=Tue Nov 24 16:42:08 2020\n",
            "[xla:2](20) Loss=0.74353 Rate=675.79 GlobalRate=795.55 Time=Tue Nov 24 16:42:08 2020\n",
            "[xla:1](20) Loss=1.02720 Rate=689.07 GlobalRate=800.94 Time=Tue Nov 24 16:42:08 2020\n",
            "[xla:1](40) Loss=0.89137 Rate=934.86 GlobalRate=922.97 Time=Tue Nov 24 16:42:11 2020\n",
            "[xla:2](40) Loss=0.93380 Rate=929.29 GlobalRate=919.14 Time=Tue Nov 24 16:42:11 2020\n",
            "[xla:7](40) Loss=0.82860 Rate=928.85 GlobalRate=916.59 Time=Tue Nov 24 16:42:11 2020\n",
            "[xla:4](40) Loss=0.85961 Rate=914.69 GlobalRate=916.64 Time=Tue Nov 24 16:42:11 2020\n",
            "[xla:6](40) Loss=0.94679 Rate=951.47 GlobalRate=942.78 Time=Tue Nov 24 16:42:11 2020\n",
            "[xla:3](40) Loss=0.91795 Rate=910.19 GlobalRate=912.42 Time=Tue Nov 24 16:42:11 2020\n",
            "[xla:0](40) Loss=0.91084 Rate=937.31 GlobalRate=916.59 Time=Tue Nov 24 16:42:11 2020\n",
            "[xla:5](40) Loss=0.87077 Rate=926.36 GlobalRate=916.47 Time=Tue Nov 24 16:42:11 2020\n",
            "Finished training epoch 17\n",
            "[xla:4] Accuracy=63.82%\n",
            "[xla:0] Accuracy=64.67%\n",
            "[xla:1] Accuracy=63.98%\n",
            "[xla:5] Accuracy=63.36%\n",
            "[xla:2] Accuracy=63.87%\n",
            "[xla:3] Accuracy=63.73%\n",
            "[xla:7] Accuracy=64.16%\n",
            "[xla:6] Accuracy=63.87%\n",
            "[xla:4](0) Loss=0.88841 Rate=209.54 GlobalRate=209.52 Time=Tue Nov 24 16:42:20 2020\n",
            "[xla:2](0) Loss=0.81230 Rate=177.94 GlobalRate=177.94 Time=Tue Nov 24 16:42:20 2020\n",
            "[xla:5](0) Loss=0.70257 Rate=160.04 GlobalRate=160.04 Time=Tue Nov 24 16:42:21 2020\n",
            "[xla:1](0) Loss=0.75883 Rate=136.27 GlobalRate=136.27 Time=Tue Nov 24 16:42:21 2020\n",
            "[xla:0](0) Loss=0.73055 Rate=126.45 GlobalRate=126.45 Time=Tue Nov 24 16:42:21 2020\n",
            "[xla:7](0) Loss=0.82904 Rate=129.78 GlobalRate=129.77 Time=Tue Nov 24 16:42:21 2020\n",
            "[xla:3](0) Loss=0.72836 Rate=124.60 GlobalRate=124.60 Time=Tue Nov 24 16:42:21 2020\n",
            "[xla:6](0) Loss=0.98278 Rate=127.18 GlobalRate=127.17 Time=Tue Nov 24 16:42:21 2020\n",
            "[xla:7](20) Loss=0.98926 Rate=657.14 GlobalRate=762.73 Time=Tue Nov 24 16:42:23 2020\n",
            "[xla:0](20) Loss=0.76952 Rate=653.65 GlobalRate=755.23 Time=Tue Nov 24 16:42:23 2020\n",
            "[xla:4](20) Loss=0.97778 Rate=599.70 GlobalRate=749.11 Time=Tue Nov 24 16:42:23 2020\n",
            "[xla:2](20) Loss=0.81065 Rate=611.98 GlobalRate=755.15 Time=Tue Nov 24 16:42:23 2020\n",
            "[xla:3](20) Loss=0.86815 Rate=663.12 GlobalRate=761.07 Time=Tue Nov 24 16:42:23 2020\n",
            "[xla:6](20) Loss=0.92730 Rate=682.17 GlobalRate=781.49 Time=Tue Nov 24 16:42:23 2020\n",
            "[xla:1](20) Loss=1.00443 Rate=640.53 GlobalRate=754.98 Time=Tue Nov 24 16:42:23 2020\n",
            "[xla:5](20) Loss=0.82899 Rate=620.29 GlobalRate=754.84 Time=Tue Nov 24 16:42:23 2020\n",
            "[xla:3](40) Loss=0.88584 Rate=901.41 GlobalRate=882.56 Time=Tue Nov 24 16:42:26 2020\n",
            "[xla:2](40) Loss=0.85388 Rate=880.95 GlobalRate=878.47 Time=Tue Nov 24 16:42:26 2020\n",
            "[xla:7](40) Loss=0.84232 Rate=899.02 GlobalRate=883.70 Time=Tue Nov 24 16:42:26 2020\n",
            "[xla:0](40) Loss=0.79848 Rate=897.65 GlobalRate=878.53 Time=Tue Nov 24 16:42:26 2020\n",
            "[xla:6](40) Loss=0.89021 Rate=909.01 GlobalRate=896.46 Time=Tue Nov 24 16:42:26 2020\n",
            "[xla:5](40) Loss=0.83273 Rate=884.64 GlobalRate=878.45 Time=Tue Nov 24 16:42:26 2020\n",
            "[xla:4](40) Loss=0.79783 Rate=876.04 GlobalRate=874.27 Time=Tue Nov 24 16:42:26 2020\n",
            "[xla:1](40) Loss=0.84688 Rate=892.37 GlobalRate=878.35 Time=Tue Nov 24 16:42:26 2020\n",
            "Finished training epoch 18\n",
            "[xla:2] Accuracy=65.71%\n",
            "[xla:7] Accuracy=65.95%\n",
            "[xla:5] Accuracy=64.95%\n",
            "[xla:1] Accuracy=65.41%\n",
            "[xla:0] Accuracy=66.05%\n",
            "[xla:4] Accuracy=65.32%\n",
            "[xla:6] Accuracy=65.71%\n",
            "[xla:3] Accuracy=65.33%\n",
            "[xla:2](0) Loss=0.78778 Rate=203.91 GlobalRate=203.90 Time=Tue Nov 24 16:42:35 2020\n",
            "[xla:7](0) Loss=0.76664 Rate=162.95 GlobalRate=162.94 Time=Tue Nov 24 16:42:36 2020\n",
            "[xla:1](0) Loss=0.75231 Rate=151.66 GlobalRate=151.65 Time=Tue Nov 24 16:42:36 2020\n",
            "[xla:4](0) Loss=0.87762 Rate=147.52 GlobalRate=147.51 Time=Tue Nov 24 16:42:36 2020\n",
            "[xla:5](0) Loss=0.73606 Rate=134.64 GlobalRate=134.64 Time=Tue Nov 24 16:42:36 2020\n",
            "[xla:0](0) Loss=0.74608 Rate=132.00 GlobalRate=131.99 Time=Tue Nov 24 16:42:36 2020\n",
            "[xla:6](0) Loss=0.87880 Rate=124.80 GlobalRate=124.79 Time=Tue Nov 24 16:42:36 2020\n",
            "[xla:3](0) Loss=0.73017 Rate=124.53 GlobalRate=124.52 Time=Tue Nov 24 16:42:36 2020\n",
            "[xla:5](20) Loss=0.79051 Rate=655.45 GlobalRate=767.14 Time=Tue Nov 24 16:42:38 2020\n",
            "[xla:2](20) Loss=0.70867 Rate=605.99 GlobalRate=755.77 Time=Tue Nov 24 16:42:38 2020\n",
            "[xla:1](20) Loss=1.02586 Rate=632.90 GlobalRate=761.86 Time=Tue Nov 24 16:42:38 2020\n",
            "[xla:4](20) Loss=0.81940 Rate=636.32 GlobalRate=761.84 Time=Tue Nov 24 16:42:38 2020\n",
            "[xla:0](20) Loss=0.63437 Rate=656.17 GlobalRate=764.63 Time=Tue Nov 24 16:42:38 2020\n",
            "[xla:3](20) Loss=0.80772 Rate=687.97 GlobalRate=782.57 Time=Tue Nov 24 16:42:38 2020\n",
            "[xla:7](20) Loss=0.83871 Rate=619.51 GlobalRate=755.81 Time=Tue Nov 24 16:42:38 2020\n",
            "[xla:6](20) Loss=0.77894 Rate=672.55 GlobalRate=769.62 Time=Tue Nov 24 16:42:38 2020\n",
            "[xla:5](40) Loss=0.85606 Rate=906.98 GlobalRate=891.60 Time=Tue Nov 24 16:42:41 2020\n",
            "[xla:1](40) Loss=0.90898 Rate=897.94 GlobalRate=887.92 Time=Tue Nov 24 16:42:41 2020\n",
            "[xla:3](40) Loss=0.80147 Rate=919.99 GlobalRate=902.19 Time=Tue Nov 24 16:42:41 2020\n",
            "[xla:7](40) Loss=0.80227 Rate=892.60 GlobalRate=883.71 Time=Tue Nov 24 16:42:41 2020\n",
            "[xla:6](40) Loss=0.86505 Rate=913.85 GlobalRate=893.33 Time=Tue Nov 24 16:42:41 2020\n",
            "[xla:0](40) Loss=0.78540 Rate=907.26 GlobalRate=889.85 Time=Tue Nov 24 16:42:41 2020\n",
            "[xla:4](40) Loss=0.76682 Rate=899.31 GlobalRate=887.91 Time=Tue Nov 24 16:42:41 2020\n",
            "[xla:2](40) Loss=0.85865 Rate=887.07 GlobalRate=883.61 Time=Tue Nov 24 16:42:41 2020\n",
            "Finished training epoch 19\n",
            "[xla:0] Accuracy=66.95%\n",
            "[xla:4] Accuracy=66.70%\n",
            "[xla:2] Accuracy=67.29%\n",
            "[xla:7] Accuracy=67.19%\n",
            "[xla:1] Accuracy=66.87%\n",
            "[xla:6] Accuracy=66.93%\n",
            "[xla:5] Accuracy=66.84%\n",
            "[xla:3] Accuracy=66.98%\n",
            "[xla:2](0) Loss=0.70224 Rate=211.33 GlobalRate=211.32 Time=Tue Nov 24 16:42:51 2020\n",
            "[xla:1](0) Loss=0.65454 Rate=150.86 GlobalRate=150.85 Time=Tue Nov 24 16:42:51 2020\n",
            "[xla:7](0) Loss=0.79306 Rate=147.87 GlobalRate=147.86 Time=Tue Nov 24 16:42:51 2020\n",
            "[xla:4](0) Loss=0.86035 Rate=142.04 GlobalRate=142.03 Time=Tue Nov 24 16:42:51 2020\n",
            "[xla:5](0) Loss=0.67637 Rate=146.19 GlobalRate=146.18 Time=Tue Nov 24 16:42:51 2020\n",
            "[xla:0](0) Loss=0.65643 Rate=122.87 GlobalRate=122.87 Time=Tue Nov 24 16:42:51 2020\n",
            "[xla:6](0) Loss=0.84649 Rate=129.99 GlobalRate=129.98 Time=Tue Nov 24 16:42:51 2020\n",
            "[xla:3](0) Loss=0.77277 Rate=122.64 GlobalRate=122.64 Time=Tue Nov 24 16:42:51 2020\n",
            "[xla:5](20) Loss=0.78903 Rate=643.44 GlobalRate=767.70 Time=Tue Nov 24 16:42:54 2020\n",
            "[xla:3](20) Loss=0.77087 Rate=679.80 GlobalRate=772.65 Time=Tue Nov 24 16:42:54 2020\n",
            "[xla:7](20) Loss=0.85724 Rate=622.39 GlobalRate=748.18 Time=Tue Nov 24 16:42:54 2020\n",
            "[xla:0](20) Loss=0.65137 Rate=648.74 GlobalRate=745.94 Time=Tue Nov 24 16:42:54 2020\n",
            "[xla:4](20) Loss=0.86980 Rate=623.45 GlobalRate=744.20 Time=Tue Nov 24 16:42:54 2020\n",
            "[xla:1](20) Loss=0.89003 Rate=620.36 GlobalRate=748.48 Time=Tue Nov 24 16:42:54 2020\n",
            "[xla:6](20) Loss=0.68719 Rate=658.17 GlobalRate=763.93 Time=Tue Nov 24 16:42:54 2020\n",
            "[xla:2](20) Loss=0.66471 Rate=595.47 GlobalRate=744.19 Time=Tue Nov 24 16:42:54 2020\n",
            "[xla:6](40) Loss=0.73225 Rate=900.61 GlobalRate=885.19 Time=Tue Nov 24 16:42:56 2020\n",
            "[xla:3](40) Loss=0.77627 Rate=909.22 GlobalRate=891.14 Time=Tue Nov 24 16:42:56 2020\n",
            "[xla:5](40) Loss=0.71868 Rate=894.67 GlobalRate=887.75 Time=Tue Nov 24 16:42:56 2020\n",
            "[xla:2](40) Loss=0.76754 Rate=875.52 GlobalRate=871.47 Time=Tue Nov 24 16:42:56 2020\n",
            "[xla:7](40) Loss=0.65133 Rate=886.28 GlobalRate=874.26 Time=Tue Nov 24 16:42:56 2020\n",
            "[xla:0](40) Loss=0.72050 Rate=896.81 GlobalRate=872.69 Time=Tue Nov 24 16:42:56 2020\n",
            "[xla:1](40) Loss=0.81317 Rate=885.33 GlobalRate=874.39 Time=Tue Nov 24 16:42:56 2020\n",
            "[xla:4](40) Loss=0.72574 Rate=886.69 GlobalRate=871.46 Time=Tue Nov 24 16:42:56 2020\n",
            "Finished training epoch 20\n",
            "[xla:7] Accuracy=69.85%\n",
            "[xla:2] Accuracy=69.76%\n",
            "[xla:1] Accuracy=69.65%\n",
            "[xla:0] Accuracy=70.04%\n",
            "[xla:4] Accuracy=69.98%\n",
            "[xla:5] Accuracy=69.42%\n",
            "[xla:6] Accuracy=69.80%\n",
            "[xla:3] Accuracy=69.89%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wt7wEVJoFmf"
      },
      "source": [
        "## Visualize Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSdVUMPjoGhy"
      },
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "img = cv2.imread(RESULT_IMG_PATH, cv2.IMREAD_UNCHANGED)\n",
        "cv2_imshow(img)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}