{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Python_Imports.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iypc-team/CoLab/blob/master/Python_Imports.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ek_52FUZJTSJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%pip install gym\n",
        "print('\\n', \"All Installs Completed.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V63WZCQ6Jtae",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, print_function, division, unicode_literals\n",
        "import gym\n",
        "import numpy as np\n",
        "import requests\n",
        "import six\n",
        "import pyglet\n",
        "import scipy\n",
        "import certifi\n",
        "import urllib3\n",
        "import idna\n",
        "import chardet\n",
        "import future\n",
        "import random\n",
        "import time\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output(wait=False)\n",
        "# print(\"gym version:\", gym.__version__)\n",
        "# print(\"numpy version:\", np.__version__)\n",
        "requests .__doc__\n",
        "dir(requests)\n",
        "clear_output()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TPVtPrH0ril",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "env = gym.make(\"FrozenLake-v0\")\n",
        "env.action_space\n",
        "env.observation_space\n",
        "print (env.action_space.n, env.observation_space.n)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eahhuLR616e6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "action_space_size = env.action_space.n\n",
        "state_space_size = env.observation_space.n\n",
        "q_table = np.zeros((state_space_size, action_space_size))\n",
        "q_table.size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CLwuTZz5RDy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_episodes = 10000\n",
        "max_steps_per_episode = 100\n",
        "\n",
        "learning_rate = 0.01\n",
        "discount_rate = 0.99\n",
        "\n",
        "exploration_rate = 1\n",
        "max_exploration_rate = 1\n",
        "min_exploration_rate = 0.01\n",
        "exploration_decay_rate = 0.01"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bh8_Ex5O-gMS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "rewards_all_episodes = []\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "  state = env.reset()\n",
        "  done = False\n",
        "  rewards_current_episode = 0\n",
        "  for step in range(max_steps_per_episode):\n",
        "    exploration_rate_threshold = random.uniform(0, 1)\n",
        "    if exploration_rate_threshold > exploration_rate:\n",
        "      action = np.argmax(q_table[state, :])\n",
        "    else:\n",
        "      action = env.action_space.sample()\n",
        "    new_state, reward, done, info = env.step(action)\n",
        "\n",
        "    q_table[state, action] = q_table[state, action] * (1 - learning_rate) +  learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))\n",
        "    \n",
        "    state = new_state\n",
        "    rewards_current_episode += reward\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}