{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "NewTPU_3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "environment": {
      "name": "tf22-cpu.2-2.m47",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf22-cpu.2-2:m47"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iypc-team/CoLab/blob/master/NewTPU_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JWL27ILbdp0",
        "outputId": "94319000-b0f7-4bbe-d981-1e49706e9465",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from google.colab import drive\n",
        "import glob, os, re, time, shutil\n",
        "from os.path import abspath, basename, exists, join\n",
        "from pathlib import Path\n",
        "\n",
        "contentPath=os.getcwd()\n",
        "bullshitPath=join(contentPath, 'sample_data')\n",
        "if exists(bullshitPath):\n",
        "    shutil.rmtree(bullshitPath)\n",
        "    time.sleep(5)\n",
        "    print(bullshitPath, 'removed')\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "os.chdir('/content/drive/MyDrive/PythonFiles')\n",
        "print(f'cwd: {Path.cwd()}')\n",
        "# %ls -l\n",
        "rootGlobList=glob.glob('**')\n",
        "for fil in sorted(rootGlobList):\n",
        "    fullPath=abspath(fil)\n",
        "    print(f'{fil}')\n",
        "    print(fullPath)\n",
        "    print()\n",
        "\n",
        "os.chdir(contentPath)\n",
        "\n",
        "impPath = '/content/drive/MyDrive/BashColors.py'\n",
        "if not exists('BashColors.py'):\n",
        "    print(contentPath)\n",
        "    print(impPath)\n",
        "    shutil.copy2(impPath, contentPath)\n",
        "    time.sleep(1)\n",
        "    from BashColors import C\n",
        "else:\n",
        "    from BashColors import C\n",
        "    print(f'{os.path.basename(impPath)}{C.BGreen} exists{C.ColorOff}')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "cwd: /content/drive/MyDrive/PythonFiles\n",
            "BashColors.py\n",
            "/content/drive/MyDrive/PythonFiles/BashColors.py\n",
            "\n",
            "CleanDrive.py\n",
            "/content/drive/MyDrive/PythonFiles/CleanDrive.py\n",
            "\n",
            "ColabDriveFiles.py\n",
            "/content/drive/MyDrive/PythonFiles/ColabDriveFiles.py\n",
            "\n",
            "FileUtilities.py\n",
            "/content/drive/MyDrive/PythonFiles/FileUtilities.py\n",
            "\n",
            "FunctionTimer.py\n",
            "/content/drive/MyDrive/PythonFiles/FunctionTimer.py\n",
            "\n",
            "GetTPU.py\n",
            "/content/drive/MyDrive/PythonFiles/GetTPU.py\n",
            "\n",
            "GithubRepos.py\n",
            "/content/drive/MyDrive/PythonFiles/GithubRepos.py\n",
            "\n",
            "ImportDriveFiles.py\n",
            "/content/drive/MyDrive/PythonFiles/ImportDriveFiles.py\n",
            "\n",
            "MountGoogleDrive.py\n",
            "/content/drive/MyDrive/PythonFiles/MountGoogleDrive.py\n",
            "\n",
            "TZInfo.py\n",
            "/content/drive/MyDrive/PythonFiles/TZInfo.py\n",
            "\n",
            "TarfileFunctions.py\n",
            "/content/drive/MyDrive/PythonFiles/TarfileFunctions.py\n",
            "\n",
            "BashColors.py\u001b[1;32m exists\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def listGoogleDriveFiles():\n",
        "    pass"
      ],
      "metadata": {
        "id": "7QRwkSZdzpG8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9u3d4Z7uQsmp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6ae7f34-2274-4cf4-d8a1-c1e5e991fbe8"
      },
      "source": [
        "%%writefile GetColabTPU.py\n",
        "# updated: 08/10/2022\n",
        "import tensorflow as tf\n",
        "\n",
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "class GetTPU:\n",
        "    # __all__ = [GetTPU, tpu, strategy, gt]\n",
        "\n",
        "    gt = None\n",
        "    tpu = None\n",
        "    strategy = None\n",
        "\n",
        "    def __init__(self):\n",
        "        self.AUTO = tf.data.experimental.AUTOTUNE\n",
        "        self.tpu = None\n",
        "        self.strategy = None\n",
        "\n",
        "    def getTPU(self):\n",
        "        return self.tpu\n",
        "\n",
        "    def getStrategy(self):\n",
        "        return self.strategy\n",
        "\n",
        "    def connectTPU(self):\n",
        "        # TPUClusterResolver automatically checks connected TPU on all Google platforms\n",
        "        try:\n",
        "            self.tpu = None\n",
        "            # tpu detection\n",
        "            self.tpu = tf.distribute.cluster_resolver.TPUClusterResolver() # TPU detection\n",
        "            tf.config.experimental_connect_to_cluster(self.tpu)\n",
        "            tf.tpu.experimental.initialize_tpu_system(self.tpu)\n",
        "            self.strategy = tf.distribute.TPUStrategy(self.tpu)\n",
        "\n",
        "        except ValueError as err: \n",
        "            # detect GPUs\n",
        "            print(err)\n",
        "            # for GPU or multi-GPU machines\n",
        "            # self.strategy = tf.distribute.MirroredStrategy()\n",
        "\n",
        "            # default strategy for CPU and single GPU machines\n",
        "            self.strategy = tf.distribute.get_strategy()\n",
        "            \n",
        "            # for clusters of multi-GPU machines\n",
        "            # self.strategy=tf.distribute.experimental.MultiWorkerMirroredStrategy()\n",
        "\n",
        "gt = GetTPU()\n",
        "gt.connectTPU()\n",
        "print(f'\\nNumber of TPUs: {gt.strategy.num_replicas_in_sync}')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting GetColabTPU.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N90ebf8bKiqr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83e84822-eb12-4cdf-b784-d2099f933ecb"
      },
      "source": [
        "initStart = time.time()\n",
        "import GetColabTPU\n",
        "from GetColabTPU import *\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "tpu = gt.getTPU()\n",
        "strategy = gt.getStrategy()\n",
        "initEnd = time.time()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide a TPU Name to connect to.\n",
            "\n",
            "Number of TPUs: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EAiF-WNl7Hv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e10a1495-b666-41fa-c083-3803e761c248"
      },
      "source": [
        "\n",
        "initTime = (initEnd - initStart)\n",
        "initTime= round(initTime, 1)\n",
        "print(f'TPU initialization: {initTime} seconds')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TPU initialization: 6.6 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTRg-X0gOKGd"
      },
      "source": [
        "def get_model():\n",
        "    # Create a simple model.\n",
        "    inputs = tf.keras.Input(shape=(32, ))\n",
        "    outputs = tf.keras.layers.Dense(9)(inputs)\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "    model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
        "    return model\n",
        "\n",
        "model = get_model()\n",
        "\n",
        "# Train the model.\n",
        "test_input = np.random.random((128 , 32))\n",
        "test_target = np.random.random((128 , 1))\n",
        "model.fit(test_input, test_target)\n",
        "\n",
        "# Calling `save('my_model')` creates a SavedModel folder `my_model`.\n",
        "# model.save(\"/gdrive/My Drive/SavedModels\")\n",
        "\n",
        "# It can be used to reconstruct the model identically.\n",
        "# reconstructed_model = tf.keras.models.load_model(\"/gdrive/My Drive/SavedModels\")\n",
        "\n",
        "# Let's check:\n",
        "# np.testing.assert_allclose(model.predict(test_input), reconstructed_model.predict(test_input))\n",
        "\n",
        "# The reconstructed model is already compiled and has retained the optimizer\n",
        "# state, so training can resume:\n",
        "# reconstructed_model.fit(test_input, test_target)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmhaoX78OKG9"
      },
      "source": [
        "with strategy.scope(): \n",
        "    # creating the model in the TPUStrategy scope places the model on the TPU\n",
        "    model = get_model()\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3G-2aUBQJ-H"
      },
      "source": [
        "\n",
        "# For image sizes are available for this dataset\n",
        "EPOCHS = 12\n",
        "IMAGE_SIZE = [331, 331]\n",
        "\n",
        "FLOWERS_DATASETS = { # available image sizes\n",
        "    192: 'gs://flowers-public/tfrecords-jpeg-192x192-2/*.tfrec',\n",
        "    224: 'gs://flowers-public/tfrecords-jpeg-224x224/*.tfrec',\n",
        "    331: 'gs://flowers-public/tfrecords-jpeg-331x331/*.tfrec',\n",
        "    512: 'gs://flowers-public/tfrecords-jpeg-512x512/*.tfrec'\n",
        "}\n",
        "CLASSES = ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips'] # do not change, maps to the labels in the data (folder names)\n",
        "assert IMAGE_SIZE[0] == IMAGE_SIZE[1], \"only square images are supported\"\n",
        "assert IMAGE_SIZE[0] in FLOWERS_DATASETS, \"this image size is not supported\"\n",
        "\n",
        "\n",
        "# mixed precision\n",
        "# On TPU, bfloat16/float32 mixed precision is automatically used in TPU computations.\n",
        "# Enabling it in Keras also stores relevant variables in bfloat16 format (memory optimization).\n",
        "# On GPU, specifically V100, mixed precision must be enabled for hardware TensorCores to be used.\n",
        "# XLA compilation must be enabled for this to work. (On TPU, XLA compilation is the default)\n",
        "MIXED_PRECISION = False\n",
        "if MIXED_PRECISION:\n",
        "    if tpu: \n",
        "        policy = tf.keras.mixed_precision.experimental.Policy('mixed_bfloat16')\n",
        "    else: #\n",
        "        policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n",
        "        tf.config.optimizer.set_jit(True) # XLA compilation\n",
        "    tf.keras.mixed_precision.experimental.set_policy(policy)\n",
        "    print('Mixed precision enabled')\n",
        "\n",
        "# batch and learning rate settings\n",
        "if strategy.num_replicas_in_sync == 8: \n",
        "    # TPU or 8xGPU\n",
        "    BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
        "    VALIDATION_BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
        "    start_lr = 0.00001\n",
        "    min_lr = 0.00001\n",
        "    max_lr = 0.00005 * strategy.num_replicas_in_sync\n",
        "    rampup_epochs = 5\n",
        "    sustain_epochs = 0\n",
        "    exp_decay = .8\n",
        "elif strategy.num_replicas_in_sync == 1: \n",
        "    # single GPU\n",
        "    BATCH_SIZE = 16\n",
        "    VALIDATION_BATCH_SIZE = 16\n",
        "    start_lr = 0.00001\n",
        "    min_lr = 0.00001\n",
        "    max_lr = 0.0002\n",
        "    rampup_epochs = 5\n",
        "    sustain_epochs = 0\n",
        "    exp_decay = .8\n",
        "else: # TPU pod\n",
        "    BATCH_SIZE = 8 * strategy.num_replicas_in_sync\n",
        "    VALIDATION_BATCH_SIZE = 8 * strategy.num_replicas_in_sync\n",
        "    start_lr = 0.00001\n",
        "    min_lr = 0.00001\n",
        "    max_lr = 0.00002 * strategy.num_replicas_in_sync\n",
        "    rampup_epochs = 7\n",
        "    sustain_epochs = 0\n",
        "    exp_decay = .8\n",
        "\n",
        "def lrfn(epoch):\n",
        "    def lr(epoch, start_lr, min_lr, max_lr, rampup_epochs, sustain_epochs, exp_decay):\n",
        "        if epoch < rampup_epochs:\n",
        "            lr = (max_lr - start_lr)/rampup_epochs * epoch + start_lr\n",
        "        elif epoch < rampup_epochs + sustain_epochs:\n",
        "            lr = max_lr\n",
        "        else:\n",
        "            lr = (max_lr - min_lr) * exp_decay**(epoch-rampup_epochs-sustain_epochs) + min_lr\n",
        "        return lr\n",
        "    return lr(epoch, start_lr, min_lr, max_lr, rampup_epochs, sustain_epochs, exp_decay)\n",
        "    \n",
        "lr_callback = tf.keras.callbacks.LearningRateScheduler(lambda epoch: lrfn(epoch), verbose=True)\n",
        "\n",
        "rng = [i for i in range(EPOCHS)]\n",
        "y = [lrfn(x) for x in rng]\n",
        "# plt.plot(rng, [lrfn(x) for x in rng])\n",
        "# print(y[0], y[-1])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPkvHdAYNt9J"
      },
      "source": [
        "#@title display utilities [RUN ME]\n",
        "\n",
        "def dataset_to_numpy_util(dataset, N):\n",
        "  dataset = dataset.unbatch().batch(N)\n",
        "  for images, labels in dataset:\n",
        "    numpy_images = images.numpy()\n",
        "    numpy_labels = labels.numpy()\n",
        "    break;  \n",
        "  return numpy_images, numpy_labels\n",
        "\n",
        "def title_from_label_and_target(label, correct_label):\n",
        "  label = np.argmax(label, axis=-1)  # one-hot to class number\n",
        "  correct_label = np.argmax(correct_label, axis=-1) # one-hot to class number\n",
        "  correct = (label == correct_label)\n",
        "  return \"{} [{}{}{}]\".format(CLASSES[label], str(correct), ', shoud be ' if not correct else '',\n",
        "                              CLASSES[correct_label] if not correct else ''), correct\n",
        "\n",
        "def display_one_flower(image, title, subplot, red=False):\n",
        "    plt.subplot(subplot)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(image)\n",
        "    plt.title(title, fontsize=16, color='red' if red else 'black')\n",
        "    return subplot+1\n",
        "  \n",
        "def display_9_images_from_dataset(dataset):\n",
        "  subplot=331\n",
        "  plt.figure(figsize=(13,13))\n",
        "  images, labels = dataset_to_numpy_util(dataset, 9)\n",
        "  for i, image in enumerate(images):\n",
        "    title = CLASSES[np.argmax(labels[i], axis=-1)]\n",
        "    subplot = display_one_flower(image, title, subplot)\n",
        "    if i >= 8:\n",
        "      break;\n",
        "              \n",
        "  #plt.tight_layout() # bug in tight layout in this version of matplotlib\n",
        "  plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
        "  plt.show()\n",
        "  \n",
        "def display_9_images_with_predictions(images, predictions, labels):\n",
        "  subplot=331\n",
        "  plt.figure(figsize=(13,13))\n",
        "  for i, image in enumerate(images):\n",
        "    title, correct = title_from_label_and_target(predictions[i], labels[i])\n",
        "    subplot = display_one_flower(image, title, subplot, not correct)\n",
        "    if i >= 8:\n",
        "      break;\n",
        "              \n",
        "  #plt.tight_layout() # bug in tight layout in this version of matplotlib\n",
        "  plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
        "  plt.show()\n",
        "  \n",
        "def display_training_curves(training, validation, title, subplot):\n",
        "  if subplot%10==1: # set up the subplots on the first call\n",
        "    plt.subplots(figsize=(10,10), facecolor='#F0F0F0')\n",
        "    #plt.tight_layout() # bug in tight layout in this version of matplotlib\n",
        "  ax = plt.subplot(subplot)\n",
        "  ax.set_facecolor('#F8F8F8')\n",
        "  ax.plot(training)\n",
        "  ax.plot(validation)\n",
        "  ax.set_title('model '+ title)\n",
        "  ax.set_ylabel(title)\n",
        "  #ax.set_ylim(0.28,1.05)\n",
        "  ax.set_xlabel('epoch')\n",
        "  ax.legend(['train', 'valid.'])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvPXiovhi3ZZ"
      },
      "source": [
        "## Read images and labels from TFRecords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtAVr-4CP1rp"
      },
      "source": [
        "def count_data_items(filenames):\n",
        "    # the number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n",
        "    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n",
        "    return np.sum(n)\n",
        "\n",
        "gcs_pattern = FLOWERS_DATASETS[IMAGE_SIZE[0]]\n",
        "validation_split = 0.19\n",
        "filenames = tf.io.gfile.glob(gcs_pattern)\n",
        "split = len(filenames) - int(len(filenames) * validation_split)\n",
        "TRAIN_FILENAMES = filenames[:split]\n",
        "VALID_FILENAMES = filenames[split:]\n",
        "TRAIN_STEPS = count_data_items(TRAIN_FILENAMES) // BATCH_SIZE\n",
        "print(\"TRAINING IMAGES: \", count_data_items(TRAIN_FILENAMES), \", STEPS PER EPOCH: \", TRAIN_STEPS)\n",
        "print(\"VALIDATION IMAGES: \", count_data_items(VALID_FILENAMES))\n",
        "        \n",
        "def read_tfrecord(example):\n",
        "    features = {\n",
        "        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n",
        "        \"class\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means scalar\n",
        "        \"one_hot_class\": tf.io.VarLenFeature(tf.float32),\n",
        "    }\n",
        "    example = tf.io.parse_single_example(example, features)\n",
        "    image = tf.image.decode_jpeg(example['image'], channels=3)\n",
        "    image = tf.cast(image, tf.float32) / 255.0  # convert image to floats in [0, 1] range\n",
        "    class_label = tf.cast(example['class'], tf.int32)\n",
        "    one_hot_class = tf.sparse.to_dense(example['one_hot_class'])\n",
        "    one_hot_class = tf.reshape(one_hot_class, [5])\n",
        "    return image, one_hot_class\n",
        "    \n",
        "def force_image_sizes(dataset, image_size):\n",
        "    # explicit size will be needed for TPU\n",
        "    reshape_images = lambda image, label: (tf.reshape(image, [*image_size, 3]), label)\n",
        "    dataset = dataset.map(reshape_images, num_parallel_calls=AUTO)\n",
        "    return dataset\n",
        "\n",
        "def load_dataset(filenames):\n",
        "    # read from TFRecords. For optimal performance, use \"interleave(tf.data.TFRecordDataset, ...)\"\n",
        "    # to read from multiple TFRecord files at once and set the option experimental_deterministic = False\n",
        "    # to allow order-altering optimizations.\n",
        "\n",
        "    opt = tf.data.Options()\n",
        "    opt.experimental_deterministic = False\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
        "    dataset = dataset.with_options(opt)\n",
        "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO) # automatically interleaves reads from multiple files\n",
        "    dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTO)\n",
        "    dataset = force_image_sizes(dataset, IMAGE_SIZE)\n",
        "    return dataset\n",
        "\n",
        "def data_augment(image, one_hot_class):\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    image = tf.image.random_saturation(image, 0, 2)\n",
        "    return image, one_hot_class\n",
        "    \n",
        "# For experts: fine adjustments of tf.data.Dataset distribution behavior:\n",
        "\n",
        "# Replicating a datset with state (even random number generator state) does not replicate the\n",
        "# state and changes the behavior of the dataset. If the state is just the RNG state, it usually\n",
        "# does not matter but this behavior can be adjusted with tf.data.experimental.ExternalStatePolicy:\n",
        "#  WARN = 0   (this is the default in Tensorflow outside of Keras)\n",
        "#  IGNORE = 1 (this is the default in Keras)\n",
        "#  FAIL = 2\n",
        "\n",
        "# On TPU pods, the dataset API attempts to shard the dataset across individual TPUs at the file\n",
        "# level so that TPUs only load the data they will actually train on. This requires more data files \n",
        "# than TPUs in the pod. (ex: TPU v3-32 pod = 4 TPUs => dataset must have at least 4 files)\n",
        "# An error will occur if there are not enough data files. File-level sharding can be disabled:\n",
        "#  opt = tf.data.Options()\n",
        "#  opt.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\n",
        "#  dataset = dataset.with_options(opt)\n",
        "   \n",
        "\n",
        "def get_training_dataset():\n",
        "    dataset = load_dataset(TRAIN_FILENAMES)\n",
        "    dataset = dataset.map(data_augment, num_parallel_calls=AUTO)\n",
        "    dataset = dataset.repeat()\n",
        "    dataset = dataset.shuffle(2048)\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
        "    return dataset\n",
        "\n",
        "def get_validation_dataset():\n",
        "    dataset = load_dataset(VALID_FILENAMES)\n",
        "    dataset = dataset.batch(VALIDATION_BATCH_SIZE)\n",
        "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
        "    \n",
        "    # needed for TPU 32-core pod: the test dataset has only 3 files but there are 4 TPUs. FILE sharding policy must be disabled.\n",
        "    opt = tf.data.Options()\n",
        "    opt.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\n",
        "    dataset = dataset.with_options(opt)\n",
        "    \n",
        "    return dataset\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22rVDTx8wCqE"
      },
      "source": [
        "## training and validation datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wxKyCklR4Gh"
      },
      "source": [
        "\n",
        "training_dataset = get_training_dataset()\n",
        "validation_dataset = get_validation_dataset()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xb-b4PRz-V6O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66a99133-642f-406a-8f93-4244724a201e"
      },
      "source": [
        "print('validation_dataset')\n",
        "# display_9_images_from_dataset(validation_dataset)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation_dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALtRUlxhw8Vt"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJl3vNtJOB-x"
      },
      "source": [
        "def create_model():\n",
        "    # pretrained_model = tf.keras.applications.MobileNetV2(input_shape=[*IMAGE_SIZE, 3], include_top=False)\n",
        "    # pretrained_model = tf.keras.applications.Xception(input_shape=[*IMAGE_SIZE, 3], include_top=False)\n",
        "    #pretrained_model = tf.keras.applications.VGG16(weights='imagenet', include_top=False ,input_shape=[*IMAGE_SIZE, 3])\n",
        "    #pretrained_model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False, input_shape=[*IMAGE_SIZE, 3])\n",
        "    pretrained_model = tf.keras.applications.MobileNet(weights='imagenet', include_top=False, input_shape=[*IMAGE_SIZE, 3])\n",
        "    pretrained_model.trainable = True\n",
        "\n",
        "    model = tf.keras.Sequential([pretrained_model,\n",
        "                                 tf.keras.layers.GlobalAveragePooling2D(),\n",
        "                                 # tf.keras.layers.Flatten(),\n",
        "                                 \n",
        "                                 # the float32 is needed on\n",
        "                                 # softmax layer when using mixed precision\n",
        "                                 tf.keras.layers.Dense(5, activation='softmax', \n",
        "                                                       dtype=tf.float32)])\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss = 'categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    model.summary\n",
        "    return model"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLJNVGwHUDy1"
      },
      "source": [
        "with strategy.scope(): \n",
        "    # creating the model in the TPUStrategy scope places the model on the TPU\n",
        "    model = create_model()\n",
        "print(f'{C.Green}')\n",
        "model.summary()\n",
        "model.input_shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMfenMQcxAAb"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS=1\n"
      ],
      "metadata": {
        "id": "1tLsdxAsEA3b"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-ID7vP5mIKs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1b6a5a1-f25f-4ece-9ea3-d78d78e5b811"
      },
      "source": [
        "start_time= time.time()\n",
        "with strategy.scope():\n",
        "    model=create_model()\n",
        "    history = model.fit(\n",
        "        training_dataset, validation_data=validation_dataset,\n",
        "        steps_per_epoch=TRAIN_STEPS, epochs=EPOCHS, callbacks=[lr_callback])\n",
        "    final_accuracy = history.history[\"val_accuracy\"][-5:]\n",
        "    print(\"FINAL ACCURACY MEAN-5: \", np.mean(final_accuracy))\n",
        "    print(\"TRAINING TIME: \", time.time() - start_time, \" seconds\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 1e-05.\n",
            "186/186 [==============================] - 1242s 6s/step - loss: 1.0762 - accuracy: 0.5813 - val_loss: 0.7894 - val_accuracy: 0.7088 - lr: 1.0000e-05\n",
            "FINAL ACCURACY MEAN-5:  0.7088235020637512\n",
            "TRAINING TIME:  1291.2582190036774  seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VngeUBIdyJ1T"
      },
      "source": [
        "print(history.history.keys())\n",
        "display_training_curves(history.history['accuracy'][1:], history.history['val_accuracy'][1:], 'accuracy', 211)\n",
        "display_training_curves(history.history['loss'][1:], history.history['val_loss'][1:], 'loss', 212)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKFMWzh0Yxsq"
      },
      "source": [
        "## Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZcpoO4jOB-8"
      },
      "source": [
        "# a couple of images to test predictions too\n",
        "some_flowers, some_labels = dataset_to_numpy_util(validation_dataset, 160)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2X07EKsqK_RW"
      },
      "source": [
        "# randomize the input so that you can execute multiple times to change results\n",
        "permutation = np.random.permutation(8*20)\n",
        "some_flowers, some_labels = (some_flowers[permutation], some_labels[permutation])\n",
        "\n",
        "predictions = model.predict(some_flowers, batch_size=16)\n",
        "evaluations = model.evaluate(some_flowers, some_labels, batch_size=16)\n",
        "  \n",
        "print(np.array(CLASSES)[np.argmax(predictions, axis=-1)].tolist())\n",
        "print('[val_loss, val_acc]', evaluations)\n",
        "\n",
        "display_9_images_with_predictions(some_flowers, predictions, some_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIC5KKcJOB_A"
      },
      "source": [
        "## Save the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivTV2qPoOB_B"
      },
      "source": [
        "model.save('MobileNet_model.h5')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOVgGwS-OB_K"
      },
      "source": [
        "## Reload the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4AAIjcoOB_K"
      },
      "source": [
        "reload_model = tf.keras.models.load_model('MobileNet_model.h5')\n",
        "\n",
        "predictions = reload_model.predict(some_flowers, batch_size=16)\n",
        "evaluations = reload_model.evaluate(some_flowers, some_labels, batch_size=16)\n",
        "print(np.array(CLASSES)[np.argmax(predictions, axis=-1)].tolist())\n",
        "print('[val_loss, val_acc]', evaluations)\n",
        "print(f'val_loss: {C.IPurple}{evaluations[0]}{C.ColorOff}')\n",
        "print(f'val_acc: {C.IPurple}{evaluations[1]}{C.ColorOff}')\n",
        "display_9_images_with_predictions(some_flowers, predictions, some_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBqnqlSVsLCW"
      },
      "source": [
        "programEnd = time.time()\n",
        "programElapseTime = (programEnd - programStart) / 60\n",
        "programElapseTime = round(programElapseTime, 2)\n",
        "print(f'completion in {programElapseTime} minutes')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}