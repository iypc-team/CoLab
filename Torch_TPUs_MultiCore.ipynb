{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Torch_TPUs: MultiCore.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iypc-team/CoLab/blob/master/Torch_TPUs_MultiCore.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAzIwJg2vweI"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from IPython.display import clear_output\n",
        "import glob, os, time, shutil\n",
        "startTime = time.time()\n",
        "assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N57-6qN0U4ps"
      },
      "source": [
        "def convertSeconds(start):\n",
        "    import time\n",
        "    start_time = start\n",
        "    end_time = time.time()\n",
        "\n",
        "    # start_time = 0\n",
        "    # end_time = 3799\n",
        "\n",
        "    elapse_time = end_time - start_time\n",
        "    if elapse_time >= 3600:\n",
        "        hours, remainder = divmod(elapse_time, 3600)\n",
        "        minutes, seconds = divmod(remainder, 60)\n",
        "\n",
        "        print(f'runtime: {int(hours)}hours {int(minutes)}mins {int(seconds)}secs')\n",
        "    else: pass\n",
        "    if elapse_time < 3600:\n",
        "        minutes, seconds = divmod(elapse_time, 60)\n",
        "        print(f'runtime: {int(minutes)}min {round(seconds,1)}sec')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imKiCg3wReXn"
      },
      "source": [
        "\n",
        "# Installs PyTorch, PyTorch/XLA, and Torchvision\n",
        "# Copy this cell into your own notebooks to use PyTorch on Cloud TPUs \n",
        "# Warning: this may take a couple minutes to run\n",
        "!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.7-cp36-cp36m-linux_x86_64.whl  \n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkK77wsK9K8F"
      },
      "source": [
        "pth=os.path.realpath('sample_data')\n",
        "if os.path.exists(pth):\n",
        "    shutil.rmtree(pth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21WNWICABAo1"
      },
      "source": [
        "### Multicore Training\n",
        "\n",
        "The following cell defines a map function that trains AlexNet on the Fashion MNIST dataset on all eight available Cloud TPU cores. The function is long and contained in a single cell, so it includes lengthy comments. It does the following:\n",
        "\n",
        "- **Setup**: every process sets the same random seed and acquires the device assigned to it (via the accelerator context, see above)\n",
        "- **Dataloading**: each process acquires its own copy of the dataset, but their sampling from it is coordinated to not overlap.\n",
        "- **Network creation**: each process has its own copy of the network, but these copies are identical since each process's random seed is the same.\n",
        "- **Training** and **Evaluation**: Training and evaluation occur as usual but use a ParallelLoader.\n",
        "\n",
        "Aside from a couple different classes, like the DistributedSampler and the ParallelLoader, the big difference between single core and multicore training is behind the scenes. The `step()` function now not only propagates gradients, but uses the Cloud TPU context to synchronize gradient updates across each processes' copy of the network. This ensures that each processes' network copy stays \"in sync\" (they are all identical). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqx-VgEizPiF"
      },
      "source": [
        "import torch\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torchsummary\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import time\n",
        "import datetime\n",
        "from datetime import timedelta\n",
        "\n",
        "def map_fn(index, flags):\n",
        "  \n",
        "  ## Setup \n",
        "\n",
        "  # Sets a common random seed - both for initialization and ensuring graph is the same\n",
        "  torch.manual_seed(flags['seed'])\n",
        "\n",
        "  # Acquires the (unique) Cloud TPU core corresponding to this process's index\n",
        "  device = xm.xla_device()  \n",
        "\n",
        "  ## Dataloader construction\n",
        "\n",
        "  # Creates the transform for the raw Torchvision data\n",
        "  # See https://pytorch.org/docs/stable/torchvision/models.html for normalization\n",
        "  # Pre-trained TorchVision models expect RGB (3 x H x W) images\n",
        "  # H and W should be >= 224\n",
        "  # Loaded into [0, 1] and normalized as follows:\n",
        "  normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                  std=[0.229, 0.224, 0.225])\n",
        "  to_rgb = transforms.Lambda(lambda image: image.convert('RGB'))\n",
        "  resize = transforms.Resize((224, 224))\n",
        "  my_transform = transforms.Compose([resize, to_rgb, transforms.ToTensor(), normalize])\n",
        "\n",
        "  # Downloads train and test datasets\n",
        "  # Note: master goes first and downloads the dataset only once (xm.rendezvous)\n",
        "  #   all the other workers wait for the master to be done downloading.\n",
        "\n",
        "  if not xm.is_master_ordinal():\n",
        "    xm.rendezvous('download_only_once')\n",
        "\n",
        "  train_dataset = datasets.FashionMNIST(\n",
        "    \"/tmp/fashionmnist\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=my_transform)\n",
        "\n",
        "  test_dataset = datasets.FashionMNIST(\n",
        "    \"/tmp/fashionmnist\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=my_transform)\n",
        "  \n",
        "  if xm.is_master_ordinal():\n",
        "    xm.rendezvous('download_only_once')\n",
        "  \n",
        "  # Creates the (distributed) train sampler, which let this process only access\n",
        "  # its portion of the training dataset.\n",
        "  train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "    train_dataset,\n",
        "    num_replicas=xm.xrt_world_size(),\n",
        "    rank=xm.get_ordinal(),\n",
        "    shuffle=True)\n",
        "  \n",
        "  test_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "    test_dataset,\n",
        "    num_replicas=xm.xrt_world_size(),\n",
        "    rank=xm.get_ordinal(),\n",
        "    shuffle=False)\n",
        "  \n",
        "  # Creates dataloaders, which load data in batches\n",
        "  # Note: test loader is not shuffled or sampled\n",
        "  train_loader = torch.utils.data.DataLoader(\n",
        "      train_dataset,\n",
        "      batch_size=flags['batch_size'],\n",
        "      sampler=train_sampler,\n",
        "      num_workers=flags['num_workers'],\n",
        "      drop_last=True)\n",
        "\n",
        "  test_loader = torch.utils.data.DataLoader(\n",
        "      test_dataset,\n",
        "      batch_size=flags['batch_size'],\n",
        "      sampler=test_sampler,\n",
        "      shuffle=False,\n",
        "      num_workers=flags['num_workers'],\n",
        "      drop_last=True)\n",
        "  \n",
        "\n",
        "  ## Network, optimizer, and loss function creation\n",
        "\n",
        "  # Creates AlexNet for 10 classes\n",
        "  # Note: each process has its own identical copy of the model\n",
        "  #  Even though each model is created independently, they're also\n",
        "  #  created in the same way.\n",
        "  net = torchvision.models.alexnet(num_classes=10).to(device).train()\n",
        "\n",
        "  loss_fn = torch.nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(net.parameters())\n",
        "\n",
        "\n",
        "  ## Trains\n",
        "  train_start = time.time()\n",
        "  for epoch in range(flags['num_epochs']):\n",
        "    para_train_loader = pl.ParallelLoader(train_loader, [device]).per_device_loader(device)\n",
        "    for batch_num, batch in enumerate(para_train_loader):\n",
        "      data, targets = batch \n",
        "\n",
        "      # Acquires the network's best guesses at each class\n",
        "      output = net(data)\n",
        "\n",
        "      # Computes loss\n",
        "      loss = loss_fn(output, targets)\n",
        "\n",
        "      # Updates model\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "\n",
        "      # Note: optimizer_step uses the implicit Cloud TPU context to\n",
        "      #  coordinate and synchronize gradient updates across processes.\n",
        "      #  This means that each process's network has the same weights after\n",
        "      #  this is called.\n",
        "      # Warning: this coordination requires the actions performed in each \n",
        "      #  process are the same. In more technical terms, the graph that\n",
        "      #  PyTorch/XLA generates must be the same across processes. \n",
        "      xm.optimizer_step(optimizer)  # Note: barrier=True not needed when using ParallelLoader \n",
        "\n",
        "  elapsed_train_time = time.time() - train_start\n",
        "  print(\"\\nProcess\", index, \"finished training. Train time was:\", elapsed_train_time) \n",
        "\n",
        "\n",
        "  ## Evaluation\n",
        "  # Sets net to eval and no grad context \n",
        "  net.eval()\n",
        "  eval_start = time.time()\n",
        "  with torch.no_grad():\n",
        "    num_correct = 0\n",
        "    total_guesses = 0\n",
        "\n",
        "    para_train_loader = pl.ParallelLoader(test_loader, [device]).per_device_loader(device)\n",
        "    for batch_num, batch in enumerate(para_train_loader):\n",
        "      data, targets = batch\n",
        "\n",
        "      # Acquires the network's best guesses at each class\n",
        "      output = net(data)\n",
        "      best_guesses = torch.argmax(output, 1)\n",
        "\n",
        "      # Updates running statistics\n",
        "      num_correct += torch.eq(targets, best_guesses).sum().item()\n",
        "      total_guesses += flags['batch_size']\n",
        "  \n",
        "  elapsed_eval_time = time.time() - eval_start\n",
        "  print(\"Process\", index, \"finished evaluation. time:\", elapsed_eval_time)\n",
        "\n",
        "  print(\"Process\", index, \"guessed\", num_correct, \"of\", total_guesses, \"correctly for\", num_correct/total_guesses * 100, \"% accuracy.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rDe9KaS1mzz"
      },
      "source": [
        "# Configures training (and evaluation) parameters\n",
        "flags = {}\n",
        "\n",
        "flags['batch_size'] = 32\n",
        "flags['num_workers'] = 8\n",
        "flags['num_epochs'] = 1\n",
        "flags['seed'] = 4321\n",
        "\n",
        "xmp.spawn(map_fn, args=(flags,), nprocs=8, start_method='fork')\n",
        "\n",
        "convertSeconds(start=startTime)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCt4utbpZ8En"
      },
      "source": [
        "\n",
        "# torchsummary.summary(model=xm, input_size=)\n",
        "\n",
        "help(torchsummary.summary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkhC6gt1N1c8"
      },
      "source": [
        "The network should take about 30 seconds to train and about 10 seconds to evaluate on each process. Using an entire Cloud TPU is, as expected, dramatically faster than training and evaluating on a single Cloud TPU core.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSp0EOq8Pg2b"
      },
      "source": [
        "##What's Next?\n",
        "\n",
        "This notebook broke down training AlexNet on the Fashion MNIST dataset using an entire Cloud TPU. A [previous notebook](https://colab.research.google.com/github/pytorch/xla/blob/master/contrib/colab/single-core-alexnet-fashion-mnist.ipynb) showed how to train AlexNet on Fashion MNIST using only a single Cloud TPU core, and can be a helpful point of comparison. \n",
        "\n",
        "In particular, this notebook showed us how to:\n",
        "\n",
        "- Define a \"map function\" that runs in parallel on one process per Cloud TPU core. \n",
        "- Run the map function using `spawn`.\n",
        "- Understand the Cloud TPU context, its benefits, like automatic cross-process coordination, and its limits, like needing each process to perform the same Cloud TPU operations.\n",
        "- Load and sample the datasets.\n",
        "- Train and evaluate the network.\n",
        "\n",
        "Additional notebooks demonstrating how to run PyTorch on Cloud TPUs can be found [here](https://github.com/pytorch/xla). While Colab provides a free Cloud TPU, training is even faster on [Google Cloud Platform](https://github.com/pytorch/xla#Cloud), especially when using multiple Cloud TPUs in a Cloud TPU pod. Scaling from a single Cloud TPU, like in this notebook, to many Cloud TPUs in a pod is easy, too. You use the same code as this notebook and just spawn more processes."
      ]
    }
  ]
}