{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    },
    "colab": {
      "name": "PT_Fundamentals_Cartpole.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iypc-team/CoLab/blob/master/PT_Fundamentals_Cartpole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXWNh-INGJmJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%writefile control_cartpole_q_learning.py\n",
        "# 09/14/2020\n",
        "import numpy as np\n",
        "\n",
        "class Agent():\n",
        "    def __init__(self, lr, gamma, n_actions, state_space, eps_start, eps_end,\n",
        "                 eps_dec):\n",
        "        self.lr = lr\n",
        "        self.gamma = gamma\n",
        "        self.actions = [i for i in range(n_actions)]\n",
        "        self.states = state_space\n",
        "        self.epsilon = eps_start\n",
        "        self.eps_min = eps_end\n",
        "        self.eps_dec = eps_dec\n",
        "\n",
        "        self.Q = {}\n",
        "\n",
        "        self.init_Q()\n",
        "\n",
        "    def init_Q(self):\n",
        "        for state in self.states:\n",
        "            for action in self.actions:\n",
        "                self.Q[(state, action)] = 0.0\n",
        "\n",
        "    def max_action(self, state):\n",
        "        actions = np.array([self.Q[(state, a)] for a in self.actions])\n",
        "        action = np.argmax(actions)\n",
        "\n",
        "        return action\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if np.random.random() < self.epsilon:\n",
        "            action = np.random.choice(self.actions)\n",
        "        else:\n",
        "            action = self.max_action(state)\n",
        "\n",
        "        return action\n",
        "\n",
        "    def decrement_epsilon(self):\n",
        "        self.epsilon = self.epsilon - self.eps_dec \\\n",
        "                if self.epsilon>self.eps_min else self.eps_min\n",
        "\n",
        "    def learn(self, state, action, reward, state_):\n",
        "        a_max = self.max_action(state_)\n",
        "\n",
        "        self.Q[(state, action)] = self.Q[(state, action)] + self.lr*(reward +\n",
        "                                        self.gamma*self.Q[(state_, a_max)] -\n",
        "                                        self.Q[(state, action)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Yi1FrdEGJnI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from control_cartpole_q_learning import Agent\n",
        "\n",
        "class CartPoleStateDigitizer():\n",
        "    def __init__(self, bounds=(2.4, 4, 0.209, 4), n_bins=10):\n",
        "        \"\"\"  \n",
        "            bounds - bounds for linear space. Single floating point number for\n",
        "                     each observation element. Space is from -bound to +bound\n",
        "                     observation -> x, dx/dt, theta, dtheta/dt\n",
        "        \"\"\"\n",
        "        self.position_space = np.linspace(-1*bounds[0], bounds[0], n_bins)\n",
        "        self.velocity_space = np.linspace(-1*bounds[1], bounds[1], n_bins)\n",
        "        self.pole_angle_space = np.linspace(-1*bounds[2], bounds[2], n_bins)\n",
        "        self.pole_velocity_space = np.linspace(-1*bounds[3], bounds[3], n_bins)\n",
        "        self.states = self.get_state_space()\n",
        "\n",
        "    def get_state_space(self):\n",
        "        states = []\n",
        "        for i in range(len(self.position_space)+1):\n",
        "            for j in range(len(self.velocity_space)+1):\n",
        "                for k in range(len(self.pole_angle_space)+1):\n",
        "                    for l in range(len(self.pole_velocity_space)+1):\n",
        "                        states.append((i,j,k,l))\n",
        "        return states\n",
        "\n",
        "    def digitize(self, observation):\n",
        "        x, dx_dt, theta, dtheta_dt = observation\n",
        "        cart_x = int(np.digitize(x, self.position_space))\n",
        "        cart_dx_dt = int(np.digitize(dx_dt, self.velocity_space))\n",
        "        pole_theta = int(np.digitize(theta, self.pole_angle_space))\n",
        "        pole_dtheta_dt = int(np.digitize(dtheta_dt, self.pole_velocity_space))\n",
        "\n",
        "        return (cart_x, cart_dx_dt, pole_theta, pole_dtheta_dt)\n",
        "\n",
        "def plot_learning_curve(scores, x):\n",
        "    running_avg = np.zeros(len(scores))\n",
        "    for i in range(len(running_avg)):\n",
        "        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\n",
        "    plt.plot(x, running_avg)\n",
        "    plt.title('Running average of scores')\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    env = gym.make('CartPole-v0')\n",
        "    n_games = 50000\n",
        "    eps_dec =  2 / n_games\n",
        "    digitizer = CartPoleStateDigitizer()\n",
        "    agent = Agent(lr=0.01, gamma=0.99, n_actions=2, eps_start=1.0,\n",
        "            eps_end=0.01, eps_dec=eps_dec, state_space=digitizer.states)\n",
        "\n",
        "    scores = []\n",
        "\n",
        "    for i in range(n_games):\n",
        "        observation = env.reset()\n",
        "        done = False\n",
        "        score = 0\n",
        "        state = digitizer.digitize(observation)\n",
        "        while not done:\n",
        "            action = agent.choose_action(state)\n",
        "            observation_, reward, done, info = env.step(action)\n",
        "            state_ = digitizer.digitize(observation_)\n",
        "            agent.learn(state, action, reward, state_)\n",
        "            state = state_\n",
        "            score += reward\n",
        "        if i % 1000 == 0:\n",
        "            print('episode ', i, 'score %.1f' % score, \n",
        "                  'epsilon %.2f' % agent.epsilon)\n",
        "\n",
        "        agent.decrement_epsilon()\n",
        "        scores.append(score)\n",
        "\n",
        "    x = [i + 1 for i in range(n_games)]\n",
        "    plot_learning_curve(scores, x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c35bay3UGJn7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %%writefile prediction_cartpole_td_zero.py\n",
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "def simple_policy(state):\n",
        "    action = 0 if state < 5 else 1\n",
        "    return action\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    env = gym.make('CartPole-v0')\n",
        "    alpha = 0.1\n",
        "    gamma = 0.99\n",
        "\n",
        "    states = np.linspace(-0.2094, 0.2094, 10)\n",
        "    V = {}\n",
        "    for state in range(len(states)+1):\n",
        "        V[state] = 0\n",
        "\n",
        "    for i in range(50000):\n",
        "        observation = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            state = int(np.digitize(observation[2], states))\n",
        "            action = simple_policy(state)\n",
        "            observation_, reward, done, info = env.step(action)\n",
        "            state_ = int(np.digitize(observation_[2], states))\n",
        "            V[state] = V[state] + alpha*(reward + gamma*V[state_] - V[state])\n",
        "            observation = observation_\n",
        "\n",
        "    for state in V:\n",
        "        print(state, '%.3f' % V[state])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}