{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    },
    "colab": {
      "name": "Untitled.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iypc-team/CoLab/blob/master/Untitled.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZB_OHzZgYpG4"
      },
      "source": [
        "import os, shutil\r\n",
        "from google.colab import drive\r\n",
        "\r\n",
        "drive.mount('/content/drive', force_remount=True)\r\n",
        "contentPth = os.getcwd()\r\n",
        "pythonFilesPth = '/content/drive/My Drive/PythonFiles'\r\n",
        "print(contentPth)\r\n",
        "\r\n",
        "deletionPth = '/content/sample_data'\r\n",
        "if os.path.exists(deletionPth):\r\n",
        "    shutil.rmtree(deletionPth)\r\n",
        "else: pass\r\n",
        "\r\n",
        "if os.path.exists(pythonFilesPth):\r\n",
        "    os.chdir(pythonFilesPth)\r\n",
        "    print(f'cwd: {os.getcwd()}')\r\n",
        "\r\n",
        "from CleanDrive import cd\r\n",
        "cd.cleanDrive()\r\n",
        "import ImportDriveFiles\r\n",
        "\r\n",
        "os.chdir(contentPth)\r\n",
        "from FunctionTimer import ft"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyzI9ak1xWmI"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "import os, time\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    print(f'tf version: {tf.__version__}')\n",
        "except ModuleNotFoundError as err:\n",
        "    print(err)\n",
        "    %pip install tensorflow\n",
        "    import tensorflow as tf\n",
        "\n",
        "try:\n",
        "    import tensorflow_datasets as tfds\n",
        "    print(f'tfds version: {tfds.__version__}')\n",
        "except ModuleNotFoundError as err:\n",
        "    print(err)\n",
        "    %pip install tensorflow_datasets\n",
        "    import tensorflow_datasets as tfds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2nn0teDxWm6"
      },
      "source": [
        "startTime=time.time()\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "# This is the TPU initialization code that has to be at the beginning.\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "# print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
        "ft.functionTimer(startTime, 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBcv_bA744Tk"
      },
      "source": [
        "logicalDevices = tf.config.list_logical_devices()\r\n",
        "for device in sorted(logicalDevices):\r\n",
        "    print(device)\r\n",
        "print()\r\n",
        "\r\n",
        "physicalDevices = tf.config.list_physical_devices()\r\n",
        "for device in sorted(physicalDevices):\r\n",
        "    print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MX79UaOZxWm_"
      },
      "source": [
        "a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
        "b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
        "with tf.device('/TPU:0'):\n",
        "  c = tf.matmul(a, b)\n",
        "print(\"c device: \", c.device)\n",
        "print(c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iky93p4OxWnB"
      },
      "source": [
        "strategy = tf.distribute.TPUStrategy(resolver)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icJTm8kQxWnL"
      },
      "source": [
        "startTime = time.time()\n",
        "@tf.function\n",
        "def matmul_fn(x, y):\n",
        "    z = tf.matmul(x, y)\n",
        "    return z\n",
        "\n",
        "z = strategy.run(matmul_fn, args=(a, b))\n",
        "print(z)\n",
        "ft.functionTimer(startTime, 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0dzvPE8xWnP"
      },
      "source": [
        "def create_model():\n",
        "    return tf.keras.Sequential(\n",
        "        [tf.keras.layers.Conv2D(256, 3, activation='relu',\n",
        "                                input_shape=(28, 28, 1)),\n",
        "         tf.keras.layers.Conv2D(256, 3, activation='relu'),\n",
        "         tf.keras.layers.Flatten(),\n",
        "         tf.keras.layers.Dense(256, activation='relu'),\n",
        "         tf.keras.layers.Dense(128, activation='relu'),\n",
        "         tf.keras.layers.Dense(10)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJ2dayT-xWnQ"
      },
      "source": [
        "global dataset\n",
        "def get_dataset(batch_size, is_training=True):\n",
        "    split = 'train' if is_training else 'test'\n",
        "    dataset, info = tfds.load(name='mnist', split=split, with_info=True,as_supervised=True, try_gcs=True)\n",
        "\n",
        "    def scale(image, label):\n",
        "        image = tf.cast(image, tf.float32)\n",
        "        image /= 255.0\n",
        "        \n",
        "        return image, label\n",
        "\n",
        "    dataset = dataset.map(scale)\n",
        "\n",
        "    # Only shuffle and repeat the dataset in training. The advantage to have a\n",
        "    # infinite dataset for training is to avoid the potential last partial batch\n",
        "    # in each epoch, so users don't need to think about scaling the gradients\n",
        "    # based on the actual batch size.\n",
        "    if is_training:\n",
        "        dataset = dataset.shuffle(10000)\n",
        "        dataset = dataset.repeat()\n",
        "\n",
        "    dataset = dataset.batch(batch_size)\n",
        "\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPJVSuKlxWnX"
      },
      "source": [
        "startTime = time.time()\n",
        "with strategy.scope(): # roughly 1 minute 0.6 seconds\n",
        "    model = create_model()\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                  metrics=['sparse_categorical_accuracy'])\n",
        "\n",
        "batch_size = 200\n",
        "steps_per_epoch = 60000 // batch_size\n",
        "validation_steps = 10000 // batch_size\n",
        "\n",
        "train_dataset = get_dataset(batch_size, is_training=True)\n",
        "test_dataset = get_dataset(batch_size, is_training=False)\n",
        "\n",
        "model.fit(train_dataset,\n",
        "          epochs=5,\n",
        "          steps_per_epoch=steps_per_epoch,\n",
        "          validation_data=test_dataset, \n",
        "          validation_steps=validation_steps)\n",
        "ft.functionTimer(start=startTime, roundedTo=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kcZ2j4-xWna"
      },
      "source": [
        "startTime = time.time()\n",
        "# with tf.device('CPU:0'): # Roughly 11 minutes 26 seconds\n",
        "with strategy.scope(): # Roughly 35.4 seconds wow!\n",
        "    model = create_model()\n",
        "    model.compile(optimizer='adam',\n",
        "                  # Anything between 2 and `steps_per_epoch` could help here.\n",
        "                  steps_per_execution = 50,\n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                  metrics=['sparse_categorical_accuracy'])\n",
        "\n",
        "model.fit(train_dataset,\n",
        "          epochs=5,\n",
        "          steps_per_epoch=steps_per_epoch,\n",
        "          validation_data=test_dataset,\n",
        "          validation_steps=validation_steps)\n",
        "\n",
        "ft.functionTimer(start=startTime, roundedTo=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a0AeTDOxWnd"
      },
      "source": [
        "# Create the model, optimizer and metrics inside strategy scope, so that the\n",
        "# variables can be mirrored on each device.\n",
        "with strategy.scope():\n",
        "  model = create_model()\n",
        "  optimizer = tf.keras.optimizers.Adam()\n",
        "  training_loss = tf.keras.metrics.Mean('training_loss', dtype=tf.float32)\n",
        "  training_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "      'training_accuracy', dtype=tf.float32)\n",
        "\n",
        "# Calculate per replica batch size, and distribute the datasets on each TPU\n",
        "# worker.\n",
        "per_replica_batch_size = batch_size // strategy.num_replicas_in_sync\n",
        "\n",
        "train_dataset = strategy.distribute_datasets_from_function(\n",
        "    lambda _: get_dataset(per_replica_batch_size, is_training=True))\n",
        "\n",
        "@tf.function\n",
        "def train_step(iterator):\n",
        "  \"\"\"The step function for one training step\"\"\"\n",
        "\n",
        "  def step_fn(inputs):\n",
        "    \"\"\"The computation to run on each TPU device.\"\"\"\n",
        "    images, labels = inputs\n",
        "    with tf.GradientTape() as tape:\n",
        "      logits = model(images, training=True)\n",
        "      loss = tf.keras.losses.sparse_categorical_crossentropy(\n",
        "          labels, logits, from_logits=True)\n",
        "      loss = tf.nn.compute_average_loss(loss, global_batch_size=batch_size)\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(list(zip(grads, model.trainable_variables)))\n",
        "    training_loss.update_state(loss * strategy.num_replicas_in_sync)\n",
        "    training_accuracy.update_state(labels, logits)\n",
        "\n",
        "  strategy.run(step_fn, args=(next(iterator),))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnC9g9ppxWng"
      },
      "source": [
        "steps_per_eval = 10000 // batch_size\n",
        "train_iterator = iter(train_dataset)\n",
        "\n",
        "startTime = time.time()\n",
        "for epoch in range(5):\n",
        "    epochTime=time.time()\n",
        "    print('Epoch: {}/5'.format(epoch))\n",
        "    for step in range(steps_per_epoch):\n",
        "        train_step(train_iterator)\n",
        "    print('step: {}, loss: {}, accuracy: {}%'.format(\n",
        "        optimizer.iterations.numpy(),\n",
        "        round(float(training_loss.result()), 4),\n",
        "        round(float(training_accuracy.result()) * 100, 1)))\n",
        "    print(f'this Epoch')\n",
        "    ft.functionTimer(epochTime)\n",
        "    # ft.functionTimer()\n",
        "    training_loss.reset_states()\n",
        "    training_accuracy.reset_states()\n",
        "print(f'total time')\n",
        "ft.functionTimer(startTime, 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtmDr_C4xWni"
      },
      "source": [
        "@tf.function\n",
        "def train_multiple_steps(iterator, steps):\n",
        "  \"\"\"The step function for one training step\"\"\"\n",
        "\n",
        "  def step_fn(inputs):\n",
        "    \"\"\"The computation to run on each TPU device.\"\"\"\n",
        "    images, labels = inputs\n",
        "    with tf.GradientTape() as tape:\n",
        "      logits = model(images, training=True)\n",
        "      loss = tf.keras.losses.sparse_categorical_crossentropy(\n",
        "          labels, logits, from_logits=True)\n",
        "      loss = tf.nn.compute_average_loss(loss, global_batch_size=batch_size)\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(list(zip(grads, model.trainable_variables)))\n",
        "    training_loss.update_state(loss * strategy.num_replicas_in_sync)\n",
        "    training_accuracy.update_state(labels, logits)\n",
        "\n",
        "  for _ in tf.range(steps):\n",
        "    strategy.run(step_fn, args=(next(iterator),))\n",
        "\n",
        "# Convert `steps_per_epoch` to `tf.Tensor` so the `tf.function` won't get \n",
        "# retraced if the value changes.\n",
        "train_multiple_steps(train_iterator, tf.convert_to_tensor(steps_per_epoch))\n",
        "\n",
        "print('step: {}, loss: {}, accuracy: {}%'.format(\n",
        "      optimizer.iterations.numpy(),\n",
        "      round(float(training_loss.result()), 4),\n",
        "      round(float(training_accuracy.result()) * 100, 2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4SRvlcpxWno"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}