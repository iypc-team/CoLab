{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TPUs_Colab",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iypc-team/CoLab/blob/master/TPUs_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "71iSWtsXe36x",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "from IPython.display import clear_output\n",
        "import os\n",
        "import pprint\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "\n",
        "# tf.debugging.set_log_device_placement(True)\n",
        "\n",
        "print(\"tensorflow version:\", tf.__version__, '\\n')\n",
        "\n",
        "if 'COLAB_TPU_ADDR' not in os.environ:\n",
        "    print('ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!')\n",
        "else:\n",
        "    tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "    print ('TPU address is', tpu_address)\n",
        "    \n",
        "    with tf.Session(tpu_address) as session:\n",
        "        devices = session.list_devices()\n",
        "        \n",
        "    print('TPU devices:', '\\n')\n",
        "    pprint.pprint(devices)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Jkh7cEWRAEA-"
      },
      "source": [
        "If the cell above reports an error, make sure that you have enabled TPU support in the notebook settings. (Edit menu â†’ Notebook settings)\n",
        "\n",
        "Now, let's try a simple computation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0iqSZvc6AX1m",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def add_op(x, y):\n",
        "    return x + y\n",
        "  \n",
        "x = tf.placeholder(tf.float32, [10,])\n",
        "y = tf.placeholder(tf.float32, [10,])\n",
        "tpu_ops = tf.contrib.tpu.rewrite(add_op, [x, y])\n",
        "  \n",
        "session = tf.Session(tpu_address)\n",
        "try:\n",
        "    print('Initializing...')\n",
        "    session.run(tf.contrib.tpu.initialize_system())\n",
        "    print('Running ops')\n",
        "    print(session.run(tpu_ops, {x: np.arange(10), y: np.arange(10)}))\n",
        "finally:\n",
        "    # For now, TPU sessions must be shutdown separately from\n",
        "    # closing the session.\n",
        "    session.run(tf.contrib.tpu.shutdown_system())\n",
        "    session.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nhXwaCNWBK2n"
      },
      "source": [
        "## TPU FLOPs\n",
        "\n",
        "Finally, we'll try a small test of floating point computations (floating point operations per seconds. (The units are FLOPS: floating point operations per second.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "llcFb_P_BNPM",
        "colab": {}
      },
      "source": [
        "N = 4096 \n",
        "print(N)\n",
        "COUNT = 100\n",
        "import time\n",
        "\n",
        "def flops():\n",
        "    x = tf.random_uniform([N, N])\n",
        "    y = tf.random_uniform([N, N])\n",
        "    def _matmul(x, y):\n",
        "        return tf.tensordot(x, y, axes=[[1], [0]]), y\n",
        "        \n",
        "    return tf.reduce_sum(tf.contrib.tpu.repeat(COUNT, _matmul, [x, y]))\n",
        "  \n",
        "tpu_ops = tf.contrib.tpu.batch_parallel(flops, [], num_shards=8)\n",
        "  \n",
        "session = tf.Session(tpu_address)\n",
        "try:\n",
        "    print('Warming up...')\n",
        "    session.run(tf.contrib.tpu.initialize_system())\n",
        "    session.run(tpu_ops)\n",
        "    print('Profiling')\n",
        "    start = time.time()\n",
        "    session.run(tpu_ops)\n",
        "    end = time.time()\n",
        "    elapsed = end - start\n",
        "    print(\"Elapse time:\", elapsed, 'TFlops: {:.2f}'.format(1e-12 * 8 * COUNT * 2*N*N*N / elapsed))\n",
        "finally:\n",
        "    session.run(tf.contrib.tpu.shutdown_system())\n",
        "    session.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "a_rjVo-RAoYd"
      },
      "source": [
        "## Next steps\n",
        "\n",
        "More involved examples include:\n",
        "- [Shakespeare in 5 minutes with Cloud TPUs and Keras](https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/shakespeare_with_tpu_and_keras.ipynb)\n",
        "- [Shakespeare in 5 minutes with Cloud TPUs via TPUEstimator](https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/shakespeare_with_tpuestimator.ipynb)\n",
        "- [Fashion MNIST with Keras and TPUs](https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/fashion_mnist.ipynb)\n",
        "\n",
        "We'll be sharing more examples of TPU use in Colab over time, so be sure to check back for additional example links, or [follow us on Twitter @GoogleColab](https://twitter.com/googlecolab).\n",
        "\n",
        "Meanwhile, you can check out the [TPUEstimator documentation on TensorFlow.org](https://www.tensorflow.org/api_docs/python/tf/contrib/tpu/TPUEstimator). TPUEstimator is an easy way to update models to take advantage of TPU acceleration."
      ]
    }
  ]
}