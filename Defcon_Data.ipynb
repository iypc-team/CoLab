{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Defcon_Data.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iypc-team/CoLab/blob/master/Defcon_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xezTAnLsQ-cP"
      },
      "source": [
        "import glob, os, shutil\r\n",
        "from os.path import *\r\n",
        "from google.colab import drive, files\r\n",
        "\r\n",
        "contentPth = os.getcwd()\r\n",
        "drivePth = abspath('/content/gdrive')\r\n",
        "myDrivePth = abspath('/content/gdrive/My Drive')\r\n",
        "pythonPth = abspath('/content/gdrive/My Drive/PythonFiles')\r\n",
        "tfImagePth = abspath('/content/gdrive/My Drive/Tensorflow Images')\r\n",
        "dataPth = abspath('/content/data')\r\n",
        "\r\n",
        "deletePth = abspath('/content/sample_data')\r\n",
        "if exists(deletePth):\r\n",
        "    print(deletePth)\r\n",
        "    shutil.rmtree(deletePth)\r\n",
        "\r\n",
        "drive.mount('/content/gdrive', force_remount=True)\r\n",
        "\r\n",
        "if not exists(dataPth):\r\n",
        "    try: shutil.copytree(src=tfImagePth, dst=dataPth)\r\n",
        "    except Exception as err:\r\n",
        "        print(err)\r\n",
        "\r\n",
        "os.chdir(pythonPth)        \r\n",
        "print(f'cwd: {os.getcwd()}')\r\n",
        "from BashColors import C"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwWxWwvCHkJf"
      },
      "source": [
        "os.chdir(dataPth)\r\n",
        "fileNames=[]\r\n",
        "imageDataList = glob.glob('**', recursive=True)\r\n",
        "for item in sorted(imageDataList):\r\n",
        "    filPth = abspath(item)\r\n",
        "    if isdir(item):\r\n",
        "        print(f'{C.IBlue}{filPth}')\r\n",
        "    elif isfile(item):\r\n",
        "        fileNames.append(filPth)\r\n",
        "        print(f'{C.IWhite}{filPth}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJIEjEIBdf-h"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "print('tensorflow:', tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Y0JtWBNR9E5"
      },
      "source": [
        "import pathlib\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# np.set_printoptions(precision=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIjPhvQ87jUT"
      },
      "source": [
        "Create the `image.ImageDataGenerator`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPCZeBQE5DfH"
      },
      "source": [
        "img_gen = tf.keras.preprocessing.image.ImageDataGenerator(\r\n",
        "    rescale=1./255,\r\n",
        "    rotation_range=180,\r\n",
        "    zoom_range=5\r\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "my4PxqfH26p6"
      },
      "source": [
        "images, labels = next(img_gen.flow_from_directory(dataPth))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hd96nH1w3eKH"
      },
      "source": [
        "print(images.dtype, images.shape)\n",
        "# print()\n",
        "print(labels.dtype, labels.shape)\n",
        "# print()\n",
        "\n",
        "imgCount=0\n",
        "for img in images:\n",
        "    imgCount+=1\n",
        "    print(f'\\n\\n{C.IBlue}{imgCount}.\\n{C.IWhite}{img}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvRwvt5E2rTH"
      },
      "source": [
        "ds  = tf.data.Dataset.from_generator(\n",
        "    lambda: img_gen.flow_from_directory(dataPth), \n",
        "    output_types=(tf.float32, tf.float32), \n",
        "    output_shapes=([13,256,256,3], [13,3]))\n",
        "\n",
        "defconData = tf.data.Dataset.from_generator(\n",
        "    lambda: img_gen.flow_from_directory(dataPth), \n",
        "    output_types=(tf.float32, tf.float32), \n",
        "    output_shapes=([13,256,256,3], [13,3]))\n",
        "\n",
        "print(ds.element_spec)\n",
        "print('defconData', defconData.element_spec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcaULBCXj_2_"
      },
      "source": [
        "for images, label in defconData.take(1):\n",
        "    print('images.shape: ', images.shape)\n",
        "    print('labels.shape: ', labels.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xhBRgvNqRRe"
      },
      "source": [
        "def process_path(dataPth):\n",
        "    label = tf.strings.split(file_path, os.sep)[-2]\n",
        "    return tf.io.read_file(file_path), label\n",
        "\n",
        "labeled_defconData = list_ds.map(ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UucIgCxWoSGx"
      },
      "source": [
        "But a repeat before a shuffle mixes the epoch boundaries together:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXw1IZVdbDcq"
      },
      "source": [
        "### Decoding image data and resizing it\n",
        "\n",
        "<!-- TODO(markdaoust): link to image augmentation when it exists -->\n",
        "When training a neural network on real-world image data, it is often necessary\n",
        "to convert images of different sizes to a common size, so that they may be\n",
        "batched into a fixed size.\n",
        "\n",
        "Rebuild the flower filenames dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMGlj8V-u-NH"
      },
      "source": [
        "# list_ds = tf.data.Dataset.list_files(str(flowers_root/'*/*'))\r\n",
        "list_ds = tf.data.Dataset.list_files(file_pattern=fileNames)\r\n",
        "\r\n",
        "list_ds\r\n",
        "defconData"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyhZLB8N5jBm"
      },
      "source": [
        "Write a function that manipulates the dataset elements."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZObC0debDcr"
      },
      "source": [
        "# Reads an image from a file, decodes it into a dense tensor, and resizes it\n",
        "# to a fixed shape.\n",
        "def parse_image(filename):\n",
        "    parts = tf.strings.split(filename, os.sep)\n",
        "    label = parts[-2]\n",
        "    \n",
        "    image = tf.io.read_file(filename)\n",
        "    image = tf.image.decode_jpeg(image)\n",
        "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "    image = tf.image.resize(image, [128, 128])\n",
        "    return image, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0dVJlCA5qHA"
      },
      "source": [
        "Test that it works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8xuN_HBzGup"
      },
      "source": [
        "file_path = next(iter(list_ds))\n",
        "image, label = parse_image(file_path)\n",
        "\n",
        "def show(image, label):\n",
        "    plt.figure()\n",
        "    plt.imshow(image)\n",
        "    plt.title(label.numpy().decode('utf-8'))\n",
        "    plt.axis('on')\n",
        "\n",
        "show(image, label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3P8N-S55vDu"
      },
      "source": [
        "Map it over the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzO8LI_H5Sk_"
      },
      "source": [
        "images_ds = list_ds.map(parse_image)\n",
        "for image, label in images_ds.take(13):\n",
        "  show(image, label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ff7IqB9bDcs"
      },
      "source": [
        "### Applying arbitrary Python logic\n",
        "\n",
        "For performance reasons, use TensorFlow operations for\n",
        "preprocessing your data whenever possible. However, it is sometimes useful to\n",
        "call external Python libraries when parsing your input data. You can use the `tf.py_function()` operation in a `Dataset.map()` transformation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBUmbERt7Czz"
      },
      "source": [
        "import scipy.ndimage as ndimage\n",
        "\n",
        "def fixed_rotate_image(image):\n",
        "    image = ndimage.rotate(image, angle=270.0)\n",
        "    return image\n",
        "\n",
        "def random_rotate_image(image):\n",
        "    image = ndimage.rotate(image, np.random.uniform(-30, 30), reshape=False)\n",
        "    return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wEyL7bS9S6t"
      },
      "source": [
        "image, label = next(iter(images_ds))\n",
        "image = fixed_rotate_image(image)\n",
        "show(image, label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxVx7z-ABNyq"
      },
      "source": [
        "To use this function with `Dataset.map` the same caveats apply as with `Dataset.from_generator`, you need to describe the return shapes and types when you apply the function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cn2nIu92BMp0"
      },
      "source": [
        "def tf_random_rotate_image(image, label):\n",
        "    print()\n",
        "    im_shape = image.shape\n",
        "    [image,] = tf.py_function(fixed_rotate_image, [image], [tf.float32])\n",
        "    image.set_shape(im_shape)\n",
        "    return image, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGypdgYOlXZz"
      },
      "source": [
        "You can work with `tf.train.Example` protos outside of a `tf.data.Dataset` to understand the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGJhKDp_61A_"
      },
      "source": [
        "img, txt = tf_parse(raw_example)\n",
        "print(txt.numpy())\n",
        "print(repr(img.numpy()[:20]), \"...\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vFIUFzD5qIC"
      },
      "source": [
        "decoded = images_ds.map(parse_image())\n",
        "decoded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRYNYkEej7Ix"
      },
      "source": [
        "image_batch, text_batch = next(iter(decoded.batch(10)))\n",
        "image_batch.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ry1n0UBeczit"
      },
      "source": [
        "<a id=\"time_series_windowing\"></a>\n",
        "\n",
        "### Time series windowing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0JMgvXEz9y1"
      },
      "source": [
        "For an end to end time series example see: [Time series forecasting](../../tutorials/text/time_series.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzBABBkAkkVJ"
      },
      "source": [
        "Time series data is often organized with the time axis intact.\n",
        "\n",
        "Use a simple `Dataset.range` to demonstrate:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTQgo49skjuY"
      },
      "source": [
        "range_ds = tf.data.Dataset.range(100000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6GLGhxgpazJ"
      },
      "source": [
        "Typically, models based on this sort of data will want a contiguous time slice. \n",
        "\n",
        "The simplest approach would be to batch the data:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETqB7QvTCNty"
      },
      "source": [
        "#### Using `batch`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSs9XqwQpvIN"
      },
      "source": [
        "batches = range_ds.batch(10, drop_remainder=True)\n",
        "\n",
        "for batch in batches.take(5):\n",
        "  print(batch.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgb2qikEtk5W"
      },
      "source": [
        "Or to make dense predictions one step into the future, you might shift the features and labels by one step relative to each other:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47XfwPhetkIN"
      },
      "source": [
        "def dense_1_step(batch):\n",
        "  # Shift features and labels one step relative to each other.\n",
        "  return batch[:-1], batch[1:]\n",
        "\n",
        "predict_dense_1_step = batches.map(dense_1_step)\n",
        "\n",
        "for features, label in predict_dense_1_step.take(3):\n",
        "  print(features.numpy(), \" => \", label.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjsXuINKqsS_"
      },
      "source": [
        "To predict a whole window instead of a fixed offset you can split the batches into two parts:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMmkQB1Gqo6x"
      },
      "source": [
        "batches = range_ds.batch(15, drop_remainder=True)\n",
        "\n",
        "def label_next_5_steps(batch):\n",
        "  return (batch[:-5],   # Take the first 5 steps\n",
        "          batch[-5:])   # take the remainder\n",
        "\n",
        "predict_5_steps = batches.map(label_next_5_steps)\n",
        "\n",
        "for features, label in predict_5_steps.take(3):\n",
        "  print(features.numpy(), \" => \", label.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a611Qr3jlhl"
      },
      "source": [
        "To allow some overlap between the features of one batch and the labels of another, use `Dataset.zip`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11dF3wyFjk2J"
      },
      "source": [
        "feature_length = 10\n",
        "label_length = 3\n",
        "\n",
        "features = range_ds.batch(feature_length, drop_remainder=True)\n",
        "labels = range_ds.batch(feature_length).skip(1).map(lambda labels: labels[:label_length])\n",
        "\n",
        "predicted_steps = tf.data.Dataset.zip((features, labels))\n",
        "\n",
        "for features, label in predicted_steps.take(5):\n",
        "  print(features.numpy(), \" => \", label.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adew3o2mCURC"
      },
      "source": [
        "#### Using `window`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fF6pEdlduq8E"
      },
      "source": [
        "While using `Dataset.batch` works, there are situations where you may need finer control. The `Dataset.window` method gives you complete control, but requires some care: it returns a `Dataset` of `Datasets`. See [Dataset structure](#dataset_structure) for details."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEI2W_EBw2OX"
      },
      "source": [
        "window_size = 5\n",
        "\n",
        "windows = range_ds.window(window_size, shift=1)\n",
        "for sub_ds in windows.take(5):\n",
        "  print(sub_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r82hWdk4x-46"
      },
      "source": [
        "The `Dataset.flat_map` method can take a dataset of datasets and flatten it into a single dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SB8AI03mnF8u"
      },
      "source": [
        " for x in windows.flat_map(lambda x: x).take(30):\n",
        "   print(x.numpy(), end=' ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgLIwq9Anc34"
      },
      "source": [
        "In nearly all cases, you will want to `.batch` the dataset first:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5j_y84rmyVQa"
      },
      "source": [
        "def sub_to_batch(sub):\n",
        "  return sub.batch(window_size, drop_remainder=True)\n",
        "\n",
        "for example in windows.flat_map(sub_to_batch).take(5):\n",
        "  print(example.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVugrmND3Grp"
      },
      "source": [
        "Now, you can see that the `shift` argument controls how much each window moves over.\n",
        "\n",
        "Putting this together you might write this function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdFRv_0D4FqW"
      },
      "source": [
        "def make_window_dataset(ds, window_size=5, shift=1, stride=1):\n",
        "  windows = ds.window(window_size, shift=shift, stride=stride)\n",
        "\n",
        "  def sub_to_batch(sub):\n",
        "    return sub.batch(window_size, drop_remainder=True)\n",
        "\n",
        "  windows = windows.flat_map(sub_to_batch)\n",
        "  return windows\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iVxcVfEdf5b"
      },
      "source": [
        "ds = make_window_dataset(range_ds, window_size=10, shift = 5, stride=3)\n",
        "\n",
        "for example in ds.take(10):\n",
        "  print(example.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMGMTPQ4w8pr"
      },
      "source": [
        "Then it's easy to extract labels, as before:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0fPfZkZw6j_"
      },
      "source": [
        "dense_labels_ds = ds.map(dense_1_step)\n",
        "\n",
        "for inputs,labels in dense_labels_ds.take(3):\n",
        "  print(inputs.numpy(), \"=>\", labels.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyi_-ft0kvy4"
      },
      "source": [
        "### Resampling\n",
        "\n",
        "When working with a dataset that is very class-imbalanced, you may want to resample the dataset. `tf.data` provides two methods to do this. The credit card fraud dataset is a good example of this sort of problem.\n",
        "\n",
        "Note: See [Imbalanced Data](../tutorials/keras/imbalanced_data.ipynb) for a full tutorial.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2e8dxVUlFHO"
      },
      "source": [
        "zip_path = tf.keras.utils.get_file(\n",
        "    origin='https://storage.googleapis.com/download.tensorflow.org/data/creditcard.zip',\n",
        "    fname='creditcard.zip',\n",
        "    extract=True)\n",
        "\n",
        "csv_path = zip_path.replace('.zip', '.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhkkM4Wx75S_"
      },
      "source": [
        "creditcard_ds = tf.data.experimental.make_csv_dataset(\n",
        "    csv_path, batch_size=1024, label_name=\"Class\",\n",
        "    # Set the column types: 30 floats and an int.\n",
        "    column_defaults=[float()]*30+[int()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8O47EmHlxYX"
      },
      "source": [
        "Now, check the distribution of classes, it is highly skewed:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8-Ss69XlzXD"
      },
      "source": [
        "def count(counts, batch):\n",
        "  features, labels = batch\n",
        "  class_1 = labels == 1\n",
        "  class_1 = tf.cast(class_1, tf.int32)\n",
        "\n",
        "  class_0 = labels == 0\n",
        "  class_0 = tf.cast(class_0, tf.int32)\n",
        "\n",
        "  counts['class_0'] += tf.reduce_sum(class_0)\n",
        "  counts['class_1'] += tf.reduce_sum(class_1)\n",
        "\n",
        "  return counts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1a3t_B4l_f6"
      },
      "source": [
        "counts = creditcard_ds.take(10).reduce(\n",
        "    initial_state={'class_0': 0, 'class_1': 0},\n",
        "    reduce_func = count)\n",
        "\n",
        "counts = np.array([counts['class_0'].numpy(),\n",
        "                   counts['class_1'].numpy()]).astype(np.float32)\n",
        "\n",
        "fractions = counts/counts.sum()\n",
        "print(fractions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1b8lFhSnDdv"
      },
      "source": [
        "A common approach to training with an imbalanced dataset is to balance it. `tf.data` includes a few methods which enable this workflow:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8jQWsgMnjQG"
      },
      "source": [
        "#### Datasets sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ov14SRrQyQE3"
      },
      "source": [
        "One approach to resampling a dataset is to use `sample_from_datasets`. This is more applicable when you have a separate `data.Dataset` for each class.\n",
        "\n",
        "Here, just use filter to generate them from the credit card fraud data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YKfCPa-nioA"
      },
      "source": [
        "negative_ds = (\n",
        "  creditcard_ds\n",
        "    .unbatch()\n",
        "    .filter(lambda features, label: label==0)\n",
        "    .repeat())\n",
        "positive_ds = (\n",
        "  creditcard_ds\n",
        "    .unbatch()\n",
        "    .filter(lambda features, label: label==1)\n",
        "    .repeat())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FNd3sQjzl9-"
      },
      "source": [
        "for features, label in positive_ds.batch(10).take(1):\n",
        "  print(label.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxLAr-7p0ATX"
      },
      "source": [
        "To use `tf.data.experimental.sample_from_datasets` pass the datasets, and the weight for each:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjdPVIFCngOb"
      },
      "source": [
        "balanced_ds = tf.data.experimental.sample_from_datasets(\n",
        "    [negative_ds, positive_ds], [0.5, 0.5]).batch(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2K4ObOms082B"
      },
      "source": [
        "Now the dataset produces examples of each class with 50/50 probability:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Myvkw21Rz-fH"
      },
      "source": [
        "for features, labels in balanced_ds.take(10):\n",
        "  print(labels.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUTE3eb9nckY"
      },
      "source": [
        "#### Rejection resampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZ9ezkK6irMD"
      },
      "source": [
        "One problem with the above `experimental.sample_from_datasets` approach is that\n",
        "it needs a separate `tf.data.Dataset` per class. Using `Dataset.filter`\n",
        "works, but results in all the data being loaded twice.\n",
        "\n",
        "The `data.experimental.rejection_resample` function can be applied to a dataset to rebalance it, while only loading it once. Elements will be dropped from the dataset to achieve balance.\n",
        "\n",
        "`data.experimental.rejection_resample` takes a `class_func` argument. This `class_func` is applied to each dataset element, and is used to determine which class an example belongs to for the purposes of balancing.\n",
        "\n",
        "The elements of `creditcard_ds` are already `(features, label)` pairs. So the `class_func` just needs to return those labels:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zC_Cuzw8lhI5"
      },
      "source": [
        "def class_func(features, label):\n",
        "  return label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdKmE8Jumlp0"
      },
      "source": [
        "The resampler also needs a target distribution, and optionally an initial distribution estimate:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tv0tWNxmkzM"
      },
      "source": [
        "resampler = tf.data.experimental.rejection_resample(\n",
        "    class_func, target_dist=[0.5, 0.5], initial_dist=fractions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxJrOZVToGuE"
      },
      "source": [
        "The resampler deals with individual examples, so you must `unbatch` the dataset before applying the resampler:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fY6VIhr3oGHG"
      },
      "source": [
        "resample_ds = creditcard_ds.unbatch().apply(resampler).batch(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-HnC1s8idqV"
      },
      "source": [
        "The resampler returns creates `(class, example)` pairs from the output of the `class_func`. In this case, the `example` was already a `(feature, label)` pair, so use `map` to drop the extra copy of the labels:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpfCGU6BiaZq"
      },
      "source": [
        "balanced_ds = resample_ds.map(lambda extra_label, features_and_label: features_and_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3d2jyEhx9kD"
      },
      "source": [
        "Now the dataset produces examples of each class with 50/50 probability:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGLYChBQwkDV"
      },
      "source": [
        "for features, labels in balanced_ds.take(10):\n",
        "  print(labels.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYFKQx3bUBeU"
      },
      "source": [
        "## Iterator Checkpointing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOGg1UFhUE4z"
      },
      "source": [
        "Tensorflow supports [taking checkpoints](https://www.tensorflow.org/guide/checkpoint) so that when your training process restarts it can restore the latest checkpoint to recover most of its progress. In addition to checkpointing the model variables, you can also checkpoint the progress of the dataset iterator. This could be useful if you have a large dataset and don't want to start the dataset from the beginning on each restart. Note however that iterator checkpoints may be large, since transformations such as `shuffle` and `prefetch` require buffering elements within the iterator. \n",
        "\n",
        "To include your iterator in a checkpoint, pass the iterator to the `tf.train.Checkpoint` constructor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Fsm9wvKUsNC"
      },
      "source": [
        "range_ds = tf.data.Dataset.range(20)\n",
        "\n",
        "iterator = iter(range_ds)\n",
        "ckpt = tf.train.Checkpoint(step=tf.Variable(0), iterator=iterator)\n",
        "manager = tf.train.CheckpointManager(ckpt, '/tmp/my_ckpt', max_to_keep=3)\n",
        "\n",
        "print([next(iterator).numpy() for _ in range(5)])\n",
        "\n",
        "save_path = manager.save()\n",
        "\n",
        "print([next(iterator).numpy() for _ in range(5)])\n",
        "\n",
        "ckpt.restore(manager.latest_checkpoint)\n",
        "\n",
        "print([next(iterator).numpy() for _ in range(5)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxWglTwX9Fex"
      },
      "source": [
        "Note: It is not possible to checkpoint an iterator which relies on external state such as a `tf.py_function`. Attempting to do so will raise an exception complaining about the external state."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLRdedPpbDdD"
      },
      "source": [
        "## Using tf.data with tf.keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTQe8daMcgFz"
      },
      "source": [
        "The `tf.keras` API simplifies many aspects of creating and executing machine\n",
        "learning models. Its `.fit()` and `.evaluate()` and `.predict()` APIs support datasets as inputs. Here is a quick dataset and model setup:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bfjqm0hOfES"
      },
      "source": [
        "train, test = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "images, labels = train\n",
        "images = images/255.0\n",
        "labels = labels.astype(np.int32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDhF3rGnbDdD"
      },
      "source": [
        "fmnist_train_ds = tf.data.Dataset.from_tensor_slices((images, labels))\n",
        "fmnist_train_ds = fmnist_train_ds.shuffle(5000).batch(32)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rdogg8CfHs-G"
      },
      "source": [
        " Passing a dataset of `(feature, label)` pairs is all that's needed for `Model.fit` and `Model.evaluate`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cu4kPzOHnlt"
      },
      "source": [
        "model.fit(fmnist_train_ds, epochs=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzpAQfJMJF41"
      },
      "source": [
        "If you pass an infinite dataset, for example by calling `Dataset.repeat()`, you just need to also pass the `steps_per_epoch` argument:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bp1BpzlyJinb"
      },
      "source": [
        "model.fit(fmnist_train_ds.repeat(), epochs=2, steps_per_epoch=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTLsw_nqJpTw"
      },
      "source": [
        "For evaluation you can pass the number of evaluation steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnlRHlaL-XUI"
      },
      "source": [
        "loss, accuracy = model.evaluate(fmnist_train_ds)\n",
        "print(\"Loss :\", loss)\n",
        "print(\"Accuracy :\", accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8UBU3CJKEA4"
      },
      "source": [
        "For long datasets, set the number of steps to evaluate:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVgamf9HKDon"
      },
      "source": [
        "loss, accuracy = model.evaluate(fmnist_train_ds.repeat(), steps=10)\n",
        "print(\"Loss :\", loss)\n",
        "print(\"Accuracy :\", accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZYhJ_YSIl6w"
      },
      "source": [
        "The labels are not required in when calling `Model.predict`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "343lXJ-pIqWD"
      },
      "source": [
        "predict_ds = tf.data.Dataset.from_tensor_slices(images).batch(32)\n",
        "result = model.predict(predict_ds, steps = 10)\n",
        "print(result.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfzZORwLI202"
      },
      "source": [
        "But the labels are ignored if you do pass a dataset containing them:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgQJTPrT-2WF"
      },
      "source": [
        "result = model.predict(fmnist_train_ds, steps = 10)\n",
        "print(result.shape)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}