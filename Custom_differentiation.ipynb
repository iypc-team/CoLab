{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Custom differentiation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "file_extension": ".swift",
      "mimetype": "text/x-swift",
      "name": "swift",
      "version": ""
    },
    "kernelspec": {
      "display_name": "Swift",
      "language": "swift",
      "name": "swift"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iypc-team/CoLab/blob/master/Custom_differentiation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LP0gMw56TlvH"
      },
      "source": [
        "You can define custom derivatives for any Swift function that has differentiable parameters and results. By doing that, you can even import a C function and make it differentiable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "j0a8prgZTlEO",
        "outputId": "6b85ce86-72d0-47a3-d57e-d455793d5345",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "import Glibc\n",
        "\n",
        "func sillyExp(_ x: Float) -> Float {\n",
        "    let ùëí = Float(M_E)\n",
        "    print(\"Taking ùëí(\\(ùëí)) to the power of \\(x)!\")\n",
        "    return pow(ùëí, x)\n",
        "}\n",
        "\n",
        "@differentiating(sillyExp)\n",
        "func sillyDerivative(_ x: Float) -> (value: Float, pullback: (Float) -> Float) {\n",
        "    let y = sillyExp(x)\n",
        "    return (value: y, pullback: { v in v * y })\n",
        "}\n",
        "\n",
        "print(\"exp(3) =\", sillyExp(3))\n",
        "print(\"ùõÅexp(3) =\", gradient(of: sillyExp)(3))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Taking ùëí(2.7182817) to the power of 3.0!\r\n",
            "exp(3) = 20.085535\r\n",
            "Taking ùëí(2.7182817) to the power of 3.0!\r\n",
            "ùõÅexp(3) = 20.085535\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eQPX9r3R5OP-"
      },
      "source": [
        "## Stop derivatives from propagating\n",
        "\n",
        "Commonly known as \"stop gradient\" in machine learning use cases, method [`withoutDerivative()`](https://www.tensorflow.org/swift/api_docs/Protocols/Differentiable#/s:10TensorFlow14DifferentiablePAAE17withoutDerivativexyF) stops derivatives from propagating.\n",
        "\n",
        "Plus, `withoutDerivative()` can sometimes help the Swift compiler with identifying what not to differentiate and producing more efficient derivaitves. When it is detectable that the derivative of a function will always be zero, the Swift compiler will produce a warning. Explicitly using `.withoutDerivative()` silences that warning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ctRt6vBO5Wle",
        "outputId": "a9ca4e2d-ed75-4cf9-8e97-d5d51e0c8647",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "let x: Float = 2.0\n",
        "let y: Float = 3.0\n",
        "gradient(at: x, y) { x, y in\n",
        "    sin(sin(sin(x))) + cos(cos(cos(y))).withoutDerivative()\n",
        "}"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "‚ñø 2 elements\n",
              "  - .0 : -0.18009877\n",
              "  - .1 : 0.0\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EeV3wXQ79WS2"
      },
      "source": [
        "## Derivative surgery\n",
        "\n",
        "Method [`withGradient(_:)`](https://www.tensorflow.org/swift/api_docs/Protocols/Differentiable#/s:10TensorFlow14DifferentiablePAAE12withGradientyxy15CotangentVectorQzzcF) makes arbitrary operations (including mutation) run on the gradient at a value during the enclosing function‚Äôs backpropagation. \n",
        "\n",
        "Use this to debug or make experimental tweaks to backpropagation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AHV0ryTiD6j8"
      },
      "source": [
        "### It works anywhere"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9zKSeUjTmbxq"
      },
      "source": [
        "All differentiation APIs provided by the standard library are defined generically over all types that conform to the `Differentiable` protocol: `Float`, `Double`, `Float80`, SIMD vectors, and even your own types!\n",
        "\n",
        "Read technical document [Differentiable Types](https://github.com/tensorflow/swift/blob/master/docs/DifferentiableTypes.md) for more insights on the `Differentiable` protocol."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eKne7szjD8lr",
        "outputId": "0206b79a-87f3-4fa0-ac69-2730bc1e4f05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "var x: Float = 30\n",
        "x.gradient { x -> Float in\n",
        "    // Print the partial derivative with respect to the result of `sin(x)`.\n",
        "    let a = sin(x).withGradient { print(\"‚àÇ+/‚àÇsin = \\($0)\") } \n",
        "    // Force the partial derivative with respect to `x` to be `0.5`.\n",
        "    let b = log(x.withGradient { (dx: inout Float) in\n",
        "        print(\"‚àÇlog/‚àÇx = \\(dx), but rewritten to 0.5\");\n",
        "        dx = 0.5\n",
        "    })\n",
        "    return a + b\n",
        "}"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "‚àÇlog/‚àÇx = 0.033333335, but rewritten to 0.5\r\n",
            "‚àÇ+/‚àÇsin = 1.0\r\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.65425146\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vmw0gkqlD9xf"
      },
      "source": [
        "### Use it in a neural network module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JCf_OplsWzhW"
      },
      "source": [
        "Just like how we used it in a simple `Float` function, we can use it in any numerical application, like the following neural network built using the [Swift for TensorFlow Deep Learning Library](https://github.com/tensorflow/swift-apis)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fnSeAbs9-hf3",
        "outputId": "ac2ea385-d447-4194-9189-24aeb1faf8ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1637
        }
      },
      "source": [
        "import TensorFlow\n",
        "\n",
        "struct MLP: Layer {\n",
        "    var layer1 = Dense<Float>(inputSize: 2, outputSize: 10, activation: relu)\n",
        "    var layer2 = Dense<Float>(inputSize: 10, outputSize: 1, activation: relu)\n",
        "    \n",
        "    @differentiable\n",
        "    func call(_ input: Tensor<Float>) -> Tensor<Float> {\n",
        "        let h0 = layer1(input).withGradient { print(\"‚àÇL/‚àÇlayer1 =\", $0) }\n",
        "        return layer2(h0)\n",
        "    }\n",
        "}\n",
        "\n",
        "var classifier = MLP()\n",
        "let optimizer = SGD(for: classifier, learningRate: 0.02)\n",
        "\n",
        "let x: Tensor<Float> = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
        "let y: Tensor<Float> = [0, 1, 1, 0]\n",
        "\n",
        "for _ in 0..<10 {\n",
        "    let ùõÅmodel = classifier.gradient { classifier -> Tensor<Float> in\n",
        "        let ≈∑ = classifier(x).withGradient { print(\"‚àÇL/‚àÇ≈∑ =\", $0) }\n",
        "        let loss = (≈∑ - y).squared().mean()\n",
        "        print(\"Loss: \\(loss)\")\n",
        "        return loss\n",
        "    }\n",
        "    optimizer.update(&classifier.allDifferentiableVariables, along: ùõÅmodel)\n",
        "}"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 0.5\n",
            "‚àÇL/‚àÇ≈∑ = [[-0.25],\n",
            " [-0.25],\n",
            " [-0.25],\n",
            " [-0.25]]\n",
            "‚àÇL/‚àÇlayer1 = [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
            " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
            " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
            " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "Loss: 0.5\n",
            "‚àÇL/‚àÇ≈∑ = [[-0.25],\n",
            " [-0.25],\n",
            " [-0.25],\n",
            " [-0.25]]\n",
            "‚àÇL/‚àÇlayer1 = [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
            " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
            " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
            " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "Loss: 0.5\n",
            "‚àÇL/‚àÇ≈∑ = [[-0.25],\n",
            " [-0.25],\n",
            " [-0.25],\n",
            " [-0.25]]\n",
            "‚àÇL/‚àÇlayer1 = [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
            " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
            " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
            " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "Loss: 0.5\n",
            "‚àÇL/‚àÇ≈∑ = [[-0.25],\n",
            " [-0.25],\n",
            " [-0.25],\n",
            " [-0.25]]\n",
            "‚àÇL/‚àÇlayer1 = [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
            " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
            " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
            " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "Loss: 0.5\n",
            "‚àÇL/‚àÇ≈∑ = [[-0.25],\n",
            " [-0.25],\n",
            " [-0.25],\n",
            " [-0.25]]\n",
            "‚àÇL/‚àÇlayer1 = [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
            " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
            " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
            " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "Loss: 0.5\n",
            "‚àÇL/‚àÇ≈∑ = [[-0.25],\n",
            " [-0.25],\n",
            " [-0.25],\n",
            " [-0.25]]\n",
            "‚àÇL/‚àÇlayer1 = [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
            " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
            " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
            " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "Loss: 0.5\n",
            "‚àÇL/‚àÇ≈∑ = [[-0.25],\n",
            " [-0.25],\n",
            " [-0.25],\n",
            " [-0.25]]\n",
            "‚àÇL/‚àÇlayer1 = [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
            " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
            " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
            " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "Loss: 0.5\n",
            "‚àÇL/‚àÇ≈∑ = [[-0.25],\n",
            " [-0.25],\n",
            " [-0.25],\n",
            " [-0.25]]\n",
            "‚àÇL/‚àÇlayer1 = [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
            " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
            " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
            " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "Loss: 0.5\n",
            "‚àÇL/‚àÇ≈∑ = [[-0.25],\n",
            " [-0.25],\n",
            " [-0.25],\n",
            " [-0.25]]\n",
            "‚àÇL/‚àÇlayer1 = [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
            " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
            " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
            " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "Loss: 0.5\n",
            "‚àÇL/‚àÇ≈∑ = [[-0.25],\n",
            " [-0.25],\n",
            " [-0.25],\n",
            " [-0.25]]\n",
            "‚àÇL/‚àÇlayer1 = [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
            " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
            " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
            " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TzLfTj28gEUD"
      },
      "source": [
        "## Recomputing activations during backpropagation to save memory (checkpointing)\n",
        "\n",
        "Checkpointing is a traditional technique in reverse-mode automatic differentiation for saving memory. Rather than saving large intermediate values in the original computation for computing derivatives, the intermediate values are instead recomputed as needed during backpropagation.\n",
        "\n",
        "This technique has been realized in modern deep learning libraries as well. In Swift, API [`withRecomputationInPullbacks(_:)`](https://www.tensorflow.org/swift/api_docs/Protocols/Differentiable#/s:10TensorFlow14DifferentiablePAAE28withRecomputationInPullbacksyqd__qd__xcAaBRd__lF) enables you to control what to recompute during backpropagation, and it is available on all `Differentiable` types.\n",
        "\n",
        "But today, let us learn how to define our own gradient checkpointing APIs from scratch, in just a few lines of code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5cZe-JbjwMfZ"
      },
      "source": [
        "### Our gradient checkpointing API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "606ob1dn2v77"
      },
      "source": [
        "We can define our own gradient checkpointing API, `makeRecomputedInGradient(_:)`, in terms of standard library function [`differentiableFunction(from:)`](https://www.tensorflow.org/swift/api_docs/Functions#/s:10TensorFlow22differentiableFunction4fromq0_x_q_tcq0_5value_15CotangentVectorQz_AEQy_tAEQy0_c8pullbacktx_q_tc_tAA14DifferentiableRzAaJR_AaJR0_r1_lF), which is a shorthand for creating a differentiable function directly from a derivative function (also called a \"vector-Jacobian products (VJP) function\").\n",
        "\n",
        "As we have seen before, the derivative function returns a tuple of the original function's result and a pullback closure. We return `original(x)` in `value:`, and call `pullback(at:in:)` on `original` to evaluate the original function again and get a pullback."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b1uU3tcVwl_1",
        "colab": {}
      },
      "source": [
        "/// Given a differentiable function, returns the same differentiable function except when\n",
        "/// derivatives of this function are being computed. In that case, values in the original function needed\n",
        "/// for computing the derivatives will be recomputed, instead of being captured by the differential or pullback.\n",
        "///\n",
        "/// - Parameter body: The body of the differentiable function.\n",
        "/// - Returns: The same differentiable function whose derivatives, when computed, will recompute\n",
        "///   some values from the original function.\n",
        "func makeRecomputedInGradient<T: Differentiable, U: Differentiable>(\n",
        "    _ original: @escaping @differentiable (T) -> U\n",
        ") -> @differentiable (T) -> U {\n",
        "    return differentiableFunction { x in\n",
        "        (value: original(x), pullback: { v in pullback(at: x, in: original)(v) })\n",
        "    }\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UbeKj7NEF7zz"
      },
      "source": [
        "### Verify it works"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oee8SXital45",
        "outputId": "ab9fe5ea-f9b3-487f-8cb4-2fd553abd930",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "let input: Float = 10.0\n",
        "print(\"Running original computation...\")\n",
        "\n",
        "// Differentiable multiplication with checkpointing.\n",
        "let square = makeRecomputedInGradient { (x: Float) -> Float in\n",
        "    print(\"  Computing square...\")\n",
        "    return x * x\n",
        "}\n",
        "\n",
        "// Differentiate `f(x) = (cos(x))^2`.\n",
        "let (output, backprop) = input.valueWithPullback { input -> Float in\n",
        "    return square(cos(input))\n",
        "}\n",
        "print(\"Running backpropagation...\")\n",
        "let grad = backprop(1)\n",
        "print(\"Gradient = \\(grad)\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running original computation...\r\n",
            "  Computing square...\r\n",
            "Running backpropagation...\r\n",
            "  Computing square...\r\n",
            "Gradient = -0.9129453\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7SxWsSUqF9Bh"
      },
      "source": [
        "### Extend it to neural network modules\n",
        "\n",
        "In this example, we define a simple convolutional neural network.\n",
        "\n",
        "```swift\n",
        "struct Model: Layer {\n",
        "    var conv = Conv2D<Float>(filterShape: (5, 5, 3, 6))\n",
        "    var maxPool = MaxPool2D<Float>(poolSize: (2, 2), strides: (2, 2))\n",
        "    var flatten = Flatten<Float>()\n",
        "    var dense = Dense<Float>(inputSize: 36 * 6, outputSize: 10)\n",
        "\n",
        "    @differentiable\n",
        "    func call(_ input: Tensor<Float>) -> Tensor<Float> {\n",
        "        return input.sequenced(through: conv, maxPool, flatten, dense)\n",
        "    }\n",
        "}\n",
        "```\n",
        "\n",
        "We want to make activations in the convolution layer (`conv`) be recomputed during backpropagation. However, using `makeRecomputedInGradient(_:)` could make the resulting code look cumbersome, especially when we want to apply layers sequentially using [`sequenced(in:through:_:_:_:_:)`](https://www.tensorflow.org/swift/api_docs/Protocols/Differentiable#/s:10TensorFlow14DifferentiablePAAE9sequenced2in7through____6OutputQyd_3_AA7ContextC_qd__qd_0_qd_1_qd_2_qd_3_t5InputQyd__RszAA5LayerRd__AaMRd_0_AaMRd_1_AaMRd_2_AaMRd_3_AKQyd_0_AGRtd__AKQyd_1_AGRtd_0_AKQyd_2_AGRtd_1_AKQyd_3_AGRtd_2_r3_lF).\n",
        "\n",
        "```swift\n",
        "input.sequenced(in: context, through: conv, maxPool, flatten, dense)\n",
        "```\n",
        "\n",
        "So, why don't we define a **special layer type** that wraps a layer and makes its activations be recomputed during backpropagation? Let's do it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZP86M5RjP3OG"
      },
      "source": [
        "First, we define a `makeRecomputedInGradient(_:)` function that takes a binary function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bEm-n5H0QB8s",
        "colab": {}
      },
      "source": [
        "// Same as the previous `makeRecomputedInGradient(_:)`, except it's for binary functions.\n",
        "func makeRecomputedInGradient<T: Differentiable, U: Differentiable, V: Differentiable>(\n",
        "    _ original: @escaping @differentiable (T, U) -> V\n",
        ") -> @differentiable (T, U) -> V {\n",
        "    return differentiableFunction { x, y in\n",
        "        (value: original(x, y), pullback: { v in pullback(at: x, y, in: original)(v) })\n",
        "    }\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YU6DgqXxP5Nl"
      },
      "source": [
        "Then, we define a generic layer `ActivationDiscarding<Wrapped>`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ao1r_lIPGeOl",
        "colab": {}
      },
      "source": [
        "/// A layer wrapper that makes the underlying layer's activations be discarded during application\n",
        "/// and recomputed during backpropagation.\n",
        "struct ActivationDiscarding<Wrapped: Layer>: Layer \n",
        "    where Wrapped.AllDifferentiableVariables == Wrapped.CotangentVector {\n",
        "    /// The wrapped layer.\n",
        "    var wrapped: Wrapped\n",
        "\n",
        "    @differentiable\n",
        "    func call(_ input: Wrapped.Input) -> Wrapped.Output {\n",
        "        let apply = makeRecomputedInGradient { (layer: Wrapped, input: Input) -> Wrapped.Output in\n",
        "            print(\"    Applying \\(Wrapped.self) layer...\")\n",
        "            return layer(input)\n",
        "        }\n",
        "        return apply(wrapped, input)\n",
        "    }\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HqPXwwuTRjmz"
      },
      "source": [
        "Finally, we can add a method on all layers that returns the same layer except its activations are discarded during application and recomputed during backpropagation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PGgkNnNNR1th",
        "colab": {}
      },
      "source": [
        "extension Layer where AllDifferentiableVariables == CotangentVector {\n",
        "    func discardingActivations() -> ActivationDiscarding<Self> {\n",
        "        return ActivationDiscarding(wrapped: self)\n",
        "    }\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8PP-NZ9XU5_n"
      },
      "source": [
        "Back in the model, all we have to change is to wrap the convolution layer into the activation-recomputing layer.\n",
        "\n",
        "```swift\n",
        "var conv = Conv2D<Float>(filterShape: (5, 5, 3, 6)).discardingActivations()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bCwNPtCfSbGi"
      },
      "source": [
        "Now, simply use it in the model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gsWGwFjOJ3Md",
        "colab": {}
      },
      "source": [
        "struct Model: Layer {\n",
        "    var conv = Conv2D<Float>(filterShape: (5, 5, 3, 6)).discardingActivations()\n",
        "    var maxPool = MaxPool2D<Float>(poolSize: (2, 2), strides: (2, 2))\n",
        "    var flatten = Flatten<Float>()\n",
        "    var dense = Dense<Float>(inputSize: 36 * 6, outputSize: 10)\n",
        "\n",
        "    @differentiable\n",
        "    func call(_ input: Tensor<Float>) -> Tensor<Float> {\n",
        "        return input.sequenced(through: conv, maxPool, flatten, dense)\n",
        "    }\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dmFxciU6VYdF"
      },
      "source": [
        "When we run a training loop, we can see that the convolution layer's activations are computed twice: once during layer application, and once during backpropagation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-x1nYu0uVSPn",
        "outputId": "cd5b66c7-69c1-47cd-9350-48bc0ee35228",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        }
      },
      "source": [
        "// Use random training data.\n",
        "let x = Tensor<Float>(randomNormal: [10, 16, 16, 3])\n",
        "let y = Tensor<Int32>(rangeFrom: 0, to: 10, stride: 1)\n",
        "\n",
        "var model = Model()\n",
        "let opt = SGD(for: model)\n",
        "\n",
        "for i in 1...5 {\n",
        "    print(\"Starting training step \\(i)\")\n",
        "    print(\"  Running original computation...\")\n",
        "    let (logits, backprop) = model.appliedForBackpropagation(to: x)\n",
        "    let (loss, dL_d≈∑) = logits.valueWithGradient { logits in\n",
        "        softmaxCrossEntropy(logits: logits, labels: y)\n",
        "    }\n",
        "    print(\"  Loss: \\(loss)\")\n",
        "    print(\"  Running backpropagation...\")\n",
        "    let (dL_dŒ∏, _) = backprop(dL_d≈∑)\n",
        "    \n",
        "    opt.update(&model.allDifferentiableVariables, along: dL_dŒ∏)\n",
        "}"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting training step 1\r\n",
            "  Running original computation...\r\n",
            "    Applying Conv2D<Float> layer...\n",
            "  Loss: 3.4061344\n",
            "  Running backpropagation...\n",
            "    Applying Conv2D<Float> layer...\n",
            "Starting training step 2\n",
            "  Running original computation...\n",
            "    Applying Conv2D<Float> layer...\n",
            "  Loss: 2.7654467\n",
            "  Running backpropagation...\n",
            "    Applying Conv2D<Float> layer...\n",
            "Starting training step 3\n",
            "  Running original computation...\n",
            "    Applying Conv2D<Float> layer...\n",
            "  Loss: 2.4142513\n",
            "  Running backpropagation...\n",
            "    Applying Conv2D<Float> layer...\n",
            "Starting training step 4\n",
            "  Running original computation...\n",
            "    Applying Conv2D<Float> layer...\n",
            "  Loss: 2.1456616\n",
            "  Running backpropagation...\n",
            "    Applying Conv2D<Float> layer...\n",
            "Starting training step 5\n",
            "  Running original computation...\n",
            "    Applying Conv2D<Float> layer...\n",
            "  Loss: 1.9202\n",
            "  Running backpropagation...\n",
            "    Applying Conv2D<Float> layer...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gzRaZLa_WX0u"
      },
      "source": [
        "Just like that, it is super easy to define generic differentiable programming libraries for different domains."
      ]
    }
  ]
}