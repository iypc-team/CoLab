{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "00 cartpole.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iypc-team/CoLab/blob/master/00_cartpole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emUEZEvldNyX",
        "colab_type": "text"
      },
      "source": [
        "# Dopamine: How to train an agent on Cartpole\n",
        "\n",
        "This colab demonstrates how to train the DQN and C51 on Cartpole, based on the default configurations provided.\n",
        "\n",
        "The hyperparameters chosen are by no mean optimal. The purpose of this colab is to illustrate how to train two\n",
        "agents on a non-Atari gym environment: cartpole.\n",
        "\n",
        "We also include default configurations for Acrobot in our repository: https://github.com/google/dopamine\n",
        "\n",
        "Run all the cells below in order."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ckq6WG-seC7F",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "# @title Install necessary packages.\n",
        "!pip install --upgrade --no-cache-dir dopamine-rl\n",
        "!pip install gin-config"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzwZoRKxdFov",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "# @title Necessary imports and globals.\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "from dopamine.discrete_domains import run_experiment\n",
        "from dopamine.colab import utils as colab_utils\n",
        "from absl import flags\n",
        "import gin.tf\n",
        "\n",
        "BASE_PATH = '/tmp/colab_dopamine_run'  # @param"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bidurBV0djGi",
        "colab_type": "text"
      },
      "source": [
        "## Train DQN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUBRSmX6dfa3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# @title Load the configuration for DQN.\n",
        "\n",
        "DQN_PATH = os.path.join(BASE_PATH, 'dqn')\n",
        "# Modified from dopamine/agents/dqn/config/dqn_cartpole.gin\n",
        "dqn_config = \"\"\"\n",
        "# Hyperparameters for a simple DQN-style Cartpole agent. The hyperparameters\n",
        "# chosen achieve reasonable performance.\n",
        "import dopamine.discrete_domains.gym_lib\n",
        "import dopamine.discrete_domains.run_experiment\n",
        "import dopamine.agents.dqn.dqn_agent\n",
        "import dopamine.replay_memory.circular_replay_buffer\n",
        "import gin.tf.external_configurables\n",
        "\n",
        "DQNAgent.observation_shape = %gym_lib.CARTPOLE_OBSERVATION_SHAPE\n",
        "DQNAgent.observation_dtype = %gym_lib.CARTPOLE_OBSERVATION_DTYPE\n",
        "DQNAgent.stack_size = %gym_lib.CARTPOLE_STACK_SIZE\n",
        "DQNAgent.network = @gym_lib.CartpoleDQNNetwork\n",
        "DQNAgent.gamma = 0.99\n",
        "DQNAgent.update_horizon = 1\n",
        "DQNAgent.min_replay_history = 500\n",
        "DQNAgent.update_period = 4\n",
        "DQNAgent.target_update_period = 100\n",
        "DQNAgent.epsilon_fn = @dqn_agent.identity_epsilon\n",
        "DQNAgent.tf_device = '/gpu:0'  # use '/cpu:*' for non-GPU version\n",
        "DQNAgent.optimizer = @tf.train.AdamOptimizer()\n",
        "\n",
        "tf.train.AdamOptimizer.learning_rate = 0.001\n",
        "tf.train.AdamOptimizer.epsilon = 0.0003125\n",
        "\n",
        "create_gym_environment.environment_name = 'CartPole'\n",
        "create_gym_environment.version = 'v0'\n",
        "create_agent.agent_name = 'dqn'\n",
        "TrainRunner.create_environment_fn = @gym_lib.create_gym_environment\n",
        "Runner.num_iterations = 50\n",
        "Runner.training_steps = 1000\n",
        "Runner.evaluation_steps = 1000\n",
        "Runner.max_steps_per_episode = 200  # Default max episode length.\n",
        "\n",
        "WrappedReplayBuffer.replay_capacity = 50000\n",
        "WrappedReplayBuffer.batch_size = 128\n",
        "\"\"\"\n",
        "gin.parse_config(dqn_config, skip_unknown=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuWFGwGHfkFp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# @title Train DQN on Cartpole\n",
        "dqn_runner = run_experiment.create_runner(DQN_PATH, schedule='continuous_train')\n",
        "print('Will train DQN agent, please be patient, may be a while...')\n",
        "dqn_runner.run_experiment()\n",
        "print('Done training!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRkvG1Nr6Etc",
        "colab_type": "text"
      },
      "source": [
        "# Train C51"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5o3a8HX6G2A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# @title Load the configuration for C51.\n",
        "\n",
        "C51_PATH = os.path.join(BASE_PATH, 'c51')\n",
        "# Modified from dopamine/agents/rainbow/config/c51_cartpole.gin\n",
        "c51_config = \"\"\"\n",
        "# Hyperparameters for a simple C51-style Cartpole agent. The hyperparameters\n",
        "# chosen achieve reasonable performance.\n",
        "import dopamine.agents.dqn.dqn_agent\n",
        "import dopamine.agents.rainbow.rainbow_agent\n",
        "import dopamine.discrete_domains.gym_lib\n",
        "import dopamine.discrete_domains.run_experiment\n",
        "import dopamine.replay_memory.prioritized_replay_buffer\n",
        "import gin.tf.external_configurables\n",
        "\n",
        "RainbowAgent.observation_shape = %gym_lib.CARTPOLE_OBSERVATION_SHAPE\n",
        "RainbowAgent.observation_dtype = %gym_lib.CARTPOLE_OBSERVATION_DTYPE\n",
        "RainbowAgent.stack_size = %gym_lib.CARTPOLE_STACK_SIZE\n",
        "RainbowAgent.network = @gym_lib.CartpoleRainbowNetwork\n",
        "RainbowAgent.num_atoms = 51\n",
        "RainbowAgent.vmax = 10.\n",
        "RainbowAgent.gamma = 0.99\n",
        "RainbowAgent.update_horizon = 1\n",
        "RainbowAgent.min_replay_history = 500\n",
        "RainbowAgent.update_period = 4\n",
        "RainbowAgent.target_update_period = 100\n",
        "RainbowAgent.epsilon_fn = @dqn_agent.identity_epsilon\n",
        "RainbowAgent.replay_scheme = 'uniform'\n",
        "RainbowAgent.tf_device = '/gpu:0'  # use '/cpu:*' for non-GPU version\n",
        "RainbowAgent.optimizer = @tf.train.AdamOptimizer()\n",
        "\n",
        "tf.train.AdamOptimizer.learning_rate = 0.001\n",
        "tf.train.AdamOptimizer.epsilon = 0.0003125\n",
        "\n",
        "create_gym_environment.environment_name = 'CartPole'\n",
        "create_gym_environment.version = 'v0'\n",
        "create_agent.agent_name = 'rainbow'\n",
        "Runner.create_environment_fn = @gym_lib.create_gym_environment\n",
        "Runner.num_iterations = 50\n",
        "Runner.training_steps = 1000\n",
        "Runner.evaluation_steps = 1000\n",
        "Runner.max_steps_per_episode = 200  # Default max episode length.\n",
        "\n",
        "WrappedPrioritizedReplayBuffer.replay_capacity = 50000\n",
        "WrappedPrioritizedReplayBuffer.batch_size = 128\n",
        "\"\"\"\n",
        "gin.parse_config(c51_config, skip_unknown=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VI_v9lm66jzq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# @title Train C51 on Cartpole\n",
        "c51_runner = run_experiment.create_runner(C51_PATH, schedule='continuous_train')\n",
        "print('Will train agent, please be patient, may be a while...')\n",
        "c51_runner.run_experiment()\n",
        "print('Done training!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqBe5Yad63FT",
        "colab_type": "text"
      },
      "source": [
        "# Plot the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IknanILXX4Zz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# @title Load the training logs.\n",
        "data = colab_utils.read_experiment(DQN_PATH, verbose=True,\n",
        "                                   summary_keys=['train_episode_returns'])\n",
        "data['agent'] = 'DQN'\n",
        "data['run'] = 1\n",
        "c51_data = colab_utils.read_experiment(C51_PATH, verbose=True,\n",
        "                                       summary_keys=['train_episode_returns'])\n",
        "c51_data['agent'] = 'C51'\n",
        "c51_data['run'] = 1\n",
        "data = data.merge(c51_data, how='outer')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSOVFUKN-kea",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# @title Plot training results.\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(16,8))\n",
        "sns.tsplot(data=data, time='iteration', unit='run',\n",
        "           condition='agent', value='train_episode_returns', ax=ax)\n",
        "plt.title('Cartpole')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}